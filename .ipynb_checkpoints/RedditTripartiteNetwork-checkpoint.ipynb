{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design and construction of tripartite network from Reddit\n",
    "\n",
    "Datasets from multipartite complex networks with 3 or more levels (tripartite, quadripartite, etc.) are very scarce, unlike the case of only 2 levels better known as bipartite graphs, which are quite common.\n",
    "\n",
    "I designed and began to construct a tripartite network for my Ph.D. thesis, using the website [Reddit](https://www.reddit.com). According to their own description, \"*Reddit is a network of communities where people can dive into their interests, hobbies and passions. There's a community for whatever you're interested in on Reddit*\". In this context, I use the term *groups* instead of *communities* for technical reasons and to avoid misunderstandings.\n",
    "\n",
    "The tripartite network I defined is composed of:\n",
    "1. **Users** (usernames)\n",
    "2. **Groups** (subreddits)\n",
    "3. **Keywords** (words)\n",
    "\n",
    "My main interest is the tripartite network analysis in two important topics:\n",
    "* **Link prediction**. This can be used in recommendation systems for example, so we could recommend an user certain groups that might find interesting based on our anaylsis.\n",
    "* **Community detection**. Also called clustering in (sligthly) different contexts, and it can be used to detect clusters of users based on the groups they frecuent and the keyword they use, for instance.\n",
    "\n",
    "I already developed many algorithms to do **link prediction** and **community detection** in multipartite networks, but I was lacking of datasets to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community\n",
    "from pyvis import network as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Reddit API you should have first a Reddit account and\n",
    "# sign up for an OAUTH Client ID in https://www.reddit.com/prefs/apps\n",
    "# and at the page bottom click on: \"are you a developer? create an app...\"\n",
    "# https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\n",
    "\n",
    "my_username = 'tripartitenetwork' #account created only for this purpose\n",
    "my_password = '987654321reddit123456789'\n",
    "\n",
    "personal_use_script = 'jVFLZzCvn9H82rRg_M_O1w'\n",
    "secret = 'djzraeUgBxE5U-BKirzY7OG9RQm7_w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_connection_request():\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(personal_use_script, secret)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': 'password',\n",
    "            'username': my_username,\n",
    "            'password': my_password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the first call to headers_connection_request() doesn't work, we need a while loop\n",
    "def Headers():\n",
    "    my_headers = None\n",
    "    while my_headers is None:\n",
    "        try: # try until connects and therefore initialize the process\n",
    "            my_headers = headers_connection_request()\n",
    "            return my_headers\n",
    "        except:\n",
    "             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyBot/0.0.1',\n",
       " 'Authorization': 'bearer 1206362233968-esqAiHzolIuLrckMRmc_5bt_YxcLNA'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "my_headers = Headers()\n",
    "my_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The starting point is any Reddit username, it's the only input we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'zip759' #'urbannomadberlin' #'GovSchwarzenegger'\n",
    "my_limit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) We start extracting all the words used from our specific user, and simultaneously, the groups where they were posted\n",
    "\n",
    "We describe every text that a certain **user** writes (publicly) as a *post*. Hence, calling the Reddit API we indentify two main types of *posts* and some more subtypes:\n",
    "\n",
    "1. `comment`\n",
    "\n",
    "\n",
    "2. `submitted`\n",
    "\n",
    "    i. `title`\n",
    "    \n",
    "    ii. `selftext` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) We extract the keywords from comments and the subreddits where they were posted.\n",
    "\n",
    "We extract the **keywords** from every `comment` *post*, every `title` of a `submitted` *post*, and optionally from the `selftext` of a `submitted` post, if any. Then we saved all of them in a common string `posts_full_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_full_text = \"\"\n",
    "groups_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/comments\",\n",
    "                                    headers = my_headers,\n",
    "                                    params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_comments.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['body']\n",
    "    groups_list.append(post['data']['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Extracting keywords from submitted title, and from submitted selftext, if any, and the subreddits where they were posted.\n",
    "\n",
    "At the same time, we will append the subreddits, i.e. the **groups** where every *post* belongs, in a list called `groups_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/submitted\",\n",
    "                                     headers = my_headers,\n",
    "                                     params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['title']\n",
    "    groups_list.append(post['data']['subreddit'])\n",
    "    if post['data']['selftext']:\n",
    "        posts_full_text += \" \" + post['data']['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having all the groups where a user posted we make a very simple analysis of them.\n",
    "\n",
    "We count the **groups** repetitions and save them as a Python dictionary `groups_dict`. This will help us later to associate every **group** with its respective **user**, where the associated value will correspond to the link weight of the newly defined bipartite **user-groups** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "#groups_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After retrieving all of the user posts keywords, we start to analyze them using the simplest approach: the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The intention is to improve this analysis later with methods such as n-grams or more sophisticaed ones within the natural language processing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_text = posts_full_text.lower()\n",
    "#corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download if necessary!\n",
    "#nltk.download('punkt') #download if necessary!\n",
    "\n",
    "stopwords_e = nltk.corpus.stopwords.words('english')\n",
    "stopwords_g = nltk.corpus.stopwords.words('german')\n",
    "stopwords_s = nltk.corpus.stopwords.words('spanish') #add languages if needed\n",
    "stopwords = stopwords_e + stopwords_g + stopwords_s\n",
    "\n",
    "mystopwords = [\"a\", \"a's\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \n",
    "                \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \n",
    "                \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \n",
    "                \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \n",
    "                \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \n",
    "                \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \n",
    "                \"aren't\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \n",
    "                \"available\", \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\", \n",
    "                \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \n",
    "                \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \n",
    "                \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \n",
    "                \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \n",
    "                \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \n",
    "                \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \n",
    "                \"contains\", \"corresponding\", \"could\", \"couldn't\", \"course\", \"currently\", \n",
    "                \"d\", \"definitely\", \"described\", \"despite\", \"did\", \"didn't\", \"different\", \n",
    "                \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\", \n",
    "                \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \n",
    "                \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \n",
    "                \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \n",
    "                \"example\", \"except\", \"f\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \n",
    "                \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \n",
    "                \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\", \"getting\", \n",
    "                \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \n",
    "                \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn't\", \n",
    "                \"have\", \"haven't\", \"having\", \"he\", \"he's\", \"hello\", \"help\", \"hence\", \"her\", \n",
    "                \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \n",
    "                \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \n",
    "                \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\", \"if\", \n",
    "                \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \n",
    "                \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \n",
    "                \"is\", \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \n",
    "                \"k\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\", \n",
    "                \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \n",
    "                \"let's\", \"like\", \"liked\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \n",
    "                \"ltd\", \"m\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \n",
    "                \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \n",
    "                \"my\", \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \n",
    "                \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \n",
    "                \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \n",
    "                \"nothing\", \"novel\", \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \n",
    "                \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \n",
    "                \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \n",
    "                \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"p\", \"particular\", \n",
    "                \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \n",
    "                \"presumably\", \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\", \n",
    "                \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \n",
    "                \"regards\", \"relatively\", \"respectively\", \"right\", \"s\", \"said\", \"same\", \n",
    "                \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \n",
    "                \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \n",
    "                \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \n",
    "                \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \n",
    "                \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \n",
    "                \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \n",
    "                \"such\", \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \n",
    "                \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\", \"thats\", \"the\", \n",
    "                \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \n",
    "                \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \n",
    "                \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \n",
    "                \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \n",
    "                \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \n",
    "                \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \n",
    "                \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \n",
    "                \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \n",
    "                \"using\", \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\", \"viz\", \n",
    "                \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\", \n",
    "                \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \n",
    "                \"what's\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \n",
    "                \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \n",
    "                \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \n",
    "                \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \n",
    "                \"without\", \"won't\", \"wonder\", \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \n",
    "                \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \n",
    "                \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"] #complete with words to exclude if necessary\n",
    "\n",
    "stopwords += mystopwords\n",
    "\n",
    "def common_words(text): # isalpha() optional for words made of only letters \n",
    "    return [word for word in TextBlob(text).words if word not in stopwords]# and word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the most common words as a Python dictionary `keywords_dict`, will help us later to associate every **keyword** with its respective **user**, where the associated value will correspond to the link weight of the newly defined **user-keywords** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) We continue extracting, for our specific input user, all the associated users.\n",
    "\n",
    "In principle, this is not really necessary. Since we already have the basic code to extract all the **groups** and **keywords** for any specific **user**, we could do the same procedure for any arbitrary list of Reddit usernames. But it would make absolute sense to search for **users** connected somehow to our input **user**, and we will find them with a similar approach to the previous one, retrieving our input **user** information. Once we obtain all the **users** associated to our input **user**, we applied to them the full procedure describe in **(A)** to obtain their respective **groups** and **keywords**, and having this we'll have all the needed information to construct our tripartite network. Other different Reddit usernames can be also added manually at any point to expand the network even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) For any given input user and from its submitted posts, we extract the users from the direct replies (first children) to any of them.\n",
    "\n",
    "We save all the associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    name = post['data']['name']\n",
    "    while True:\n",
    "        try:\n",
    "            res_name = requests.get(\"https://oauth.reddit.com\" + \"/comments\" + \"/\" + name[3:] + \"/api\"\n",
    "                                    + \"/morechildren\",\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    for comment in res_name.json()[1]['data']['children']:\n",
    "        if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "            associated_users.append(comment['data']['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) For the same input user and from its comments, we extract the users from the previous comment (parent or link author).\n",
    "\n",
    "We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    link = post['data']['link_id']\n",
    "    parent = post['data']['parent_id']\n",
    "    if link != parent: #if parent is not the main post\n",
    "        while True:\n",
    "            try:\n",
    "                res_parent = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'][:-8]\n",
    "                                          + parent[3:],\n",
    "                                          headers = my_headers)\n",
    "                break\n",
    "            except requests.ConnectionError:\n",
    "                print(\"ConnectionError, trying again...\")\n",
    "                my_headers = Headers()#headers_connection_request()\n",
    "        for j, comment in enumerate(res_parent.json()[1]['data']['children']):\n",
    "            if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "                #print(j, comment['data']['author'])\n",
    "                associated_users.append(comment['data']['author'])\n",
    "    else: #parent is the main post\n",
    "        associated_users.append(post['data']['link_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) For the same input user and from its comments, we extract the users from all the following comments (first childrens).\n",
    "\n",
    "This is very tricky to do given the structure of the retrieved information, we need to define a recursive function which acts directly over the adecuate part of the retrieved json and returns a list of **users**. We start doing it only for one comment, then for all of them. We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_in_json(subjson, i=0, depth_limit=1, lst=[]): #depth_limit=1 will show only direct children from a comment\n",
    "    for post in subjson['data']['children']:\n",
    "        if i <= depth_limit:\n",
    "            if 'replies' in post['data']:\n",
    "                lst.append(post['data']['author'])\n",
    "                if post['data']['replies']:\n",
    "                    recursive_in_json(post['data']['replies'], i+1, depth_limit=depth_limit, lst=lst)\n",
    "    return lst[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "userstestlist = []\n",
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    while True:\n",
    "        try:\n",
    "            res_test = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'],\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    utl = recursive_in_json(res_test.json()[1], lst=[])\n",
    "    if utl:\n",
    "        userstestlist.extend(utl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users.extend(userstestlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean this list deleting repeating entries using a Python set, deleting the input **user** and the `'[deleted]'` ones (profiles that doesn't exist anymore), finally creating the list `users_list` to save all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = list(set(associated_users))\n",
    "if username in users_list:\n",
    "    users_list.remove(username)\n",
    "if '[deleted]' in users_list:\n",
    "    users_list.remove('[deleted]')\n",
    "if 'AutoModerator' in users_list:\n",
    "    users_list.remove('AutoModerator')\n",
    "    \n",
    "users_full_list = users_list + [username]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the input user, we found the associated groups, keywords and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_users_groups_keywords_dict = {}\n",
    "final_users_groups_keywords_dict[username] = {}\n",
    "final_users_groups_keywords_dict[username]['groups'] = groups_dict\n",
    "final_users_groups_keywords_dict[username]['keywords'] = keywords_dict\n",
    "final_users_groups_keywords_dict[username]['users'] = users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_full_set = set(groups_dict.keys())\n",
    "keywords_full_set = set(keywords_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all groups and keywords for the associated users\n",
    "\n",
    "We automatize now the previous procedure to obtain **groups** and **keywords** for every **user** in `users_list`, and save them in a Python dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_keywords_dict(user, the_headers):\n",
    "\n",
    "    posts_full_text = \"\"\n",
    "    groups_list = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/comments\",\n",
    "                                        headers = the_headers,\n",
    "                                        params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "    try:\n",
    "        for post in res_comments.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['body']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/submitted\",\n",
    "                                         headers = the_headers,\n",
    "                                         params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "    try:\n",
    "        for post in res_submitted.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['title']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "            if post['data']['selftext']:\n",
    "                posts_full_text += \" \" + post['data']['selftext']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "\n",
    "    corpus_text = posts_full_text.lower()\n",
    "    keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "\n",
    "    return groups_dict, keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 HLef\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "1 Okzuo\n",
      "2 cerebraldormancy\n",
      "3 billionai1\n",
      "4 SilentSamamander\n",
      "5 Jungaktien_Jannik\n",
      "6 qhyirrstynne\n",
      "7 laymanlinguist\n",
      "8 docsyzygy\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "9 General_Ad4617\n",
      "10 BobVosh\n",
      "11 itsbotpixel\n",
      "12 flatfisher\n",
      "13 DemocraticRepublic\n",
      "14 thisimpetus\n",
      "15 BrewCityChaser\n",
      "16 Four4TheRoad\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "17 wsbfan1123\n",
      "18 secret759\n",
      "19 ThatPortraitGuy\n",
      "20 biggest_____chungus\n",
      "21 EpaFdx\n",
      "22 Patty_Henry\n",
      "23 sonia72quebec\n",
      "24 Oculosdegrau\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n"
     ]
    }
   ],
   "source": [
    "#my_headers = headers_connection_request() #if needed\n",
    "\n",
    "for i, user in enumerate(users_list):\n",
    "    print(i, user)\n",
    "    gkd = groups_keywords_dict(user, my_headers)\n",
    "    final_users_groups_keywords_dict[user] = {}\n",
    "    final_users_groups_keywords_dict[user]['groups'] = gkd[0]\n",
    "    final_users_groups_keywords_dict[user]['keywords'] = gkd[1]\n",
    "    \n",
    "    groups_full_set = groups_full_set.union(gkd[0].keys())\n",
    "    keywords_full_set = keywords_full_set.union(gkd[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groups': {'u_zip759': 3,\n",
       "  'Pharmadrug': 1,\n",
       "  'france': 1,\n",
       "  'technology': 1,\n",
       "  'aviation': 1,\n",
       "  'AskReddit': 1,\n",
       "  'programming': 1,\n",
       "  'science': 1},\n",
       " 'keywords': {'canon': 2,\n",
       "  'time': 2,\n",
       "  'worry': 1,\n",
       "  'nida-funded': 1,\n",
       "  \"'s\": 1,\n",
       "  'shame': 1,\n",
       "  'uk': 1,\n",
       "  'left': 1,\n",
       "  'eu': 1,\n",
       "  'reason': 1,\n",
       "  'enabled': 1,\n",
       "  '‘': 1,\n",
       "  'auto': 1,\n",
       "  'refill': 1,\n",
       "  '’': 1,\n",
       "  'printer': 1,\n",
       "  'office': 1,\n",
       "  'guy': 1,\n",
       "  'stupid': 1,\n",
       "  'write': 1,\n",
       "  'instant': 1,\n",
       "  'messages': 1,\n",
       "  'bragging': 1,\n",
       "  'misleading': 1,\n",
       "  'faa': 1,\n",
       "  'moving': 1,\n",
       "  'fast': 1,\n",
       "  '1-17': 1,\n",
       "  'forever': 1,\n",
       "  '18-20': 1,\n",
       "  'feels': 1,\n",
       "  'month': 1,\n",
       "  '20-30': 1,\n",
       "  'feel': 1,\n",
       "  'week': 1,\n",
       "  'phrx': 1,\n",
       "  'partner': 1,\n",
       "  'jhu': 1,\n",
       "  'news': 1,\n",
       "  'receiving': 1,\n",
       "  'federal': 1,\n",
       "  'funding': 1,\n",
       "  'psychedelic': 1,\n",
       "  'research': 1,\n",
       "  'sued': 1,\n",
       "  'disabling': 1,\n",
       "  'scanner': 1,\n",
       "  'printers': 1,\n",
       "  'run': 1,\n",
       "  'ink': 1,\n",
       "  'software': 1,\n",
       "  'developers': 1,\n",
       "  'stopped': 1,\n",
       "  'caring': 1,\n",
       "  'reliability': 1,\n",
       "  'covid-19': 1,\n",
       "  'caused': 1,\n",
       "  'extinction': 1,\n",
       "  'influenza': 1,\n",
       "  'lineage': 1,\n",
       "  'b/yamagata': 1,\n",
       "  'april': 1,\n",
       "  '2020': 1,\n",
       "  'august': 1,\n",
       "  '2021': 1,\n",
       "  'dopamine': 1,\n",
       "  'smartphones': 1,\n",
       "  'amp': 1,\n",
       "  'battle': 1},\n",
       " 'users': ['HLef',\n",
       "  'Okzuo',\n",
       "  'cerebraldormancy',\n",
       "  'billionai1',\n",
       "  'SilentSamamander',\n",
       "  'Jungaktien_Jannik',\n",
       "  'qhyirrstynne',\n",
       "  'laymanlinguist',\n",
       "  'docsyzygy',\n",
       "  'General_Ad4617',\n",
       "  'BobVosh',\n",
       "  'itsbotpixel',\n",
       "  'flatfisher',\n",
       "  'DemocraticRepublic',\n",
       "  'thisimpetus',\n",
       "  'BrewCityChaser',\n",
       "  'Four4TheRoad',\n",
       "  'wsbfan1123',\n",
       "  'secret759',\n",
       "  'ThatPortraitGuy',\n",
       "  'biggest_____chungus',\n",
       "  'EpaFdx',\n",
       "  'Patty_Henry',\n",
       "  'sonia72quebec',\n",
       "  'Oculosdegrau']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_users_groups_keywords_dict[username]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping to a json file the raw information of the tripartite network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on he input user, this could create a json file of a couple of MB\n",
    "with open('tripartite_raw.json', 'w') as f:\n",
    "    json.dump(final_users_groups_keywords_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing all elements \n",
    "\n",
    "We need to asign every element (user, group or keyword) a certain index, we choose to do it alphabetically, in order to make the correspondence to Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_users = {us: i for i, us in enumerate(sorted(users_full_list))}\n",
    "full_groups = {gr: i for i, gr in enumerate(sorted(groups_full_set))}\n",
    "full_keywords = {ke: i for i, ke in enumerate(sorted(keywords_full_set))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14816"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'195': 0,\n",
       " '196': 1,\n",
       " '2624': 2,\n",
       " '2healthbars': 3,\n",
       " '2meirl4meirl': 4,\n",
       " '4x4': 5,\n",
       " '52weeksofcooking': 6,\n",
       " 'AMA': 7,\n",
       " 'ANormalDayInRussia': 8,\n",
       " 'AOC': 9,\n",
       " 'AccidentalComedy': 10,\n",
       " 'AcousticOriginals': 11,\n",
       " 'Advice': 12,\n",
       " 'AmITheAngel': 13,\n",
       " 'AmItheAsshole': 14,\n",
       " 'AnimalsBeingDerps': 15,\n",
       " 'AntiJokes': 16,\n",
       " 'ArchitecturalRevival': 17,\n",
       " 'ArenaHS': 18,\n",
       " 'AskALiberal': 19,\n",
       " 'AskAnAmerican': 20,\n",
       " 'AskBiology': 21,\n",
       " 'AskEngineers': 22,\n",
       " 'AskEurope': 23,\n",
       " 'AskGameMasters': 24,\n",
       " 'AskHistorians': 25,\n",
       " 'AskHistory': 26,\n",
       " 'AskMen': 27,\n",
       " 'AskReddit': 28,\n",
       " 'AskScienceFiction': 29,\n",
       " 'Assistance': 30,\n",
       " 'Atlanta': 31,\n",
       " 'AutoChess': 32,\n",
       " 'AutoTuga': 33,\n",
       " 'Awww': 34,\n",
       " 'Baking': 35,\n",
       " 'BakingNoobs': 36,\n",
       " 'Bigpharmagame': 37,\n",
       " 'BirdsArentReal': 38,\n",
       " 'BlackPeopleTwitter': 39,\n",
       " 'BobsTavern': 40,\n",
       " 'Borderlands2': 41,\n",
       " 'Brewers': 42,\n",
       " 'Buttcoin': 43,\n",
       " 'COMPLETEANARCHY': 44,\n",
       " 'Calgary': 45,\n",
       " 'CalgaryClassifieds': 46,\n",
       " 'CanadaPolitics': 47,\n",
       " 'CasualConversation': 48,\n",
       " 'CasualUK': 49,\n",
       " 'CatastrophicFailure': 50,\n",
       " 'Catbun': 51,\n",
       " 'ChineseLanguage': 52,\n",
       " 'ChoosingBeggars': 53,\n",
       " 'CircleofTrust': 54,\n",
       " 'CollectiveGaming': 55,\n",
       " 'CompetitiveHS': 56,\n",
       " 'ContagiousLaughter': 57,\n",
       " 'ConvenientCop': 58,\n",
       " 'Cooking': 59,\n",
       " 'Coronavirus': 60,\n",
       " 'CrappyDesign': 61,\n",
       " 'CrazyIdeas': 62,\n",
       " 'CreateMod': 63,\n",
       " 'Cringetopia': 64,\n",
       " 'CrueltySquad': 65,\n",
       " 'CrusaderKings': 66,\n",
       " 'CryptoCurrency': 67,\n",
       " 'CryptoMarkets': 68,\n",
       " 'CuratedTumblr': 69,\n",
       " 'CurseofStrahd': 70,\n",
       " 'Cynicalbrit': 71,\n",
       " 'DC_Cinematic': 72,\n",
       " 'DIY': 73,\n",
       " 'DMAcademy': 74,\n",
       " 'Damnthatsinteresting': 75,\n",
       " 'DaysGone': 76,\n",
       " 'DeepIntoYouTube': 77,\n",
       " 'Deltarune': 78,\n",
       " 'Design': 79,\n",
       " 'Dimension20': 80,\n",
       " 'DnD': 81,\n",
       " 'DnDHomebrew': 82,\n",
       " 'DragonAgeCoOp': 83,\n",
       " 'DunderMifflin': 84,\n",
       " 'Economics': 85,\n",
       " 'EggInc': 86,\n",
       " 'EliteDangerous': 87,\n",
       " 'ElizabethWarren': 88,\n",
       " 'EnterTheGungeon': 89,\n",
       " 'FPGA': 90,\n",
       " 'FanTheories': 91,\n",
       " 'FellowKids': 92,\n",
       " 'Finanzen': 93,\n",
       " 'FindMeADistro': 94,\n",
       " 'Fitness': 95,\n",
       " 'FoundPaper': 96,\n",
       " 'Frisson': 97,\n",
       " 'Frugal': 98,\n",
       " 'FuckApple': 99,\n",
       " 'Futurology': 100,\n",
       " 'GSAT': 101,\n",
       " 'Games': 102,\n",
       " 'GamingDetails': 103,\n",
       " 'Gamingcirclejerk': 104,\n",
       " 'Georgia': 105,\n",
       " 'GoCommitDie': 106,\n",
       " 'GoldenAgeMinecraft': 107,\n",
       " 'Goldendoodles': 108,\n",
       " 'GoogleAnalytics': 109,\n",
       " 'GoogleWiFi': 110,\n",
       " 'GrinningGoat': 111,\n",
       " 'GrumpyBabyBirds': 112,\n",
       " 'HPfanfiction': 113,\n",
       " 'Habs': 114,\n",
       " 'HadesTheGame': 115,\n",
       " 'HawaiiVisitors': 116,\n",
       " 'HeadphoneAdvice': 117,\n",
       " 'HeavySeas': 118,\n",
       " 'Helicopters': 119,\n",
       " 'HermitCraft': 120,\n",
       " 'Hiphopcirclejerk': 121,\n",
       " 'HistoryPorn': 122,\n",
       " 'HolUp': 123,\n",
       " 'HongKong': 124,\n",
       " 'HumansBeingBros': 125,\n",
       " 'HydroHomies': 126,\n",
       " 'ImTheMainCharacter': 127,\n",
       " 'Invincible': 128,\n",
       " 'IoniqEV': 129,\n",
       " 'IsItBullshit': 130,\n",
       " 'Jeopardy': 131,\n",
       " 'JimSterling': 132,\n",
       " 'Jokes': 133,\n",
       " 'Kanye': 134,\n",
       " 'KidsAreFuckingStupid': 135,\n",
       " 'KipoAndTheAgeOfWB': 136,\n",
       " 'Kitboga': 137,\n",
       " 'KitchenConfidential': 138,\n",
       " 'LateStageCapitalism': 139,\n",
       " 'LifeProTips': 140,\n",
       " 'LuckBeALandlord': 141,\n",
       " 'MBMBAM': 142,\n",
       " 'MINI': 143,\n",
       " 'MadeMeSmile': 144,\n",
       " 'MadokaMagica': 145,\n",
       " 'MaliciousCompliance': 146,\n",
       " 'MapPorn': 147,\n",
       " 'MasterchefAU': 148,\n",
       " 'MechanicAdvice': 149,\n",
       " 'MemeEconomy': 150,\n",
       " 'Memes_Of_The_Dank': 151,\n",
       " 'MensLib': 152,\n",
       " 'Minecraft': 153,\n",
       " 'Missing411': 154,\n",
       " 'Missing411Discussions': 155,\n",
       " 'MissingPersons': 156,\n",
       " 'MkeBucks': 157,\n",
       " 'MonsterStrike': 158,\n",
       " 'MorbidReality': 159,\n",
       " 'MovieDetails': 160,\n",
       " 'MovieSuggestions': 161,\n",
       " 'MurderedByWords': 162,\n",
       " 'Music': 163,\n",
       " 'NASCAR': 164,\n",
       " 'NASCARCollectors': 165,\n",
       " 'NEU': 166,\n",
       " 'NFT': 167,\n",
       " 'NFTCollect': 168,\n",
       " 'NFTExchange': 169,\n",
       " 'NLSSCircleJerk': 170,\n",
       " 'NascarPaintBooth': 171,\n",
       " 'NatureIsFuckingLit': 172,\n",
       " 'NewSkaters': 173,\n",
       " 'NintendoSwitch': 174,\n",
       " 'NintendoSwitchDeals': 175,\n",
       " 'NoStupidQuestions': 176,\n",
       " 'NobodyAsked': 177,\n",
       " 'NotMyJob': 178,\n",
       " 'Nr2003': 179,\n",
       " 'OculusQuest2': 180,\n",
       " 'OfficeChairs': 181,\n",
       " 'OldSchoolCool': 182,\n",
       " 'Oldhouses': 183,\n",
       " 'OnePiece': 184,\n",
       " 'OpenseaMarket': 185,\n",
       " 'OutOfTheLoop': 186,\n",
       " 'PORTUGALCARALHO': 187,\n",
       " 'PS4': 188,\n",
       " 'PS5': 189,\n",
       " 'Parenting': 190,\n",
       " 'PastaPortuguesa': 191,\n",
       " 'PersonalFinanceCanada': 192,\n",
       " 'Peterborough': 193,\n",
       " 'PhantomBorders': 194,\n",
       " 'Pharmadrug': 195,\n",
       " 'PokemonGoCalgary': 196,\n",
       " 'PokemonSwordAndShield': 197,\n",
       " 'PoliticalCompassMemes': 198,\n",
       " 'PoliticalHumor': 199,\n",
       " 'Prague': 200,\n",
       " 'ProgrammerHumor': 201,\n",
       " 'PromoteYourMusic': 202,\n",
       " 'PublicFreakout': 203,\n",
       " 'PuzzleAndDragons': 204,\n",
       " 'RadicalChristianity': 205,\n",
       " 'RelayForReddit': 206,\n",
       " 'RimWorld': 207,\n",
       " 'Roadcam': 208,\n",
       " 'RocketLeague': 209,\n",
       " 'SHIBArmy': 210,\n",
       " 'SapphoAndHerFriend': 211,\n",
       " 'Scotland': 212,\n",
       " 'ScottishPeopleTwitter': 213,\n",
       " 'Sherbrooke': 214,\n",
       " 'ShitAmericansSay': 215,\n",
       " 'ShittyMapPorn': 216,\n",
       " 'Showerthoughts': 217,\n",
       " 'SiliconValleyHBO': 218,\n",
       " 'SimpsonsFaces': 219,\n",
       " 'SpaceXLounge': 220,\n",
       " 'StardewValley': 221,\n",
       " 'Stellaris': 222,\n",
       " 'Superstonk': 223,\n",
       " 'Tarmack': 224,\n",
       " 'Terminator': 225,\n",
       " 'TheExpanse': 226,\n",
       " 'TheWayWeWere': 227,\n",
       " 'TikTokCringe': 228,\n",
       " 'Tinder': 229,\n",
       " 'TooAfraidToAsk': 230,\n",
       " 'Tools': 231,\n",
       " 'TwoSentenceHorror': 232,\n",
       " 'TwoXChromosomes': 233,\n",
       " 'UFOs': 234,\n",
       " 'Unexpected': 235,\n",
       " 'UniversityOfHouston': 236,\n",
       " 'UnresolvedMysteries': 237,\n",
       " 'UnsentLetters': 238,\n",
       " 'UpliftingNews': 239,\n",
       " 'VintageApple': 240,\n",
       " 'VoteBlue': 241,\n",
       " 'WTF': 242,\n",
       " 'Wallstreetbetsnew': 243,\n",
       " 'Warbreaker': 244,\n",
       " 'WatchPeopleDieInside': 245,\n",
       " 'Watches': 246,\n",
       " 'WeAreTheMusicMakers': 247,\n",
       " 'WeatherGifs': 248,\n",
       " 'WeirdWings': 249,\n",
       " 'Wellthatsucks': 250,\n",
       " 'Whatcouldgowrong': 251,\n",
       " 'WhatsWrongWithYourDog': 252,\n",
       " 'Whiskyporn': 253,\n",
       " 'WhitePeopleTwitter': 254,\n",
       " 'WinStupidPrizes': 255,\n",
       " 'Wordpress': 256,\n",
       " 'WormMemes': 257,\n",
       " 'WritingPrompts': 258,\n",
       " 'Zoomies': 259,\n",
       " 'aaaaaaacccccccce': 260,\n",
       " 'adhdmeme': 261,\n",
       " 'ageofcivilization': 262,\n",
       " 'angband': 263,\n",
       " 'anime_titties': 264,\n",
       " 'animememes': 265,\n",
       " 'ankylosingspondylitis': 266,\n",
       " 'antergos': 267,\n",
       " 'ape': 268,\n",
       " 'apolloapp': 269,\n",
       " 'apple': 270,\n",
       " 'argentina': 271,\n",
       " 'aromantic': 272,\n",
       " 'asexuality': 273,\n",
       " 'askastronomy': 274,\n",
       " 'askgeology': 275,\n",
       " 'askscience': 276,\n",
       " 'askspain': 277,\n",
       " 'autorepair': 278,\n",
       " 'aviation': 279,\n",
       " 'awfuleverything': 280,\n",
       " 'aws': 281,\n",
       " 'aww': 282,\n",
       " 'badwomensanatomy': 283,\n",
       " 'baseball': 284,\n",
       " 'bearsdoinghumanthings': 285,\n",
       " 'bindingofisaac': 286,\n",
       " 'blackmagicfuckery': 287,\n",
       " 'blacksummer_': 288,\n",
       " 'books': 289,\n",
       " 'booksuggestions': 290,\n",
       " 'borderlands3': 291,\n",
       " 'boston': 292,\n",
       " 'braga': 293,\n",
       " 'brasil': 294,\n",
       " 'bropill': 295,\n",
       " 'bugs': 296,\n",
       " 'buildapc': 297,\n",
       " 'buildapcforme': 298,\n",
       " 'bullcity': 299,\n",
       " 'business': 300,\n",
       " 'callofcthulhu': 301,\n",
       " 'camping': 302,\n",
       " 'canada': 303,\n",
       " 'cardistry': 304,\n",
       " 'carporn': 305,\n",
       " 'casualiama': 306,\n",
       " 'clevercomebacks': 307,\n",
       " 'climate_science': 308,\n",
       " 'confusing_perspective': 309,\n",
       " 'conspiracy': 310,\n",
       " 'consulting': 311,\n",
       " 'crappyoffbrands': 312,\n",
       " 'cryptopt': 313,\n",
       " 'cursedcomments': 314,\n",
       " 'dadjokes': 315,\n",
       " 'dankmemes': 316,\n",
       " 'darkestdungeon': 317,\n",
       " 'dataisbeautiful': 318,\n",
       " 'daughtersofash': 319,\n",
       " 'depression': 320,\n",
       " 'dndmaps': 321,\n",
       " 'dndmemes': 322,\n",
       " 'dogelore': 323,\n",
       " 'dogpictures': 324,\n",
       " 'dogs': 325,\n",
       " 'dune': 326,\n",
       " 'egg_irl': 327,\n",
       " 'electricvehicles': 328,\n",
       " 'elliottsmith': 329,\n",
       " 'entertainment': 330,\n",
       " 'environment': 331,\n",
       " 'ethstaker': 332,\n",
       " 'eu4': 333,\n",
       " 'europe': 334,\n",
       " 'evopsych': 335,\n",
       " 'excel': 336,\n",
       " 'explainlikeimfive': 337,\n",
       " 'facepalm': 338,\n",
       " 'fatlogic': 339,\n",
       " 'fightporn': 340,\n",
       " 'food': 341,\n",
       " 'footballmanagergames': 342,\n",
       " 'france': 343,\n",
       " 'fuckHOA': 344,\n",
       " 'funny': 345,\n",
       " 'funnysigns': 346,\n",
       " 'furry_irl': 347,\n",
       " 'gaming': 348,\n",
       " 'gayspiderbrothel': 349,\n",
       " 'geology': 350,\n",
       " 'geopolitics': 351,\n",
       " 'germany': 352,\n",
       " 'gifs': 353,\n",
       " 'gifsthatkeepongiving': 354,\n",
       " 'giftcardexchange': 355,\n",
       " 'godtiersuperpowers': 356,\n",
       " 'goldenretrievers': 357,\n",
       " 'goodmythicalmorning': 358,\n",
       " 'greebles': 359,\n",
       " 'grubhub': 360,\n",
       " 'grubhubdrivers': 361,\n",
       " 'guitars': 362,\n",
       " 'hackintosh': 363,\n",
       " 'halifax': 364,\n",
       " 'harrypotter': 365,\n",
       " 'hearthstone': 366,\n",
       " 'help': 367,\n",
       " 'heroesofthestorm': 368,\n",
       " 'hiphopheads': 369,\n",
       " 'hockey': 370,\n",
       " 'hockeygoalies': 371,\n",
       " 'hockeymemes': 372,\n",
       " 'hockeyplayers': 373,\n",
       " 'hoi4': 374,\n",
       " 'homestead': 375,\n",
       " 'hometheater': 376,\n",
       " 'horror': 377,\n",
       " 'housepetscomic': 378,\n",
       " 'iRacing': 379,\n",
       " 'iamatotalpieceofshit': 380,\n",
       " 'ifyoulikeblank': 381,\n",
       " 'improv': 382,\n",
       " 'india': 383,\n",
       " 'insanepeoplefacebook': 384,\n",
       " 'insects': 385,\n",
       " 'insideno9': 386,\n",
       " 'interestingasfuck': 387,\n",
       " 'iphone': 388,\n",
       " 'itookapicture': 389,\n",
       " 'jerma985': 390,\n",
       " 'laptops': 391,\n",
       " 'latin': 392,\n",
       " 'legaladvice': 393,\n",
       " 'lgbt': 394,\n",
       " 'lifehacks': 395,\n",
       " 'linux': 396,\n",
       " 'linuxmemes': 397,\n",
       " 'lisboa': 398,\n",
       " 'listentothis': 399,\n",
       " 'literaciafinanceira': 400,\n",
       " 'london': 401,\n",
       " 'longisland': 402,\n",
       " 'loseit': 403,\n",
       " 'lostgeneration': 404,\n",
       " 'lotrmemes': 405,\n",
       " 'malelivingspace': 406,\n",
       " 'marvelstudios': 407,\n",
       " 'mauerstrassenwetten': 408,\n",
       " 'me_irl': 409,\n",
       " 'mead': 410,\n",
       " 'mechanics': 411,\n",
       " 'medical_advice': 412,\n",
       " 'meirl': 413,\n",
       " 'memes': 414,\n",
       " 'midlyintersting': 415,\n",
       " 'mildlyinfuriating': 416,\n",
       " 'mildlyinteresting': 417,\n",
       " 'millionairemakers': 418,\n",
       " 'milwaukee': 419,\n",
       " 'movies': 420,\n",
       " 'natureismetal': 421,\n",
       " 'netflix': 422,\n",
       " 'news': 423,\n",
       " 'nextfuckinglevel': 424,\n",
       " 'nfl': 425,\n",
       " 'noita': 426,\n",
       " 'northernlion': 427,\n",
       " 'nottheonion': 428,\n",
       " 'nutrition': 429,\n",
       " 'nuzlocke': 430,\n",
       " 'oddlysatisfying': 431,\n",
       " 'oddlyterrifying': 432,\n",
       " 'ontario': 433,\n",
       " 'oopsotherhand': 434,\n",
       " 'outriders': 435,\n",
       " 'parrots': 436,\n",
       " 'paslegorafi': 437,\n",
       " 'patientgamers': 438,\n",
       " 'pcgaming': 439,\n",
       " 'pcmasterrace': 440,\n",
       " 'penguins': 441,\n",
       " 'personalfinance': 442,\n",
       " 'photography': 443,\n",
       " 'pics': 444,\n",
       " 'playboicarti': 445,\n",
       " 'playstation': 446,\n",
       " 'pokemonzetaomicron': 447,\n",
       " 'politics': 448,\n",
       " 'popping': 449,\n",
       " 'porto': 450,\n",
       " 'portugal': 451,\n",
       " 'postprocessing': 452,\n",
       " 'powerwashingporn': 453,\n",
       " 'programming': 454,\n",
       " 'projectors': 455,\n",
       " 'raisedbynarcissists': 456,\n",
       " 'rance': 457,\n",
       " 'rape': 458,\n",
       " 'readanotherbook': 459,\n",
       " 'redditgetsdrawn': 460,\n",
       " 'roosterteeth': 461,\n",
       " 'runescape': 462,\n",
       " 'rust': 463,\n",
       " 'sad': 464,\n",
       " 'science': 465,\n",
       " 'shittyaskscience': 466,\n",
       " 'shittyfoodporn': 467,\n",
       " 'short': 468,\n",
       " 'shortscarystories': 469,\n",
       " 'shutupandtakemymoney': 470,\n",
       " 'skateboarding': 471,\n",
       " 'slavelabour': 472,\n",
       " 'slaythespire': 473,\n",
       " 'soccer': 474,\n",
       " 'solarpunk': 475,\n",
       " 'space': 476,\n",
       " 'spelunky': 477,\n",
       " 'squidgame': 478,\n",
       " 'starterpacks': 479,\n",
       " 'startrek': 480,\n",
       " 'stocks': 481,\n",
       " 'stopdrinking': 482,\n",
       " 'suspiciouslyspecific': 483,\n",
       " 'talesfromtechsupport': 484,\n",
       " 'taskmaster': 485,\n",
       " 'tattoos': 486,\n",
       " 'tax': 487,\n",
       " 'technicallythetruth': 488,\n",
       " 'technology': 489,\n",
       " 'techsupport': 490,\n",
       " 'television': 491,\n",
       " 'terriblefacebookmemes': 492,\n",
       " 'teslamotors': 493,\n",
       " 'theocho': 494,\n",
       " 'therewasanattempt': 495,\n",
       " 'theydidthemath': 496,\n",
       " 'thisismylifenow': 497,\n",
       " 'tifu': 498,\n",
       " 'timetravel': 499,\n",
       " 'tipofmytongue': 500,\n",
       " 'todayilearned': 501,\n",
       " 'traderjoes': 502,\n",
       " 'travel': 503,\n",
       " 'trees': 504,\n",
       " 'truegaming': 505,\n",
       " 'tumblr': 506,\n",
       " 'u_sonia72quebec': 507,\n",
       " 'u_zip759': 508,\n",
       " 'ukpolitics': 509,\n",
       " 'underlords': 510,\n",
       " 'unitedkingdom': 511,\n",
       " 'unpopularopinion': 512,\n",
       " 'vexillology': 513,\n",
       " 'vexillologycirclejerk': 514,\n",
       " 'videography': 515,\n",
       " 'videos': 516,\n",
       " 'vim': 517,\n",
       " 'visualization': 518,\n",
       " 'vosfinances': 519,\n",
       " 'wallstreetbets': 520,\n",
       " 'wallstreetbets2': 521,\n",
       " 'weather': 522,\n",
       " 'web_design': 523,\n",
       " 'webhosting': 524,\n",
       " 'whatisthisthing': 525,\n",
       " 'whatsthisbug': 526,\n",
       " 'whatsthisrock': 527,\n",
       " 'windowsphone': 528,\n",
       " 'wisconsin': 529,\n",
       " 'worldbuilding': 530,\n",
       " 'worldnews': 531,\n",
       " 'worldpolitics': 532,\n",
       " 'wow': 533,\n",
       " 'ynab': 534,\n",
       " 'youtubehaiku': 535,\n",
       " 'zelda': 536}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy bipartite arrays\n",
    "biparr_gu = np.zeros((len(full_groups), len(full_users)))\n",
    "biparr_uk = np.zeros((len(full_users), len(full_keywords)))\n",
    "\n",
    "#pandas bipartite dataframes\n",
    "df_gu = pd.DataFrame(columns=('group', 'user', 'repetitions'))\n",
    "#df_uk = pd.DataFrame(columns=('user', 'keywords', 'repetitions'))\n",
    "\n",
    "#biparr_gu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same loop for populating the numpy arrays, we create some pandas dataframes to be called by networkx and immediately used by pyvis to obtain interactive network visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#j = 0\n",
    "for user, values in final_users_groups_keywords_dict.items():\n",
    "    u_idx = full_users[user]\n",
    "    for group, gvalue in values['groups'].items():\n",
    "        g_idx = full_groups[group]\n",
    "        biparr_gu[g_idx][u_idx] = gvalue\n",
    "        df_gu.loc[i] = [group, user, gvalue]\n",
    "        i += 1\n",
    "    for keyword, kvalue in values['keywords'].items():\n",
    "        k_idx = full_keywords[keyword]\n",
    "        biparr_uk[u_idx][k_idx] = kvalue\n",
    "        #df_uk.loc[j] = [user, keyword, kvalue]\n",
    "        #j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"G_uk = nx.from_pandas_edgelist(df_uk, 'user', 'keyword', edge_attr='repetitions')\\npartition_G_uk = community.best_partition(G_uk, weight='repetitions')\\nfor n, p in partition_G_uk.items():\\n    G_uk.nodes[n]['group'] = p\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_gu = nx.from_pandas_edgelist(df_gu, 'group', 'user', edge_attr='repetitions')\n",
    "partition_G_gu = community.best_partition(G_gu, weight='repetitions')\n",
    "for n, p in partition_G_gu.items():\n",
    "    G_gu.nodes[n]['group'] = p\n",
    "    \n",
    "'''G_uk = nx.from_pandas_edgelist(df_uk, 'user', 'keyword', edge_attr='repetitions')\n",
    "partition_G_uk = community.best_partition(G_uk, weight='repetitions')\n",
    "for n, p in partition_G_uk.items():\n",
    "    G_uk.nodes[n]['group'] = p'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"bipartite_ggu.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff6466d1550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggu = net.Network(width=1000, height=1000, notebook=True, heading='Bipartite network of groups-users simple plot (unipartite Louvain communities)')\n",
    "ggu.toggle_physics(False)\n",
    "ggu.from_nx(G_gu)\n",
    "ggu.show(\"bipartite_ggu.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guk = net.Network(width=1000, height=1000, notebook=True, heading=\\'Bipartite users-keywords\\')\\nguk.toggle_physics(False)\\nguk.from_nx(G_uk)\\nguk.show(\"test_guk.html\")'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''guk = net.Network(width=1000, height=1000, notebook=True, heading='Bipartite users-keywords')\n",
    "guk.toggle_physics(False)\n",
    "guk.from_nx(G_uk)\n",
    "guk.show(\"test_guk.html\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
