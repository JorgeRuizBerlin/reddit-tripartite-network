{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design and construction of tripartite network from Reddit\n",
    "\n",
    "Datasets from multipartite complex networks with 3 or more levels (tripartite, quadripartite, etc.) are very scarce, unlike the case of only 2 levels better known as bipartite graphs, which are quite common.\n",
    "\n",
    "I designed and began to construct a tripartite network for my Ph.D. thesis, using the website [Reddit](https://www.reddit.com). According to their own description, \"*Reddit is a network of communities where people can dive into their interests, hobbies and passions. There's a community for whatever you're interested in on Reddit*\". In this context, I use the term *groups* instead of *communities* for technical reasons and to avoid misunderstandings.\n",
    "\n",
    "The tripartite network I defined is composed of:\n",
    "1. **Users** (usernames)\n",
    "2. **Groups** (subreddits)\n",
    "3. **Keywords** (words)\n",
    "\n",
    "My main interest is the tripartite network analysis in two important topics:\n",
    "* **Link prediction**. This can be used in recommendation systems for example, so we could recommend an user certain groups that might find interesting based on our anaylsis.\n",
    "* **Community detection**. Also called clustering in (sligthly) different contexts, and it can be used to detect clusters of users based on the groups they frecuent and the keyword they use, for instance.\n",
    "\n",
    "I already developed many algorithms to do **link prediction** and **community detection** in multipartite networks, but I was lacking of datasets to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Reddit API you should have first a Reddit account and\n",
    "# sign up for an OAUTH Client ID in https://www.reddit.com/prefs/apps\n",
    "# and at the page bottom click on: \"are you a developer? create an app...\"\n",
    "# https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\n",
    "\n",
    "my_username = 'tripartitenetwork' #account created only for this purpose\n",
    "my_password = '987654321reddit123456789'\n",
    "\n",
    "personal_use_script = 'jVFLZzCvn9H82rRg_M_O1w'\n",
    "secret = 'djzraeUgBxE5U-BKirzY7OG9RQm7_w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_connection_request():\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(personal_use_script, secret)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': 'password',\n",
    "            'username': my_username,\n",
    "            'password': my_password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyBot/0.0.1',\n",
       " 'Authorization': 'bearer 1206362233968-CE0VIO-ExQy8R80EVzxua6vyVP4uYQ'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize it once, if later fails, is called again within the while loops\n",
    "my_headers = headers_connection_request()\n",
    "my_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_headers = None\\nwhile my_headers is None:\\n    print(\"test\")\\n    try:\\n        # connect\\n        my_headers = headers_connection_request()\\n    except:\\n         pass'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case of the first call doesn't work within the while loops, we nned to add this extra while loop\n",
    "# being used in groups_keywords_dict function at the bottom\n",
    "'''my_headers = None\n",
    "while my_headers is None:\n",
    "    print(\"test\")\n",
    "    try:\n",
    "        # connect\n",
    "        my_headers = headers_connection_request()\n",
    "    except:\n",
    "         pass'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The starting point is any Reddit username, it's the only input we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'urbannomadberlin' #'GovSchwarzenegger'\n",
    "my_limit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) We start extracting all the words used from our specific user, and simultaneously, the groups where they were posted\n",
    "\n",
    "We describe every text that a certain **user** writes (publicly) as a *post*. Hence, calling the Reddit API we indentify two main types of *posts* and some more subtypes:\n",
    "\n",
    "1. `comment`\n",
    "\n",
    "\n",
    "2. `submitted`\n",
    "\n",
    "    i. `title`\n",
    "    \n",
    "    ii. `selftext` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) We extract the keywords from comments and the subreddits where they were posted.\n",
    "\n",
    "We extract the **keywords** from every `comment` *post*, every `title` of a `submitted` *post*, and optionally from the `selftext` of a `submitted` post, if any. Then we saved all of them in a common string `posts_full_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_full_text = \"\"\n",
    "groups_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/comments\",\n",
    "                                    headers = my_headers,\n",
    "                                    params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_comments.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['body']\n",
    "    groups_list.append(post['data']['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Extracting keywords from submitted title, and from submitted selftext, if any, and the subreddits where they were posted.\n",
    "\n",
    "At the same time, we will append the subreddits, i.e. the **groups** where every *post* belongs, in a list called `groups_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/submitted\",\n",
    "                                     headers = my_headers,\n",
    "                                     params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['title']\n",
    "    groups_list.append(post['data']['subreddit'])\n",
    "    if post['data']['selftext']:\n",
    "        posts_full_text += \" \" + post['data']['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having all the groups where a user posted we make a very simple analysis of them.\n",
    "\n",
    "We count the **groups** repetitions and save them as a Python dictionary `groups_dict`. This will help us later to associate every **group** with its respective **user**, where the associated value will correspond to the link weight of the newly defined bipartite **user-groups** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "#groups_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After retrieving all of the user posts keywords, we start to analyze them using the simplest approach: the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The intention is to improve this analysis later with methods such as n-grams or more sophisticaed ones within the natural language processing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_text = posts_full_text.lower()\n",
    "#corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download if necessary!\n",
    "\n",
    "stopwords_e = nltk.corpus.stopwords.words('english')\n",
    "stopwords_g = nltk.corpus.stopwords.words('german')\n",
    "stopwords_s = nltk.corpus.stopwords.words('spanish') #add languages if needed\n",
    "stopwords = stopwords_e + stopwords_g + stopwords_s\n",
    "\n",
    "mystopwords = [\"also\", \"b\", \"best\", \"cannot\", \"can't\", \"cant\"] #complete with words to exclude if necessary\n",
    "\n",
    "stopwords += mystopwords\n",
    "\n",
    "def common_words(text):\n",
    "    # isalpha() method optional for words made of only letters \n",
    "    return [word for word in TextBlob(text).words if word not in stopwords]# and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_words(corpus_text)\n",
    "#set(common_words(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the most common words as a Python dictionary `keywords_dict`, will help us later to associate every **keyword** with its respective **user**, where the associated value will correspond to the link weight of the newly defined **user-keywords** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "#keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) We continue extracting, for our specific input user, all the associated users.\n",
    "\n",
    "In principle, this is not really necessary. Since we already have the basic code to extract all the **groups** and **keywords** for any specific **user**, we could do the same procedure for any arbitrary list of Reddit usernames. But it would make absolute sense to search for **users** connected somehow to our input **user**, and we will find them with a similar approach to the previous one, retrieving our input **user** information. Once we obtain all the **users** associated to our input **user**, we applied to them the full procedure describe in **(A)** to obtain their respective **groups** and **keywords**, and having this we'll have all the needed information to construct our tripartite network. Other different Reddit usernames can be also added manually at any point to expand the network even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) For any given input user and from its submitted posts, we extract the users from the direct replies (first children) to any of them.\n",
    "\n",
    "We save all the associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConnectionError, trying again...\n"
     ]
    }
   ],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    name = post['data']['name']\n",
    "    while True:\n",
    "        try:\n",
    "            res_name = requests.get(\"https://oauth.reddit.com\" + \"/comments\" + \"/\" + name[3:] + \"/api\"\n",
    "                                    + \"/morechildren\",\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = headers_connection_request()\n",
    "    for comment in res_name.json()[1]['data']['children']:\n",
    "        if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "            associated_users.append(comment['data']['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#associated_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) For the same input user and from its comments, we extract the users from the previous comment (parent or link author).\n",
    "\n",
    "We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    #print(i, \"https://www.reddit.com\" + post['data']['permalink'])#, post['data']['body'][:50])\n",
    "    link = post['data']['link_id']\n",
    "    parent = post['data']['parent_id']\n",
    "    if link != parent: #if parent is not the main post\n",
    "        while True:\n",
    "            try:\n",
    "                res_parent = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'][:-8]\n",
    "                                          + parent[3:],\n",
    "                                          headers = my_headers)\n",
    "                break\n",
    "            except requests.ConnectionError:\n",
    "                print(\"ConnectionError, trying again...\")\n",
    "                my_headers = headers_connection_request()\n",
    "        for j, comment in enumerate(res_parent.json()[1]['data']['children']):\n",
    "            if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "                #print(j, comment['data']['author'])\n",
    "                associated_users.append(comment['data']['author'])\n",
    "    else: #parent is the main post\n",
    "        #print(post['data']['link_author'])\n",
    "        associated_users.append(post['data']['link_author'])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#associated_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) For the same input user and from its comments, we extract the users from all the following comments (first childrens).\n",
    "\n",
    "This is very tricky to do given the structure of the retrieved information, we need to define a recursive function which acts directly over the adecuate part of the retrieved json and returns a list of **users**. We start doing it only for one comment, then for all of them. We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_in_json(subjson, i=0, depth_limit=1, lst=[]): #depth_limit=1 will show only direct children from a comment\n",
    "    for post in subjson['data']['children']:\n",
    "        if i <= depth_limit:\n",
    "            if 'replies' in post['data']:\n",
    "                #print(post['data']['author'])\n",
    "                lst.append(post['data']['author'])\n",
    "                #print(i, \"Name:\", post['data']['name'], \"Depth:\", post['data']['depth'], \"Body:\", post['data']['body'][:50])\n",
    "                #print(\"********************\")\n",
    "                if post['data']['replies']:\n",
    "                    recursive_in_json(post['data']['replies'], i+1, depth_limit=depth_limit, lst=lst)\n",
    "                #if 'parent_id' in post['data']:\n",
    "                #    print('Parent_id:', post['data']['parent_id'])\n",
    "            #print(\"________________________________________________________________________________________________\")\n",
    "            #print()\n",
    "    return lst[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS EXAMPLE IS MEANT TO SHOW THE PARENTS OF A COMMENT\n",
    "# IT CAN USE \"comment\" INSTEAD OF POST NAME AT THE 2ND TO LAST PLACE\n",
    "\n",
    "##res_test = requests.get(\"https://oauth.reddit.com\"\n",
    "                        #+ \"/r/counting/comments/plti3p/4482k_counting_thread/hcdbx5n\"\n",
    "                        #+ \"/r/books/comments/q1sq8m/comment/hfh0glo/?utm_source=share&utm_medium=web2x&context=3\"\n",
    "##                        + \"/r/berlin/comments/pzeryw/why_is_getting_an_anmeldung_so_hard/hf3285d/\",\n",
    "                        #+ \"/?context=8\",\n",
    "##                        headers = my_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#userstest = recursive_in_json(res_test.json()[1], lst=[]) #lst=[] is needed to call the function correctly\n",
    "#userstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConnectionError, trying again...\n"
     ]
    }
   ],
   "source": [
    "userstestlist = []\n",
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    #print(i, \"https://www.reddit.com\" + post['data']['permalink'])\n",
    "    while True:\n",
    "        try:\n",
    "            res_test = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'],\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = headers_connection_request()\n",
    "    utl = recursive_in_json(res_test.json()[1], lst=[])\n",
    "    #print(utl)\n",
    "    #print()\n",
    "    if utl:\n",
    "        userstestlist.extend(utl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userstestlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users.extend(userstestlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean this list deleting repeating entries using a Python set, deleting the input **user** and the `'[deleted]'` ones (profiles that doesn't exist anymore), finally creating the list `users_list` to save all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = list(set(associated_users))\n",
    "users_list.remove(username)\n",
    "users_list.remove('[deleted]')\n",
    "#users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final step for the input user is to find the associated groups, keywords and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urbannomadberlin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chile': 32,\n",
       " 'berlin': 22,\n",
       " 'ifyoulikeblank': 21,\n",
       " 'berlinsocialclub': 16,\n",
       " 'musicsuggestions': 10,\n",
       " 'InternetIsBeautiful': 2,\n",
       " 'worldnews': 2,\n",
       " 'LesPaul': 2,\n",
       " 'MusicCritique': 2,\n",
       " 'mildlyinteresting': 1,\n",
       " 'europe': 1,\n",
       " 'Music': 1,\n",
       " 'dataisbeautiful': 1,\n",
       " 'movies': 1,\n",
       " 'funny': 1,\n",
       " 'AskReddit': 1,\n",
       " 'HeadphoneAdvice': 1,\n",
       " 'CryptoCurrency': 1,\n",
       " 'KrakenSupport': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https': 31,\n",
       " 'amp': 18,\n",
       " \"n't\": 16,\n",
       " 'music': 16,\n",
       " 'www.youtube.com/watch': 14,\n",
       " \"'s\": 13,\n",
       " 'jajaja': 12,\n",
       " 'one': 12,\n",
       " 'rock': 12,\n",
       " 'please': 11,\n",
       " 'thanks': 11,\n",
       " 'like': 11,\n",
       " 'x200b': 11,\n",
       " 'hi': 9,\n",
       " \"'m\": 9,\n",
       " 'chile': 8,\n",
       " 'si': 8,\n",
       " 'looking': 8,\n",
       " '3': 7,\n",
       " 'well': 7,\n",
       " 'know': 7,\n",
       " 'want': 7,\n",
       " 'make': 7,\n",
       " 'way': 7,\n",
       " 'songs': 7,\n",
       " 'could': 6,\n",
       " 'pa': 6,\n",
       " 'listen': 6,\n",
       " 'bass': 6,\n",
       " 'course': 6,\n",
       " 'really': 6,\n",
       " 'think': 6,\n",
       " 'comida': 5,\n",
       " 'wrote': 5,\n",
       " 'love': 5,\n",
       " 'berlin': 5,\n",
       " 'soon': 5,\n",
       " 'bands': 5,\n",
       " 'days': 5,\n",
       " 'jazz': 5,\n",
       " 'album': 4,\n",
       " 'going': 4,\n",
       " 'possible': 4,\n",
       " 'find': 4,\n",
       " 'say': 4,\n",
       " '2': 4,\n",
       " 'would': 4,\n",
       " 'let': 4,\n",
       " 'even': 4,\n",
       " 'hahaha': 4,\n",
       " 'something': 4,\n",
       " 'first': 4,\n",
       " '1m5': 4,\n",
       " '1m1': 4,\n",
       " '2m2': 4,\n",
       " 'different': 4,\n",
       " 'sense': 4,\n",
       " 'live': 4,\n",
       " 'good': 4,\n",
       " 'new': 4,\n",
       " 'look': 4,\n",
       " 'canciones': 4,\n",
       " 'metal': 4,\n",
       " 'unfortunately': 4,\n",
       " 'bring': 4,\n",
       " 'march': 4,\n",
       " 'contract': 4,\n",
       " 'money': 4,\n",
       " 'chat': 3,\n",
       " '100': 3,\n",
       " 'sólo': 3,\n",
       " 'procesada': 3,\n",
       " 'hace': 3,\n",
       " 'alguna': 3,\n",
       " 'c3': 3,\n",
       " 'chucha': 3,\n",
       " 'aún': 3,\n",
       " 'años': 3,\n",
       " 'ahora': 3,\n",
       " 'today': 3,\n",
       " 'lot': 3,\n",
       " 'hola': 3,\n",
       " 'lt': 3,\n",
       " 'prost': 3,\n",
       " 'last': 3,\n",
       " 'working': 3,\n",
       " 'park': 3,\n",
       " 'appreciated': 3,\n",
       " '5': 3,\n",
       " 'casi': 3,\n",
       " 'everyone': 3,\n",
       " 'ezra': 3,\n",
       " 'collective': 3,\n",
       " 'bank': 3,\n",
       " 'listening': 3,\n",
       " 'message': 3,\n",
       " 'público': 3,\n",
       " 'available': 3,\n",
       " 'enjoy': 3,\n",
       " 'clearly': 3,\n",
       " 'santana': 3,\n",
       " 'apruebo': 3,\n",
       " '10': 3,\n",
       " '4': 3,\n",
       " 'similar': 3,\n",
       " 'tool': 3,\n",
       " 'previous': 3,\n",
       " 'floydian': 3,\n",
       " 'pt': 3,\n",
       " 'mastodon': 3,\n",
       " 'albums': 3,\n",
       " 'totally': 3,\n",
       " 'sound': 3,\n",
       " 'check': 3,\n",
       " 'home': 3,\n",
       " 'marino': 3,\n",
       " 'nubiyan': 3,\n",
       " 'twist': 3,\n",
       " 'musical': 3,\n",
       " 'and/or': 3,\n",
       " 'playing': 3,\n",
       " 'like/know': 3,\n",
       " 'feel': 3,\n",
       " 'fusion': 3,\n",
       " 'people': 3,\n",
       " 'amazing': 3,\n",
       " 'reddit': 3,\n",
       " 'bluetooth': 3,\n",
       " 'use': 3,\n",
       " 'receive': 3,\n",
       " 'experience': 3,\n",
       " '2017': 3,\n",
       " 'dominance': 3,\n",
       " 'price': 3,\n",
       " 'another': 3,\n",
       " 'deposit': 3,\n",
       " 'german': 3,\n",
       " 'gt': 2,\n",
       " 'sent': 2,\n",
       " 'seguro': 2,\n",
       " 'haber': 2,\n",
       " 'mala': 2,\n",
       " 'cara': 2,\n",
       " 'hecho': 2,\n",
       " 'wea': 2,\n",
       " 'q': 2,\n",
       " 'parece': 2,\n",
       " 'total': 2,\n",
       " 'tan': 2,\n",
       " 'ojalá': 2,\n",
       " 'causa': 2,\n",
       " 'calidad': 2,\n",
       " 'vez': 2,\n",
       " 'interested': 2,\n",
       " 'went': 2,\n",
       " 'daughter': 2,\n",
       " 'funny': 2,\n",
       " 'correct': 2,\n",
       " 'vulfpeck': 2,\n",
       " 'things': 2,\n",
       " 'juntamos': 2,\n",
       " 'parque': 2,\n",
       " 'still': 2,\n",
       " 'info': 2,\n",
       " 'information': 2,\n",
       " 'car': 2,\n",
       " 'basically': 2,\n",
       " 'wonder': 2,\n",
       " 'czech': 2,\n",
       " 'side': 2,\n",
       " 'advance': 2,\n",
       " 'mine': 2,\n",
       " 'typically': 2,\n",
       " 'beers': 2,\n",
       " '7': 2,\n",
       " 'vivo': 2,\n",
       " 'días': 2,\n",
       " 'band': 2,\n",
       " 'play': 2,\n",
       " 'around': 2,\n",
       " 'guess': 2,\n",
       " 'thread': 2,\n",
       " 'yes': 2,\n",
       " 'telegram': 2,\n",
       " 'afrobeat': 2,\n",
       " 'tell': 2,\n",
       " 'exact': 2,\n",
       " 'video': 2,\n",
       " 'chilean': 2,\n",
       " 'president': 2,\n",
       " 'since': 2,\n",
       " 'actual': 2,\n",
       " 'many': 2,\n",
       " 'despicable': 2,\n",
       " 'always': 2,\n",
       " 'weed': 2,\n",
       " 'send': 2,\n",
       " 'adrian': 2,\n",
       " 'belew': 2,\n",
       " 'trio': 2,\n",
       " 'x': 2,\n",
       " 'king': 2,\n",
       " 'us': 2,\n",
       " 'help': 2,\n",
       " 'read': 2,\n",
       " \"'re\": 2,\n",
       " 'happens': 2,\n",
       " 'flies': 2,\n",
       " 'witches': 2,\n",
       " 'www.google.com/maps/dir/beverly+hills': 2,\n",
       " 'california': 2,\n",
       " 'usa/the+bronx': 2,\n",
       " 'new+york': 2,\n",
       " 'usa': 2,\n",
       " '35.9865814': 2,\n",
       " '114.2529463,4z/data': 2,\n",
       " '3m1': 2,\n",
       " '4b1': 2,\n",
       " '4m14': 2,\n",
       " '4m13': 2,\n",
       " '1s0x80c2bc04d6d147ab:0xd6c7c379fd081ed1': 2,\n",
       " '1d-118.4003563': 2,\n",
       " '2d34.0736204': 2,\n",
       " '1s0x89c28b553a697cb1:0x556e43a78ff15c77': 2,\n",
       " '1d-73.8648268': 2,\n",
       " '2d40.8447819': 2,\n",
       " '3e1': 2,\n",
       " 'acá': 2,\n",
       " 'tipo': 2,\n",
       " 'así': 2,\n",
       " 'fila': 2,\n",
       " 'demoré': 2,\n",
       " 'minutos': 2,\n",
       " 'enorme': 2,\n",
       " 'onda': 2,\n",
       " 'balance': 2,\n",
       " 'opinion': 2,\n",
       " 'knower': 2,\n",
       " 'v=zqlsmix03ng': 2,\n",
       " 'ok': 2,\n",
       " 'time': 2,\n",
       " 'pink': 2,\n",
       " 'floyd': 2,\n",
       " 'frame': 2,\n",
       " 'case': 2,\n",
       " 'talking': 2,\n",
       " 'vibe': 2,\n",
       " 'maybe': 2,\n",
       " 'ears': 2,\n",
       " 'wings': 2,\n",
       " 'place': 2,\n",
       " 'set': 2,\n",
       " 'kind': 2,\n",
       " 'order': 2,\n",
       " 'influences': 2,\n",
       " 'none': 2,\n",
       " 'much': 2,\n",
       " 'show': 2,\n",
       " 'youtube': 2,\n",
       " 'later': 2,\n",
       " 'gente': 2,\n",
       " 'twitter.com/ibranaber/status/1314441249089355776': 2,\n",
       " 'puerto': 2,\n",
       " 'elefante': 2,\n",
       " 'general': 2,\n",
       " 'fela': 2,\n",
       " 'kuti': 2,\n",
       " 'contemporary': 2,\n",
       " 'jungle': 2,\n",
       " 'run': 2,\n",
       " 'beer': 2,\n",
       " 'ustedes': 2,\n",
       " 'siempre': 2,\n",
       " 'advice': 2,\n",
       " 'actually': 2,\n",
       " 'post': 2,\n",
       " 'reply': 2,\n",
       " 'especially': 2,\n",
       " 'ana': 2,\n",
       " 'though': 2,\n",
       " 'original': 2,\n",
       " 'guitar': 2,\n",
       " 'month': 2,\n",
       " 'soul': 2,\n",
       " 'tocar': 2,\n",
       " 'elegir': 2,\n",
       " 'feedback': 2,\n",
       " 'jamallama': 2,\n",
       " 'jeo': 2,\n",
       " 'na': 2,\n",
       " 'connections': 2,\n",
       " 'made': 2,\n",
       " 'styles': 2,\n",
       " 'give': 2,\n",
       " 'main': 2,\n",
       " 'starting': 2,\n",
       " 'ending': 2,\n",
       " 'latin': 2,\n",
       " 'folk': 2,\n",
       " 'electronic': 2,\n",
       " 'hop': 2,\n",
       " 'free': 2,\n",
       " '€': 2,\n",
       " 'headphone': 2,\n",
       " 'works': 2,\n",
       " 'wired': 2,\n",
       " 'neutral': 2,\n",
       " 'budget': 2,\n",
       " 'gear': 2,\n",
       " 'mode': 2,\n",
       " 'preferred': 2,\n",
       " 'everything': 2,\n",
       " 'highly': 2,\n",
       " 'festival': 2,\n",
       " 'evening': 2,\n",
       " 'problem': 2,\n",
       " 'speakers': 2,\n",
       " 'every': 2,\n",
       " 'happened': 2,\n",
       " '11': 2,\n",
       " 'appreciate': 2,\n",
       " 'btc': 2,\n",
       " 'cryptomarket': 2,\n",
       " 'increase': 2,\n",
       " 'rent': 2,\n",
       " 'year': 2,\n",
       " 'indefinite': 2,\n",
       " 'user': 2,\n",
       " 'two': 2,\n",
       " 'teacher': 2,\n",
       " 'spanish': 2,\n",
       " 'suggestions': 2,\n",
       " 'recycling': 2,\n",
       " 'bottles': 2,\n",
       " 'supermarkets': 2,\n",
       " 'landlord': 1,\n",
       " 'rarely': 1,\n",
       " 'refuse': 1,\n",
       " 'elaborate': 1,\n",
       " 'duele': 1,\n",
       " 'loc': 1,\n",
       " 'relax': 1,\n",
       " 'quiero': 1,\n",
       " 'hacerte': 1,\n",
       " 'entender': 1,\n",
       " 'salido': 1,\n",
       " 'podría': 1,\n",
       " 'tener': 1,\n",
       " 'opinión': 1,\n",
       " 'aunque': 1,\n",
       " 'intuición': 1,\n",
       " 'allá': 1,\n",
       " 'año': 1,\n",
       " 'quejado': 1,\n",
       " 'relacionada': 1,\n",
       " 'www.reddit.com/r/chile/comments/inrd1j/met': 1,\n",
       " 'ad\\\\_panes\\\\_al\\\\_azar\\\\_bien\\\\_piola\\\\_en\\\\_la\\\\_bolsa\\\\_y\\\\_dio/g49fuvc': 1,\n",
       " 'voy': 1,\n",
       " 'comparar': 1,\n",
       " 'algún': 1,\n",
       " 'país': 1,\n",
       " 'salir': 1,\n",
       " 'vale': 1,\n",
       " 'hectáreas': 1,\n",
       " 'callampa': 1,\n",
       " 'viajado': 1,\n",
       " 'vivir': 1,\n",
       " 'afuera': 1,\n",
       " 'alumbrarse': 1,\n",
       " 'único': 1,\n",
       " 'importante': 1,\n",
       " 'cierta': 1,\n",
       " 'experiencia': 1,\n",
       " 'ayuda': 1,\n",
       " 'afirmar': 1,\n",
       " 'aseveración': 1,\n",
       " 'sellos': 1,\n",
       " 'azúcar': 1,\n",
       " 'buen': 1,\n",
       " 'primer': 1,\n",
       " 'paso': 1,\n",
       " 'falta': 1,\n",
       " 'viviendo': 1,\n",
       " '8': 1,\n",
       " 'puta': 1,\n",
       " 'producen': 1,\n",
       " 'wns': 1,\n",
       " 'hundan': 1,\n",
       " 'desaparezcan': 1,\n",
       " 'pronto': 1,\n",
       " 'nuevo': 1,\n",
       " 'cuestionamiento': 1,\n",
       " 'cómo': 1,\n",
       " 'mejorar': 1,\n",
       " 'disminuir': 1,\n",
       " 'precios': 1,\n",
       " 'aberrantes': 1,\n",
       " 'subsidios': 1,\n",
       " 'estatales': 1,\n",
       " 'buena': 1,\n",
       " 'tal': 1,\n",
       " 'dura': 1,\n",
       " 'lol': 1,\n",
       " 'rave.dj/dlcbmlg9umukga': 1,\n",
       " 'probably': 1,\n",
       " 'chili': 1,\n",
       " 'carne': 1,\n",
       " 'briefly': 1,\n",
       " 'teepeland': 1,\n",
       " 'haha': 1,\n",
       " 'coincidence': 1,\n",
       " 'answer': 1,\n",
       " 'dean': 1,\n",
       " 'town': 1,\n",
       " 'noticed': 1,\n",
       " 'full': 1,\n",
       " 'debut': 1,\n",
       " 'escribes': 1,\n",
       " 'coordinamos': 1,\n",
       " 'jammear': 1,\n",
       " 'además': 1,\n",
       " 'buscando': 1,\n",
       " 'vocalista': 1,\n",
       " 'bandas': 1,\n",
       " 'fair': 1,\n",
       " 'twice': 1,\n",
       " 'months': 1,\n",
       " 'quite': 1,\n",
       " 'functionally': 1,\n",
       " 'justmusic': 1,\n",
       " 'reopens': 1,\n",
       " 'tomorrow': 1,\n",
       " 'relevant': 1,\n",
       " 'camping': 1,\n",
       " 'camp': 1,\n",
       " 'scenario': 1,\n",
       " 'official': 1,\n",
       " 'prices': 1,\n",
       " 'including': 1,\n",
       " '1312': 1,\n",
       " 'squats': 1,\n",
       " 'left': 1,\n",
       " 'wing': 1,\n",
       " 'bars': 1,\n",
       " 'favorite': 1,\n",
       " 'viví': 1,\n",
       " 'santiago': 1,\n",
       " '6': 1,\n",
       " 'quieres': 1,\n",
       " 'charlar': 1,\n",
       " 'chelear': 1,\n",
       " 'puedo': 1,\n",
       " 'complementar': 1,\n",
       " 'información': 1,\n",
       " 'redditores': 1,\n",
       " 'locales': 1,\n",
       " 'dado': 1,\n",
       " 'puedes': 1,\n",
       " 'mandar': 1,\n",
       " 'mensaje': 1,\n",
       " \"'ll\": 1,\n",
       " 'jamming': 1,\n",
       " 'boxi': 1,\n",
       " 'come': 1,\n",
       " '19:00': 1,\n",
       " 'yeah': 1,\n",
       " '90': 1,\n",
       " 'minutes': 1,\n",
       " 'ca': 1,\n",
       " 'joke': 1,\n",
       " 'yet': 1,\n",
       " 'igual': 1,\n",
       " 'humor': 1,\n",
       " 'r/whoosh': 1,\n",
       " 'en-tien-do': 1,\n",
       " 'entendemos': 1,\n",
       " 'jaja': 1,\n",
       " 'desmerecer': 1,\n",
       " 'entretenido': 1,\n",
       " 'interesante': 1,\n",
       " 'historia': 1,\n",
       " 'entiendes': 1,\n",
       " 'nft': 1,\n",
       " 'es.wikipedia.org/wiki/token\\\\_no\\\\_fungible': 1,\n",
       " 'es.wikipedia.org/wiki/token_no_fungible': 1,\n",
       " 'störtebeker': 1,\n",
       " 'atlantik': 1,\n",
       " 'ale': 1,\n",
       " 'thinking': 1,\n",
       " 'making': 1,\n",
       " 'discord': 1,\n",
       " 'replied': 1,\n",
       " 'aka': 1,\n",
       " 'stoner/music-lovers': 1,\n",
       " 'group': 1,\n",
       " 'newen': 1,\n",
       " 'funnier': 1,\n",
       " 'moment': 1,\n",
       " 'television': 1,\n",
       " 'cut': 1,\n",
       " 'says': 1,\n",
       " 'compartimos': 1,\n",
       " 'translates': 1,\n",
       " 'share': 1,\n",
       " 'hilarious': 1,\n",
       " 'known': 1,\n",
       " 'disgusting': 1,\n",
       " 'thief': 1,\n",
       " 'stole': 1,\n",
       " 'become': 1,\n",
       " 'millionaire': 1,\n",
       " 'en.wikipedia.org/wiki/sebasti': 1,\n",
       " 'a1n_pi': 1,\n",
       " 'b1era': 1,\n",
       " 'businesses': 1,\n",
       " 'life': 1,\n",
       " 'fucking': 1,\n",
       " 'burocracy': 1,\n",
       " 'planly': 1,\n",
       " 'stupid': 1,\n",
       " 'cool': 1,\n",
       " 'addict': 1,\n",
       " 'connosieur': 1,\n",
       " 'session': 1,\n",
       " \"'d\": 1,\n",
       " 'learn': 1,\n",
       " 'vegan': 1,\n",
       " 'recipes': 1,\n",
       " 'ad': 1,\n",
       " 'v=v4ww3uryrlg': 1,\n",
       " 'v=ylqeq4jqluk': 1,\n",
       " 'gathering': 1,\n",
       " 'contexto': 1,\n",
       " 'www.reddit.com/r/whitepeopletwitter/comments/989xz2/yeah\\\\_fuck\\\\_off\\\\_rebecca': 1,\n",
       " 'www.reddit.com/r/whitepeopletwitter/comments/989xz2/yeah_fuck_off_rebecca': 1,\n",
       " 'xq': 1,\n",
       " 'censuran': 1,\n",
       " 'nombre': 1,\n",
       " 'rebecca': 1,\n",
       " 'qlia': 1,\n",
       " 'wn': 1,\n",
       " 'ooooooooo': 1,\n",
       " 'twitter': 1,\n",
       " 'reqliao': 1,\n",
       " 'publico': 1,\n",
       " 'ctm': 1,\n",
       " 'hackeado': 1,\n",
       " 'mail': 1,\n",
       " 'ilegalmente': 1,\n",
       " 'publicaron': 1,\n",
       " 'weaita': 1,\n",
       " 'repite': 1,\n",
       " 'conmigo': 1,\n",
       " 'twiter': 1,\n",
       " 'rant': 1,\n",
       " 'día': 1,\n",
       " 'gracias': 1,\n",
       " 'atención': 1,\n",
       " 'gizzard': 1,\n",
       " 'lizard': 1,\n",
       " 'wizard': 1,\n",
       " 'www.setlist.fm/setlists/king-gizzard-and-the-lizard-wizard-23de1823.html': 1,\n",
       " 'columbiahalle': 1,\n",
       " 'join': 1,\n",
       " 'mieterverein': 1,\n",
       " 'talk': 1,\n",
       " 'lawyer': 1,\n",
       " 'paul': 1,\n",
       " 'newest': 1,\n",
       " 'artists': 1,\n",
       " 'inspiring': 1,\n",
       " 'millions': 1,\n",
       " 'forgot': 1,\n",
       " 'comment': 1,\n",
       " 'straightforward': 1,\n",
       " 'deal': 1,\n",
       " 'sarcasm': 1,\n",
       " 'seems': 1,\n",
       " 'trying': 1,\n",
       " 'lines': 1,\n",
       " 'simply': 1,\n",
       " 'projecting': 1,\n",
       " 'issues': 1,\n",
       " 'trolling': 1,\n",
       " 'edgy': 1,\n",
       " 'ignore': 1,\n",
       " 'gut': 1,\n",
       " 'overthink': 1,\n",
       " 'pfaueninsel': 1,\n",
       " 'jar': 1,\n",
       " 'alice': 1,\n",
       " 'chains': 1,\n",
       " 'cnn': 1,\n",
       " 'times': 1,\n",
       " 'bad': 1,\n",
       " 'website': 1,\n",
       " 'manso': 1,\n",
       " 'pique': 1,\n",
       " '5000': 1,\n",
       " 'km': 1,\n",
       " 'súper': 1,\n",
       " 'pendiente': 1,\n",
       " 'resultados': 1,\n",
       " 'online': 1,\n",
       " 'informarlos': 1,\n",
       " 'radio': 1,\n",
       " 'pueblo': 1,\n",
       " 'vives': 1,\n",
       " 'celebración': 1,\n",
       " 'bueno': 1,\n",
       " 'votaciones': 1,\n",
       " 'importantes': 1,\n",
       " 'siguientes': 1,\n",
       " 'ganó': 1,\n",
       " '95': 1,\n",
       " 'voté': 1,\n",
       " 'alemania': 1,\n",
       " 'llegué': 1,\n",
       " '11:00': 1,\n",
       " 'mediodía': 1,\n",
       " '50': 1,\n",
       " 'metros': 1,\n",
       " '20': 1,\n",
       " 'entrar': 1,\n",
       " 'consulado': 1,\n",
       " 'votar': 1,\n",
       " 'salí': 1,\n",
       " 'cuadras': 1,\n",
       " 'rajé': 1,\n",
       " 'esperar': 1,\n",
       " 'horas': 1,\n",
       " 'conteo': 1,\n",
       " 'votos': 1,\n",
       " 'mismo': 1,\n",
       " 'arrasa': 1,\n",
       " '61.69': 1,\n",
       " 'rechazo': 1,\n",
       " '38.31': 1,\n",
       " 'cmc': 1,\n",
       " '42.01': 1,\n",
       " 'cc': 1,\n",
       " '57.99': 1,\n",
       " 'floripondio': 1,\n",
       " 'virtually': 1,\n",
       " 'unknown': 1,\n",
       " 'godzillionarie': 1,\n",
       " 'negative': 1,\n",
       " 'v=s5bv5ppegl0': 1,\n",
       " 'list=pl-aje_d7tppw4uiz6d-batdlloiybgfll': 1,\n",
       " 'eeeeh': 1,\n",
       " 'nope': 1,\n",
       " 'facebook': 1,\n",
       " 'banned': 1,\n",
       " 'china': 1,\n",
       " 'chichichilelele': 1,\n",
       " 'ultra': 1,\n",
       " 'cringe': 1,\n",
       " 'grito': 1,\n",
       " 'decadas': 1,\n",
       " 'popular': 1,\n",
       " 'dias': 1,\n",
       " 'unpopular': 1,\n",
       " 'overtime': 1,\n",
       " 'v=gnemd17kyse': 1,\n",
       " 'mention': 1,\n",
       " 'fault': 1,\n",
       " 'extremely': 1,\n",
       " 'porcupine': 1,\n",
       " 'tree': 1,\n",
       " 'v=qecg-yl4ky8': 1,\n",
       " 'dogs': 1,\n",
       " 'v=4qa30qkryy8': 1,\n",
       " 'related': 1,\n",
       " 'mentioned': 1,\n",
       " '7empest': 1,\n",
       " 'v=9d2r69gvyz0': 1,\n",
       " 'crimson': 1,\n",
       " 'youtu.be/oxfsc9gybpa': 1,\n",
       " 't=67': 1,\n",
       " 'riffs': 1,\n",
       " 'sooooo': 1,\n",
       " 'tribute': 1,\n",
       " 'distinguish': 1,\n",
       " 'influence': 1,\n",
       " 'obviously': 1,\n",
       " 'examples': 1,\n",
       " 'end': 1,\n",
       " 'stuff': 1,\n",
       " 'third': 1,\n",
       " 'eye': 1,\n",
       " 'reflection': 1,\n",
       " 'marie': 1,\n",
       " '1': 1,\n",
       " '10,000': 1,\n",
       " 'czar': 1,\n",
       " 'cold': 1,\n",
       " 'dark': 1,\n",
       " 'north': 1,\n",
       " 'star': 1,\n",
       " 'remember': 1,\n",
       " 'pf': 1,\n",
       " 'controls': 1,\n",
       " 'heart': 1,\n",
       " 'sun': 1,\n",
       " 'careful': 1,\n",
       " 'axe': 1,\n",
       " 'eugene': 1,\n",
       " 'echoes': 1,\n",
       " 'welcome': 1,\n",
       " 'machine': 1,\n",
       " 'hey': 1,\n",
       " 'chronological': 1,\n",
       " 'anyway': 1,\n",
       " 'difficult': 1,\n",
       " 'compare': 1,\n",
       " 'piper': 1,\n",
       " 'gates': 1,\n",
       " 'dawn': 1,\n",
       " 'meddle': 1,\n",
       " 'years': 1,\n",
       " 'someone': 1,\n",
       " 'musicians': 1,\n",
       " 'logic': 1,\n",
       " 'applies': 1,\n",
       " 'shows': 1,\n",
       " 'agree': 1,\n",
       " 'less': 1,\n",
       " 'remotely': 1,\n",
       " 'avishai': 1,\n",
       " 'cohen': 1,\n",
       " 'never': 1,\n",
       " 'listened': 1,\n",
       " 'eh': 1,\n",
       " 'spectrum': 1,\n",
       " 'billy': 1,\n",
       " 'cobham': 1,\n",
       " 'louis': 1,\n",
       " 'cole': 1,\n",
       " 'project': 1,\n",
       " 'sesh': 1,\n",
       " 'sessions': 1,\n",
       " 'thank': 1,\n",
       " 'crack': 1,\n",
       " 'skye': 1,\n",
       " 'bien': 1,\n",
       " 'frialdad': 1,\n",
       " 'conociste': 1,\n",
       " 'debes': 1,\n",
       " 'sumar': 1,\n",
       " 'ciudad': 1,\n",
       " 'caótica': 1,\n",
       " 'temer': 1,\n",
       " 'dedíquense': 1,\n",
       " 'simplente': 1,\n",
       " 'disfrutar': 1,\n",
       " 'claro': 1,\n",
       " 'esperes': 1,\n",
       " 'tampoco': 1,\n",
       " 'vaya': 1,\n",
       " 'actitud': 1,\n",
       " 'pura': 1,\n",
       " 'vida': 1,\n",
       " 'calle': 1,\n",
       " 'vengan': 1,\n",
       " 'contactan': 1,\n",
       " 'tiempo': 1,\n",
       " 'costa': 1,\n",
       " 'rica': 1,\n",
       " 'lindas': 1,\n",
       " 'memorias': 1,\n",
       " 'context': 1,\n",
       " 'kurzgesagt': 1,\n",
       " 'forward': 1,\n",
       " 'ro': 1,\n",
       " 'watch': 1,\n",
       " 'compared': 1,\n",
       " 'documentary': 1,\n",
       " 'conocí': 1,\n",
       " 'descendiente': 1,\n",
       " 'kawésqar': 1,\n",
       " 'creo': 1,\n",
       " 'guardaparques': 1,\n",
       " 'nacional': 1,\n",
       " 'bernardo': 1,\n",
       " \"o'higgins\": 1,\n",
       " 'edén': 1,\n",
       " 'contaba': 1,\n",
       " 'tomaban': 1,\n",
       " 'cucharada': 1,\n",
       " 'aceite': 1,\n",
       " 'puro': 1,\n",
       " 'lobo': 1,\n",
       " 'después': 1,\n",
       " 'sumergían': 1,\n",
       " 'bucear': 1,\n",
       " 'desnudos': 1,\n",
       " 'aguas': 1,\n",
       " 'gélidas': 1,\n",
       " 'fuente': 1,\n",
       " 'www.facebook.com/radioautenticafmdepuertocisnes/videos/1253583628346466': 1,\n",
       " 'aparentemente': 1,\n",
       " 'maisha': 1,\n",
       " 'gogo': 1,\n",
       " 'penguin': 1,\n",
       " 'jacob': 1,\n",
       " 'collier': 1,\n",
       " 'mainly': 1,\n",
       " 'classics': 1,\n",
       " 'gentleman': 1,\n",
       " 'v=0tbwndhdowu': 1,\n",
       " 'v=jmtccus8l9o': 1,\n",
       " 'sounds': 1,\n",
       " 'v=ejftdshk-f0': 1,\n",
       " 'question': 1,\n",
       " 'right': 1,\n",
       " 'makes': 1,\n",
       " 'craiglist': 1,\n",
       " 'told': 1,\n",
       " 'thing': 1,\n",
       " 'pasa': 1,\n",
       " 'leen': 1,\n",
       " 'noticia': 1,\n",
       " 'portales': 1,\n",
       " 'extranjeros': 1,\n",
       " 'pasado': 1,\n",
       " 'noticias': 1,\n",
       " 'tono': 1,\n",
       " 'energy': 1,\n",
       " 'technology': 1,\n",
       " 'great': 1,\n",
       " 'suspicacias': 1,\n",
       " 'razón': 1,\n",
       " 'logro': 1,\n",
       " 'comprender': 1,\n",
       " 'alguien': 1,\n",
       " 'luces': 1,\n",
       " 'asunto': 1,\n",
       " 'agradezco': 1,\n",
       " 'short': 1,\n",
       " 'go': 1,\n",
       " 'schöneberg': 1,\n",
       " 'source': 1,\n",
       " 'bayerisches': 1,\n",
       " 'viertel': 1,\n",
       " 'drink': 1,\n",
       " 'move': 1,\n",
       " 'looks': 1,\n",
       " 'primus': 1,\n",
       " 'members': 1,\n",
       " 'v=aydfwujzyqg': 1,\n",
       " 'www.reddit.com/r/ifyoulikeblank/comments/iqh7oy/iil_larger_than_life_music_such_as_echoes_by_pink/g4t6g89': 1,\n",
       " 'utm_source=share': 1,\n",
       " 'utm_medium=web2x': 1,\n",
       " 'context=3': 1,\n",
       " 'schwarzes': 1,\n",
       " 'cafe': 1,\n",
       " 'simple': 1,\n",
       " 'hervirlos': 1,\n",
       " 'rato': 1,\n",
       " 'olla': 1,\n",
       " 'presión': 1,\n",
       " 'mejor': 1,\n",
       " 'piñones': 1,\n",
       " 'subterranean': 1,\n",
       " 'homesick': 1,\n",
       " 'alien': 1,\n",
       " 'radiohead': 1,\n",
       " \"c'mmon\": 1,\n",
       " 'hollywood': 1,\n",
       " 'movie': 1,\n",
       " 'center': 1,\n",
       " 'planet': 1,\n",
       " 'tetas': 1,\n",
       " 'mercedes': 1,\n",
       " 'call': 1,\n",
       " 'exactly': 1,\n",
       " 'protest': 1,\n",
       " 'singer': 1,\n",
       " 'vengo': 1,\n",
       " '2014': 1,\n",
       " 'v=s-zgljquqwm': 1,\n",
       " 'tijoux': 1,\n",
       " 'friend': 1,\n",
       " 'old': 1,\n",
       " 'laptop': 1,\n",
       " 'ignorance': 1,\n",
       " 'www.ebay-kleinanzeigen.de/s-anzeige/e-gitarre-les-paul/1814238758-74-3481': 1,\n",
       " 'madame': 1,\n",
       " 'tussauds': 1,\n",
       " 'dumps': 1,\n",
       " 'trump': 1,\n",
       " 'u.s': 1,\n",
       " 'election': 1,\n",
       " 'cisnes': 1,\n",
       " 'nearly': 1,\n",
       " 'beirut': 1,\n",
       " 'explosion': 1,\n",
       " 'rescue': 1,\n",
       " 'dog': 1,\n",
       " 'able': 1,\n",
       " 'presence': 1,\n",
       " 'heartbeat': 1,\n",
       " 'rubble': 1,\n",
       " 'miraculously': 1,\n",
       " 'save': 1,\n",
       " 'wounded': 1,\n",
       " 'trapped': 1,\n",
       " 'chilenas': 1,\n",
       " 'latinoamericanas': 1,\n",
       " 'extranjero': 1,\n",
       " 'tod': 1,\n",
       " 'invitaron': 1,\n",
       " 'cantar': 1,\n",
       " 'par': 1,\n",
       " 'pequeño': 1,\n",
       " 'concierto': 1,\n",
       " 'castellano': 1,\n",
       " 'supuesto': 1,\n",
       " 'formal': 1,\n",
       " 'guitarra': 1,\n",
       " 'europa': 1,\n",
       " 'hispanohablante': 1,\n",
       " 'cuesta': 1,\n",
       " 'imaginar': 1,\n",
       " 'chileno': 1,\n",
       " 'español': 1,\n",
       " 'latinoamericano': 1,\n",
       " 'fácil': 1,\n",
       " 'conoce': 1,\n",
       " 'letra': 1,\n",
       " 'populares': 1,\n",
       " 'letras': 1,\n",
       " 'relevantes': 1,\n",
       " 'ritmo': 1,\n",
       " 'melodía': 1,\n",
       " 'pasan': 1,\n",
       " 'ser': 1,\n",
       " 'primordial': 1,\n",
       " '¿qué': 1,\n",
       " 'canción': 1,\n",
       " 'elegirían': 1,\n",
       " 'vacilonas': 1,\n",
       " 'lentas': 1,\n",
       " 'representen': 1,\n",
       " 'forma': 1,\n",
       " 'cultura': 1,\n",
       " '¡gracias': 1,\n",
       " 'consejos': 1,\n",
       " 'real': 1,\n",
       " 'virtual': 1,\n",
       " 'hungry': 1,\n",
       " 'quick': 1,\n",
       " 'list': 1,\n",
       " 'comfortable': 1,\n",
       " 'categorizing': 1,\n",
       " 'brief': 1,\n",
       " 'idea': 1,\n",
       " 'combination': 1,\n",
       " 'try': 1,\n",
       " 'put': 1,\n",
       " 'genres': 1,\n",
       " 'ones': 1,\n",
       " 'progressive': 1,\n",
       " 'american': 1,\n",
       " 'alternative': 1,\n",
       " 'world': 1,\n",
       " 'grunge': 1,\n",
       " 'psychedelic': 1,\n",
       " 'nu': 1,\n",
       " 'tropicalia': 1,\n",
       " 'funk': 1,\n",
       " 'cumbia': 1,\n",
       " 'trip': 1,\n",
       " 'trash': 1,\n",
       " 'bossa': 1,\n",
       " 'nova': 1,\n",
       " 'avant-garde': 1,\n",
       " 'classic': 1,\n",
       " 'flamenco': 1,\n",
       " 'krautrock': 1,\n",
       " 'dub': 1,\n",
       " 'space': 1,\n",
       " 'nueva': 1,\n",
       " 'cancion': 1,\n",
       " 'african': 1,\n",
       " 'bebop': 1,\n",
       " 'bolero': 1,\n",
       " 'manouche': 1,\n",
       " 'reggae': 1,\n",
       " 'acid': 1,\n",
       " 'experimental': 1,\n",
       " 'punk': 1,\n",
       " 'electroacoustic': 1,\n",
       " 'classical': 1,\n",
       " 'idm': 1,\n",
       " 'groove': 1,\n",
       " 'andean': 1,\n",
       " 'art': 1,\n",
       " 'trance': 1,\n",
       " 'balkan': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maur0',\n",
       " 'puntastic_name',\n",
       " 'navtaq',\n",
       " 'alexb599',\n",
       " 'og_han',\n",
       " 'cYzzie',\n",
       " 'PM-me-ur-kittenz',\n",
       " 'h-u-g-o',\n",
       " 'satalana',\n",
       " 'kachol',\n",
       " 'magallanes2010',\n",
       " 'OsoGuti',\n",
       " 'FunLovinMonotreme',\n",
       " 'rainman_104',\n",
       " 'vu67',\n",
       " 'upka',\n",
       " 'ancdefghi',\n",
       " 'Melonemelo123',\n",
       " 'Kawtcho',\n",
       " 'Javiercdx',\n",
       " 'Irresponsible_Tune',\n",
       " 'Deneb0la',\n",
       " 'AntonioZamorano58',\n",
       " 'Tierrrez',\n",
       " '93WhiteStrat',\n",
       " 'chicosapo',\n",
       " 'musicatito',\n",
       " 'PhilipJay99',\n",
       " 'ivan_xd',\n",
       " 'badwives',\n",
       " 'flashcatcher',\n",
       " 'JMG_99',\n",
       " 'its_mango_time',\n",
       " 'NNorAl',\n",
       " 'CVirus',\n",
       " 'butchYbutch_',\n",
       " 'magezt',\n",
       " 'DirtyProtest',\n",
       " 'JaLogoJa',\n",
       " 'longanizas',\n",
       " 'TheRealWeedAtman',\n",
       " 'Tiger_Mann',\n",
       " 'ky2k',\n",
       " 'raverbashing',\n",
       " 'gramoun-kal',\n",
       " 'rebelrebel2013',\n",
       " 'Phrodo_00',\n",
       " 'frenchliner',\n",
       " 'Mugen_1212',\n",
       " 'AutoModerator',\n",
       " 'weaweonaaweonao',\n",
       " 'ohravenyouneverlearn',\n",
       " 'flrianjst',\n",
       " 'NuQ',\n",
       " 'vectorpropio',\n",
       " 'Just-me-fmCR',\n",
       " 'markzlz',\n",
       " 'sandiaazucar',\n",
       " 'maialen09',\n",
       " 'Schtiglitz',\n",
       " 'restoreprivacydotcom',\n",
       " 'lilo910',\n",
       " 'jsnaomi6',\n",
       " 'patiperro_v3',\n",
       " 'saproxilico',\n",
       " 'CashmereShiv',\n",
       " 'ComfortablyFloyd',\n",
       " 'kwanbix',\n",
       " 'warmans',\n",
       " 'Afterlifehappydeath',\n",
       " 'idontbrowseaww',\n",
       " 'burupie',\n",
       " 'meep_42',\n",
       " 'mambita',\n",
       " 'danggupta90',\n",
       " 'paulmccartney',\n",
       " 'Peyotine',\n",
       " 'KDamage',\n",
       " 'SkyNet03',\n",
       " 'n1c0_ds',\n",
       " 'EuSouAssimReddit',\n",
       " 'salomown',\n",
       " 'Kid_In_Sandbox',\n",
       " 'Yuseiger',\n",
       " 'Dynaparte',\n",
       " 'Iyon_Tichy',\n",
       " 'FookenZyzzBrah',\n",
       " 'Relayedroid',\n",
       " 'Hard_Rain_Falling',\n",
       " 'Lo_Innombrable',\n",
       " 'Manomana-cl',\n",
       " 'underground_Luau',\n",
       " 'Ok-Aerie-6822',\n",
       " 'covidparis',\n",
       " 'jabonkagigi',\n",
       " 'HombreContrafactual',\n",
       " 'kinte',\n",
       " 'Lopr1621',\n",
       " 'uffjedn',\n",
       " 'rakeguitar',\n",
       " 'acid8k',\n",
       " 'verycrafty',\n",
       " 'godless-life',\n",
       " 'Cuntable',\n",
       " 'dxtp',\n",
       " 'Rodrigoarc',\n",
       " 'Trubinio',\n",
       " 'gerrypoliteandcunty',\n",
       " 'Aquarium-Luxor',\n",
       " 'Mayhzon',\n",
       " 'KennyTurbo',\n",
       " 'ElFlaco2',\n",
       " 'ruincreep',\n",
       " 'Belodri',\n",
       " 'Tukurito',\n",
       " 'Naldrek',\n",
       " 'BlackViperMWG',\n",
       " 'brent_mused',\n",
       " 'cimocw',\n",
       " 'AstaS-',\n",
       " 'gazaehl',\n",
       " 'EB3031',\n",
       " 'Iatrogenia',\n",
       " 'jotanova',\n",
       " 'IamYodaBot',\n",
       " 'Milo0192',\n",
       " 'FortePianoForte',\n",
       " 'eeeeonflux',\n",
       " 'tresclow',\n",
       " 'LJMcMillan',\n",
       " 'pweepweemuggins',\n",
       " 'TitoDantito',\n",
       " 'kingdraven',\n",
       " 'BunyipPouch',\n",
       " 'iushiush',\n",
       " 'possi1',\n",
       " 'IronStrange7',\n",
       " 'JohnAvi',\n",
       " 'saoirsecaoilfhoinn',\n",
       " 'bold_and_brash_trash',\n",
       " 'xzinik',\n",
       " 'SevenOfSpadess',\n",
       " 's_berliner',\n",
       " 'ghsgjgfngngf',\n",
       " 'breathepls',\n",
       " 'Keanu_Norris',\n",
       " 'alimuzammil1998',\n",
       " 'will1707',\n",
       " 'Skrondo87',\n",
       " 'lulaloops',\n",
       " 'GreenSpectr3',\n",
       " 'dummdumm92',\n",
       " 'pdonoso']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all groups and keywords for the associated users\n",
    "\n",
    "We automatize now the previous procedure to obtain **groups** and **keywords** for every **user** in `users_list`, and save them in a Python dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_keywords_dict(user, the_headers):\n",
    "\n",
    "    posts_full_text = \"\"\n",
    "    groups_list = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/comments\",\n",
    "                                        headers = the_headers,\n",
    "                                        params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = None\n",
    "            while my_headers is None:\n",
    "                try:\n",
    "                    # connect\n",
    "                    my_headers = headers_connection_request()\n",
    "                except:\n",
    "                     pass\n",
    "    try:\n",
    "        for post in res_comments.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['body']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/submitted\",\n",
    "                                         headers = the_headers,\n",
    "                                         params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = None\n",
    "            while my_headers is None:\n",
    "                try:\n",
    "                    # connect\n",
    "                    my_headers = headers_connection_request()\n",
    "                except:\n",
    "                     pass\n",
    "    try:\n",
    "        for post in res_submitted.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['title']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "            if post['data']['selftext']:\n",
    "                posts_full_text += \" \" + post['data']['selftext']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "\n",
    "    corpus_text = posts_full_text.lower()\n",
    "    keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "\n",
    "    return groups_dict, keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_headers = headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Maur0\n",
      "1 puntastic_name\n",
      "2 navtaq\n",
      "ConnectionError, trying again...\n",
      "3 alexb599\n",
      "4 og_han\n",
      "5 cYzzie\n",
      "6 PM-me-ur-kittenz\n",
      "7 h-u-g-o\n",
      "8 satalana\n",
      "9 kachol\n",
      "10 magallanes2010\n",
      "11 OsoGuti\n",
      "12 FunLovinMonotreme\n",
      "13 rainman_104\n",
      "14 vu67\n",
      "15 upka\n",
      "16 ancdefghi\n",
      "17 Melonemelo123\n",
      "18 Kawtcho\n",
      "19 Javiercdx\n",
      "20 Irresponsible_Tune\n",
      "21 Deneb0la\n",
      "22 AntonioZamorano58\n",
      "23 Tierrrez\n",
      "24 93WhiteStrat\n",
      "25 chicosapo\n",
      "26 musicatito\n",
      "27 PhilipJay99\n",
      "28 ivan_xd\n",
      "ConnectionError, trying again...\n",
      "29 badwives\n",
      "30 flashcatcher\n",
      "31 JMG_99\n",
      "32 its_mango_time\n",
      "33 NNorAl\n",
      "34 CVirus\n",
      "35 butchYbutch_\n",
      "36 magezt\n",
      "37 DirtyProtest\n",
      "38 JaLogoJa\n",
      "39 longanizas\n",
      "40 TheRealWeedAtman\n",
      "41 Tiger_Mann\n",
      "42 ky2k\n",
      "43 raverbashing\n",
      "44 gramoun-kal\n",
      "45 rebelrebel2013\n",
      "46 Phrodo_00\n",
      "47 frenchliner\n",
      "48 Mugen_1212\n",
      "49 AutoModerator\n",
      "50 weaweonaaweonao\n",
      "51 ohravenyouneverlearn\n",
      "52 flrianjst\n",
      "53 NuQ\n",
      "54 vectorpropio\n",
      "55 Just-me-fmCR\n",
      "56 markzlz\n",
      "57 sandiaazucar\n",
      "58 maialen09\n",
      "59 Schtiglitz\n",
      "60 restoreprivacydotcom\n",
      "61 lilo910\n",
      "62 jsnaomi6\n",
      "63 patiperro_v3\n",
      "64 saproxilico\n",
      "65 CashmereShiv\n",
      "66 ComfortablyFloyd\n",
      "67 kwanbix\n",
      "68 warmans\n",
      "69 Afterlifehappydeath\n",
      "70 idontbrowseaww\n",
      "71 burupie\n",
      "72 meep_42\n",
      "73 mambita\n",
      "74 danggupta90\n",
      "75 paulmccartney\n",
      "76 Peyotine\n",
      "77 KDamage\n",
      "78 SkyNet03\n",
      "79 n1c0_ds\n",
      "80 EuSouAssimReddit\n",
      "81 salomown\n",
      "82 Kid_In_Sandbox\n",
      "83 Yuseiger\n",
      "84 Dynaparte\n",
      "85 Iyon_Tichy\n",
      "86 FookenZyzzBrah\n",
      "87 Relayedroid\n",
      "88 Hard_Rain_Falling\n",
      "89 Lo_Innombrable\n",
      "90 Manomana-cl\n",
      "91 underground_Luau\n",
      "92 Ok-Aerie-6822\n",
      "93 covidparis\n",
      "94 jabonkagigi\n",
      "95 HombreContrafactual\n",
      "96 kinte\n",
      "97 Lopr1621\n",
      "98 uffjedn\n",
      "99 rakeguitar\n",
      "100 acid8k\n",
      "101 verycrafty\n",
      "102 godless-life\n",
      "103 Cuntable\n",
      "104 dxtp\n",
      "105 Rodrigoarc\n",
      "106 Trubinio\n",
      "107 gerrypoliteandcunty\n",
      "108 Aquarium-Luxor\n",
      "ConnectionError, trying again...\n",
      "109 Mayhzon\n",
      "110 KennyTurbo\n",
      "111 ElFlaco2\n",
      "112 ruincreep\n",
      "113 Belodri\n",
      "114 Tukurito\n",
      "115 Naldrek\n",
      "116 BlackViperMWG\n",
      "117 brent_mused\n",
      "118 cimocw\n",
      "119 AstaS-\n",
      "120 gazaehl\n",
      "121 EB3031\n",
      "122 Iatrogenia\n",
      "123 jotanova\n",
      "124 IamYodaBot\n",
      "125 Milo0192\n",
      "126 FortePianoForte\n",
      "127 eeeeonflux\n",
      "128 tresclow\n",
      "129 LJMcMillan\n",
      "130 pweepweemuggins\n",
      "131 TitoDantito\n",
      "132 kingdraven\n",
      "133 BunyipPouch\n",
      "134 iushiush\n",
      "135 possi1\n",
      "136 IronStrange7\n",
      "137 JohnAvi\n",
      "138 saoirsecaoilfhoinn\n",
      "139 bold_and_brash_trash\n",
      "140 xzinik\n",
      "141 SevenOfSpadess\n",
      "142 s_berliner\n",
      "143 ghsgjgfngngf\n",
      "144 breathepls\n",
      "145 Keanu_Norris\n",
      "146 alimuzammil1998\n",
      "147 will1707\n",
      "148 Skrondo87\n",
      "149 lulaloops\n",
      "150 GreenSpectr3\n",
      "151 dummdumm92\n",
      "152 pdonoso\n"
     ]
    }
   ],
   "source": [
    "final_users_groups_keywords_dict = {}\n",
    "for i, user in enumerate(users_list):\n",
    "    print(i, user)\n",
    "    gkd = groups_keywords_dict(user, my_headers)\n",
    "    final_users_groups_keywords_dict[user] = {}\n",
    "    final_users_groups_keywords_dict[user]['groups'] = gkd[0]\n",
    "    final_users_groups_keywords_dict[user]['keywords'] = gkd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groups': {'chile': 102,\n",
       "  'asklatinamerica': 10,\n",
       "  'Music': 7,\n",
       "  'trees': 4,\n",
       "  'Jazz': 3,\n",
       "  'bizarrebuildings': 2,\n",
       "  'UserExperienceDesign': 2,\n",
       "  'Showerthoughts': 2,\n",
       "  'askscience': 2,\n",
       "  'AskReddit': 2,\n",
       "  'MusicaEnEspanol': 2,\n",
       "  'WTFMusicVideos': 2,\n",
       "  'woof_irl': 1,\n",
       "  'Santiago': 1,\n",
       "  'randomactsofmusic': 1,\n",
       "  'RedditForGrownups': 1,\n",
       "  'icecreamery': 1,\n",
       "  'delusionalartists': 1,\n",
       "  'LatinoPeopleTwitter': 1,\n",
       "  'MovieDetails': 1,\n",
       "  'coolguides': 1,\n",
       "  'GardeningIndoors': 1,\n",
       "  'toptalent': 1,\n",
       "  'worldnews': 1,\n",
       "  'LatinAmerica': 1,\n",
       "  'firewater': 1,\n",
       "  'AskHistorians': 1,\n",
       "  'beatles': 1,\n",
       "  'Jimi_Hendrix': 1,\n",
       "  'helpmewin': 1,\n",
       "  'retiredgif': 1,\n",
       "  'HomeworkHelp': 1,\n",
       "  'shittyaskscience': 1,\n",
       "  'Lollapalooza': 1,\n",
       "  'drums': 1,\n",
       "  'woahdudemusic': 1,\n",
       "  'gaming': 1,\n",
       "  'Marijuana': 1},\n",
       " 'keywords': {'si': 15,\n",
       "  'really': 14,\n",
       "  'gin': 10,\n",
       "  'ser': 10,\n",
       "  'mundo': 10,\n",
       "  '’': 9,\n",
       "  'love': 8,\n",
       "  'creo': 8,\n",
       "  'hacer': 8,\n",
       "  'help': 8,\n",
       "  'like': 7,\n",
       "  'weon': 7,\n",
       "  'good': 7,\n",
       "  'get': 7,\n",
       "  'cosas': 7,\n",
       "  'gastos': 6,\n",
       "  'mal': 6,\n",
       "  'voy': 6,\n",
       "  'alguien': 6,\n",
       "  'time': 6,\n",
       "  'vida': 6,\n",
       "  'know': 6,\n",
       "  'country': 6,\n",
       "  'encuentro': 6,\n",
       "  'jazz': 6,\n",
       "  'people': 5,\n",
       "  'world': 5,\n",
       "  'pa': 5,\n",
       "  'new': 5,\n",
       "  'gracias': 5,\n",
       "  'wea': 5,\n",
       "  'chile': 5,\n",
       "  'acá': 5,\n",
       "  'would': 5,\n",
       "  'puede': 5,\n",
       "  'hace': 5,\n",
       "  'cine': 5,\n",
       "  'mejor': 5,\n",
       "  'sub': 5,\n",
       "  'cabros': 5,\n",
       "  'gusta': 5,\n",
       "  'teni': 5,\n",
       "  'r/chile': 5,\n",
       "  'little': 5,\n",
       "  'something': 4,\n",
       "  'reservados': 4,\n",
       "  'dejo': 4,\n",
       "  'ahora': 4,\n",
       "  'aprender': 4,\n",
       "  'gente': 4,\n",
       "  'make': 4,\n",
       "  'try': 4,\n",
       "  'chico': 4,\n",
       "  'bien': 4,\n",
       "  'libro': 4,\n",
       "  'ficción': 4,\n",
       "  'lot': 4,\n",
       "  'could': 4,\n",
       "  'quieren': 4,\n",
       "  'raja': 4,\n",
       "  'reddit': 4,\n",
       "  'palta': 4,\n",
       "  'going': 4,\n",
       "  'one': 4,\n",
       "  'want': 4,\n",
       "  'sistema': 4,\n",
       "  'va': 4,\n",
       "  'poder': 4,\n",
       "  'pasar': 4,\n",
       "  'bueno': 4,\n",
       "  'chocolate': 4,\n",
       "  \"'s\": 4,\n",
       "  '¿que': 4,\n",
       "  'removed': 4,\n",
       "  'remember': 4,\n",
       "  'couple': 4,\n",
       "  'band': 4,\n",
       "  'http': 4,\n",
       "  'www.socialab.com/ideas/imaginachile/2578': 4,\n",
       "  'thanks': 4,\n",
       "  'wanted': 3,\n",
       "  'bad': 3,\n",
       "  'knows': 3,\n",
       "  'true': 3,\n",
       "  'go': 3,\n",
       "  'ningún': 3,\n",
       "  'lado': 3,\n",
       "  'villalobos': 3,\n",
       "  'información': 3,\n",
       "  'nunca': 3,\n",
       "  'llama': 3,\n",
       "  'echo': 3,\n",
       "  'explicado': 3,\n",
       "  'comentario': 3,\n",
       "  'medio': 3,\n",
       "  'solo': 3,\n",
       "  'momento': 3,\n",
       "  'hago': 3,\n",
       "  'edit': 3,\n",
       "  'igual': 3,\n",
       "  '2': 3,\n",
       "  'sorry': 3,\n",
       "  'años': 3,\n",
       "  'cuenta': 3,\n",
       "  'cómo': 3,\n",
       "  'tiempo': 3,\n",
       "  'primer': 3,\n",
       "  'ciencia': 3,\n",
       "  'dont': 3,\n",
       "  'talk': 3,\n",
       "  'stray': 3,\n",
       "  'dogs': 3,\n",
       "  'care': 3,\n",
       "  'well': 3,\n",
       "  'linda': 3,\n",
       "  'todas': 3,\n",
       "  'base': 3,\n",
       "  'pan': 3,\n",
       "  'real': 3,\n",
       "  'derecha': 3,\n",
       "  'ver': 3,\n",
       "  'leer': 3,\n",
       "  'cada': 3,\n",
       "  'idea': 3,\n",
       "  'post': 3,\n",
       "  'texto': 3,\n",
       "  'creatividad': 3,\n",
       "  'pensar': 3,\n",
       "  'quizás': 3,\n",
       "  'barcos': 3,\n",
       "  'van': 3,\n",
       "  'armar': 3,\n",
       "  'buenos': 3,\n",
       "  'plata': 3,\n",
       "  'nacional': 3,\n",
       "  'ustedes': 3,\n",
       "  'opinan': 3,\n",
       "  'historical': 3,\n",
       "  'cualquier': 3,\n",
       "  'dedicado': 3,\n",
       "  \"'m\": 3,\n",
       "  'libros': 3,\n",
       "  'pueda': 3,\n",
       "  'religión': 3,\n",
       "  'life': 3,\n",
       "  'radiohead': 3,\n",
       "  'tan': 3,\n",
       "  'cover': 3,\n",
       "  'years': 3,\n",
       "  'please': 3,\n",
       "  'normal': 3,\n",
       "  \"n't\": 3,\n",
       "  'porn': 3,\n",
       "  'nugs': 3,\n",
       "  'briones': 2,\n",
       "  'zorra': 2,\n",
       "  'argentina': 2,\n",
       "  'seguro': 2,\n",
       "  'jenever': 2,\n",
       "  'feel': 2,\n",
       "  'culiao': 2,\n",
       "  'every': 2,\n",
       "  'producer': 2,\n",
       "  'ever': 2,\n",
       "  'used': 2,\n",
       "  'drink': 2,\n",
       "  'became': 2,\n",
       "  'santiago': 2,\n",
       "  'distillery': 2,\n",
       "  'send': 2,\n",
       "  'adress': 2,\n",
       "  'inbox': 2,\n",
       "  'give': 2,\n",
       "  'nop': 2,\n",
       "  'procesando': 2,\n",
       "  'uso': 2,\n",
       "  'indebidos': 2,\n",
       "  'quieras': 2,\n",
       "  'leído': 2,\n",
       "  'hizo': 2,\n",
       "  'problema': 2,\n",
       "  'disculpas': 2,\n",
       "  'caso': 2,\n",
       "  'dicho': 2,\n",
       "  'propia': 2,\n",
       "  'gobierno': 2,\n",
       "  'atención': 2,\n",
       "  'aweonao': 2,\n",
       "  'súper': 2,\n",
       "  'trying': 2,\n",
       "  'chilean': 2,\n",
       "  'parte': 2,\n",
       "  'kast': 2,\n",
       "  'encuestas': 2,\n",
       "  'muchas': 2,\n",
       "  'intentando': 2,\n",
       "  'jaja': 2,\n",
       "  'literalmente': 2,\n",
       "  'escribir': 2,\n",
       "  'value': 2,\n",
       "  'say': 2,\n",
       "  'violeta': 2,\n",
       "  'según': 2,\n",
       "  'compre': 2,\n",
       "  'recomendación': 2,\n",
       "  'syrup': 2,\n",
       "  'jajaja': 2,\n",
       "  'mano': 2,\n",
       "  'local': 2,\n",
       "  'imagino': 2,\n",
       "  'buena': 2,\n",
       "  'comprar': 2,\n",
       "  'disfrutaba': 2,\n",
       "  'alternativas': 2,\n",
       "  'introduce': 2,\n",
       "  'amiga': 2,\n",
       "  'principio': 2,\n",
       "  'general': 2,\n",
       "  'taken': 2,\n",
       "  'national': 2,\n",
       "  '“': 2,\n",
       "  '”': 2,\n",
       "  'even': 2,\n",
       "  'many': 2,\n",
       "  'owners': 2,\n",
       "  'venga': 2,\n",
       "  'need': 2,\n",
       "  'todavía': 2,\n",
       "  'parece': 2,\n",
       "  'gone': 2,\n",
       "  'carne': 2,\n",
       "  'cruda': 2,\n",
       "  'dormir': 2,\n",
       "  'tomate': 2,\n",
       "  'comer': 2,\n",
       "  'mente': 2,\n",
       "  'abierta': 2,\n",
       "  'pro': 2,\n",
       "  'fin': 2,\n",
       "  'lleno': 2,\n",
       "  'together': 2,\n",
       "  '6': 2,\n",
       "  'hour': 2,\n",
       "  'coordinate': 2,\n",
       "  'centro': 2,\n",
       "  'tirado': 2,\n",
       "  'habla': 2,\n",
       "  'pico': 2,\n",
       "  'selecta': 2,\n",
       "  'wena': 2,\n",
       "  'entonces': 2,\n",
       "  'hospital': 2,\n",
       "  'tener': 2,\n",
       "  'salud': 2,\n",
       "  'seguir': 2,\n",
       "  'falta': 2,\n",
       "  'anything': 2,\n",
       "  'belt': 2,\n",
       "  'digo': 2,\n",
       "  'vas': 2,\n",
       "  'perspectiva': 2,\n",
       "  'tipo': 2,\n",
       "  'sensación': 2,\n",
       "  'saber': 2,\n",
       "  'ibas': 2,\n",
       "  'carrete': 2,\n",
       "  'siento': 2,\n",
       "  'grado': 2,\n",
       "  'educacion': 2,\n",
       "  'habilidades': 2,\n",
       "  'trabajo': 2,\n",
       "  'viene': 2,\n",
       "  'recomiendo': 2,\n",
       "  'cerebro': 2,\n",
       "  'personas': 2,\n",
       "  'distintas': 2,\n",
       "  'intereses': 2,\n",
       "  'manos': 2,\n",
       "  'felices': 2,\n",
       "  'buen': 2,\n",
       "  'tecnología': 2,\n",
       "  'aunque': 2,\n",
       "  'ejemplo': 2,\n",
       "  'estructura': 2,\n",
       "  'gustan': 2,\n",
       "  'minas': 2,\n",
       "  'nadie': 2,\n",
       "  'educación': 2,\n",
       "  'disciplinas': 2,\n",
       "  'empezar': 2,\n",
       "  'hacerlo': 2,\n",
       "  'después': 2,\n",
       "  'estudiar': 2,\n",
       "  'podi': 2,\n",
       "  'dirección': 2,\n",
       "  'nuevas': 2,\n",
       "  'tomar': 2,\n",
       "  'riesgos': 2,\n",
       "  'explorar': 2,\n",
       "  'weas': 2,\n",
       "  'mierda': 2,\n",
       "  '100': 2,\n",
       "  'salir': 2,\n",
       "  'place': 2,\n",
       "  'work': 2,\n",
       "  'verdad': 2,\n",
       "  'lugar': 2,\n",
       "  'today': 2,\n",
       "  'días': 2,\n",
       "  'nueva': 2,\n",
       "  'definir': 2,\n",
       "  'gen': 2,\n",
       "  'z': 2,\n",
       "  'podrían': 2,\n",
       "  'cuarentena': 2,\n",
       "  'alguno': 2,\n",
       "  'spoon': 2,\n",
       "  '¿cuál': 2,\n",
       "  'cabeza': 2,\n",
       "  'mezcla': 2,\n",
       "  'minutos': 2,\n",
       "  'antenas': 2,\n",
       "  '5g': 2,\n",
       "  'cocinar': 2,\n",
       "  'caleta': 2,\n",
       "  'full': 2,\n",
       "  'mushrooms': 2,\n",
       "  'done': 2,\n",
       "  'vacaciones': 2,\n",
       "  'quiero': 2,\n",
       "  'funny': 2,\n",
       "  'moment': 2,\n",
       "  'english': 2,\n",
       "  'unir': 2,\n",
       "  'patria': 2,\n",
       "  'historia': 2,\n",
       "  'respeto': 2,\n",
       "  'ocurra': 2,\n",
       "  'song': 2,\n",
       "  '¿alguien': 2,\n",
       "  'end': 2,\n",
       "  'learn': 2,\n",
       "  'sudor': 2,\n",
       "  'olor': 2,\n",
       "  'asi': 2,\n",
       "  'listening': 2,\n",
       "  'great': 2,\n",
       "  'ago': 2,\n",
       "  'don´t': 2,\n",
       "  'name': 2,\n",
       "  'feat': 2,\n",
       "  'ayuda': 2,\n",
       "  'sabe': 2,\n",
       "  'recuperar': 2,\n",
       "  'computador': 2,\n",
       "  'paper': 2,\n",
       "  'family': 2,\n",
       "  'mas': 2,\n",
       "  'porfa': 2,\n",
       "  'historica': 2,\n",
       "  'voten': 2,\n",
       "  'proyecto': 2,\n",
       "  'video': 2,\n",
       "  'facebook': 2,\n",
       "  'start': 2,\n",
       "  'beautiful': 2,\n",
       "  'distribution': 2,\n",
       "  'places': 2,\n",
       "  'listen': 2,\n",
       "  'must': 2,\n",
       "  'see': 2,\n",
       "  'us': 2,\n",
       "  'told': 2,\n",
       "  'high': 2,\n",
       "  'think': 2,\n",
       "  'r/trees': 2,\n",
       "  'much': 2,\n",
       "  'weed': 2,\n",
       "  'first': 2,\n",
       "  'purple': 2,\n",
       "  'cogollos': 2,\n",
       "  'spanish': 2,\n",
       "  'kind': 2,\n",
       "  'thats': 2,\n",
       "  'big': 2,\n",
       "  'trees': 2,\n",
       "  'pretty': 2,\n",
       "  'happening': 2,\n",
       "  'around': 2,\n",
       "  'marijuana': 2,\n",
       "  'debate': 2,\n",
       "  'corazón': 1,\n",
       "  'dice': 1,\n",
       "  'llendo': 1,\n",
       "  'amistad': 1,\n",
       "  'peru': 1,\n",
       "  'gamba': 1,\n",
       "  'compro': 1,\n",
       "  'copesa': 1,\n",
       "  'oye': 1,\n",
       "  'emboque': 1,\n",
       "  'weeena': 1,\n",
       "  'nati': 1,\n",
       "  'sound': 1,\n",
       "  'condescending': 1,\n",
       "  'reinforce': 1,\n",
       "  'important': 1,\n",
       "  'halo': 1,\n",
       "  'mysticism': 1,\n",
       "  'sacred': 1,\n",
       "  'beverage': 1,\n",
       "  'amo': 1,\n",
       "  'yeah': 1,\n",
       "  'story': 1,\n",
       "  'always': 1,\n",
       "  'hear': 1,\n",
       "  'means': 1,\n",
       "  'courage': 1,\n",
       "  'water': 1,\n",
       "  'dutch': 1,\n",
       "  'soldiers': 1,\n",
       "  'battle': 1,\n",
       "  'fearce': 1,\n",
       "  'strong': 1,\n",
       "  'come': 1,\n",
       "  'message': 1,\n",
       "  'write': 1,\n",
       "  'leíste': 1,\n",
       "  'monto': 1,\n",
       "  'puedo': 1,\n",
       "  'apostar': 1,\n",
       "  'haz': 1,\n",
       "  'bruno': 1,\n",
       "  'encuentras': 1,\n",
       "  'reconocer': 1,\n",
       "  'error': 1,\n",
       "  'pedir': 1,\n",
       "  'asignación': 1,\n",
       "  'personajes': 1,\n",
       "  'l': 1,\n",
       "  'blanco': 1,\n",
       "  'hagan': 1,\n",
       "  'quejó': 1,\n",
       "  'puta': 1,\n",
       "  'pesao': 1,\n",
       "  'inútil': 1,\n",
       "  'pendejo': 1,\n",
       "  'mañoso': 1,\n",
       "  'diciendo': 1,\n",
       "  'pasos': 1,\n",
       "  'función': 1,\n",
       "  'importante': 1,\n",
       "  'intrested': 1,\n",
       "  'open': 1,\n",
       "  'markets': 1,\n",
       "  'netherlands': 1,\n",
       "  'valle': 1,\n",
       "  'sombras': 1,\n",
       "  'downvoteado': 1,\n",
       "  'muerte': 1,\n",
       "  'utilizados': 1,\n",
       "  'andean': 1,\n",
       "  'futurism': 1,\n",
       "  'close': 1,\n",
       "  'cigar': 1,\n",
       "  'pasará': 1,\n",
       "  'politicos': 1,\n",
       "  'elegido': 1,\n",
       "  'deja': 1,\n",
       "  'par': 1,\n",
       "  'pasa': 1,\n",
       "  'elite': 1,\n",
       "  'sube': 1,\n",
       "  'cago': 1,\n",
       "  'mala': 1,\n",
       "  'persona': 1,\n",
       "  'clínica': 1,\n",
       "  'vives': 1,\n",
       "  'demando': 1,\n",
       "  'jefes': 1,\n",
       "  'médicos': 1,\n",
       "  'aparentemente': 1,\n",
       "  'equivoqué': 1,\n",
       "  'pido': 1,\n",
       "  'formales': 1,\n",
       "  'webada': 1,\n",
       "  'entienda': 1,\n",
       "  'conversación': 1,\n",
       "  '—————': 1,\n",
       "  'espérate': 1,\n",
       "  'reclamando': 1,\n",
       "  'cobraron': 1,\n",
       "  'arriendo': 1,\n",
       "  'ventiladores': 1,\n",
       "  'clínicas': 1,\n",
       "  'obligando': 1,\n",
       "  'hospitalizar': 1,\n",
       "  'tratar': 1,\n",
       "  'atiende': 1,\n",
       "  'fonasa': 1,\n",
       "  'manera': 1,\n",
       "  'gratuita': 1,\n",
       "  'noticia': 1,\n",
       "  'obvio': 1,\n",
       "  'cobrarán': 1,\n",
       "  'dedos': 1,\n",
       "  'frente': 1,\n",
       "  'meal': 1,\n",
       "  'centennials': 1,\n",
       "  'kinky': 1,\n",
       "  'bisexual': 1,\n",
       "  'millenials': 1,\n",
       "  'nothing': 1,\n",
       "  'add': 1,\n",
       "  'milenial': 1,\n",
       "  'title': 1,\n",
       "  'read': 1,\n",
       "  'dado': 1,\n",
       "  'encontráis': 1,\n",
       "  'harto': 1,\n",
       "  'carney': 1,\n",
       "  'piso': 1,\n",
       "  'razón': 1,\n",
       "  'totally': 1,\n",
       "  'maple': 1,\n",
       "  'hotcakes': 1,\n",
       "  'shitty': 1,\n",
       "  'walmart': 1,\n",
       "  'canadian': 1,\n",
       "  'candy': 1,\n",
       "  'sapo': 1,\n",
       "  '20.000': 1,\n",
       "  'distancia': 1,\n",
       "  'gorreó': 1,\n",
       "  'ogu': 1,\n",
       "  'definición': 1,\n",
       "  'chaotic': 1,\n",
       "  'valores': 1,\n",
       "  'siempre': 1,\n",
       "  'delante': 1,\n",
       "  'términos': 1,\n",
       "  'animación': 1,\n",
       "  'cómic': 1,\n",
       "  'surge': 1,\n",
       "  'sabemos': 1,\n",
       "  'valorar': 1,\n",
       "  'pena': 1,\n",
       "  'tanta': 1,\n",
       "  'semilla': 1,\n",
       "  'regalada': 1,\n",
       "  'nación': 1,\n",
       "  'sol': 1,\n",
       "  'naciente': 1,\n",
       "  'kink': 1,\n",
       "  'shame': 1,\n",
       "  'erai': 1,\n",
       "  'varios': 1,\n",
       "  'caímos': 1,\n",
       "  'tentación': 1,\n",
       "  'hyperion': 1,\n",
       "  'agarré': 1,\n",
       "  'saga': 1,\n",
       "  'dune': 1,\n",
       "  'hacia': 1,\n",
       "  'leí': 1,\n",
       "  'pienso': 1,\n",
       "  'ahí': 1,\n",
       "  'fascinación': 1,\n",
       "  'realidades': 1,\n",
       "  'themo': 1,\n",
       "  'lobos': 1,\n",
       "  'marco': 1,\n",
       "  'rena': 1,\n",
       "  'amor': 1,\n",
       "  'platonico': 1,\n",
       "  'consider': 1,\n",
       "  'singing': 1,\n",
       "  'karaoke': 1,\n",
       "  'random': 1,\n",
       "  'else': 1,\n",
       "  'mood': 1,\n",
       "  'topic': 1,\n",
       "  'autos': 1,\n",
       "  'teles': 1,\n",
       "  'grandes': 1,\n",
       "  'tele': 1,\n",
       "  '50': 1,\n",
       "  'pulgadas': 1,\n",
       "  '00': 1,\n",
       "  'íbamos': 1,\n",
       "  'casa': 1,\n",
       "  'sentía': 1,\n",
       "  'funcionaba': 1,\n",
       "  'proyector': 1,\n",
       "  'interno': 1,\n",
       "  'anda': 1,\n",
       "  'pasear': 1,\n",
       "  'vaca': 1,\n",
       "  'huaso': 1,\n",
       "  'turned': 1,\n",
       "  'racists': 1,\n",
       "  'hard': 1,\n",
       "  'fast': 1,\n",
       "  'boric': 1,\n",
       "  'perdió': 1,\n",
       "  'voto': 1,\n",
       "  'unique': 1,\n",
       "  'relationship': 1,\n",
       "  'along': 1,\n",
       "  'friendly': 1,\n",
       "  'qultros': 1,\n",
       "  'mixed': 1,\n",
       "  'race': 1,\n",
       "  'live': 1,\n",
       "  'street': 1,\n",
       "  'fed': 1,\n",
       "  'clothes': 1,\n",
       "  'instead': 1,\n",
       "  'decided': 1,\n",
       "  'take': 1,\n",
       "  'condenar': 1,\n",
       "  'violencia': 1,\n",
       "  'jaaaa': 1,\n",
       "  'caraaaa': 1,\n",
       "  'church': 1,\n",
       "  'andan': 1,\n",
       "  'proponiendo': 1,\n",
       "  'wish': 1,\n",
       "  'equipment': 1,\n",
       "  'krieger': 1,\n",
       "  'postulando': 1,\n",
       "  'cargo': 1,\n",
       "  'público': 1,\n",
       "  'vincula': 1,\n",
       "  'directamente': 1,\n",
       "  'mira': 1,\n",
       "  'pasando': 1,\n",
       "  'udi': 1,\n",
       "  'torrealba': 1,\n",
       "  'controlando': 1,\n",
       "  'presidentes': 1,\n",
       "  'amlo': 1,\n",
       "  'populist': 1,\n",
       "  'mr': 1,\n",
       "  'popo': 1,\n",
       "  'yes': 1,\n",
       "  'shutter': 1,\n",
       "  'fights': 1,\n",
       "  'sesgo': 1,\n",
       "  'soa': 1,\n",
       "  'cavernícola': 1,\n",
       "  'caído': 1,\n",
       "  'anoche': 1,\n",
       "  'hice': 1,\n",
       "  'kilo': 1,\n",
       "  'kubbe': 1,\n",
       "  'dormí': 1,\n",
       "  'alérgico': 1,\n",
       "  'ocurre': 1,\n",
       "  'ensalada': 1,\n",
       "  'acidez': 1,\n",
       "  'aquellas': 1,\n",
       "  'prácticamente': 1,\n",
       "  'vivo': 1,\n",
       "  'picked': 1,\n",
       "  'comida': 1,\n",
       "  'fermentada': 1,\n",
       "  'menos': 1,\n",
       "  'naturaleza': 1,\n",
       "  'dialogante': 1,\n",
       "  'vote': 1,\n",
       "  'honorable': 1,\n",
       "  'ignominiosos': 1,\n",
       "  'abajo': 1,\n",
       "  'separa': 1,\n",
       "  'evitando': 1,\n",
       "  'moje': 1,\n",
       "  'friends': 1,\n",
       "  'use': 1,\n",
       "  'phrases': 1,\n",
       "  'let': 1,\n",
       "  '7': 1,\n",
       "  'least': 1,\n",
       "  'late': 1,\n",
       "  'collectively': 1,\n",
       "  'decide': 1,\n",
       "  'early': 1,\n",
       "  'punctual': 1,\n",
       "  'unrealistic': 1,\n",
       "  'manuel': 1,\n",
       "  'montt': 1,\n",
       "  'parriba': 1,\n",
       "  'moderado': 1,\n",
       "  'rr.ss': 1,\n",
       "  'tendiente': 1,\n",
       "  'pendulo': 1,\n",
       "  'mueve': 1,\n",
       "  'izq': 1,\n",
       "  'actualmente': 1,\n",
       "  'hablar': 1,\n",
       "  'vizio': 1,\n",
       "  'harina': 1,\n",
       "  'whaaat': 1,\n",
       "  'dude': 1,\n",
       "  'julia': 1,\n",
       "  'robertas': 1,\n",
       "  'brother': 1,\n",
       "  'relacionadas': 1,\n",
       "  'quede': 1,\n",
       "  'disculpa': 1,\n",
       "  'ok': 1,\n",
       "  'niño': 1,\n",
       "  'muera': 1,\n",
       "  'sacan': 1,\n",
       "  'estacionamientos': 1,\n",
       "  'mall': 1,\n",
       "  'puedes': 1,\n",
       "  'siendo': 1,\n",
       "  'ética': 1,\n",
       "  'descomunal': 1,\n",
       "  'intolerable': 1,\n",
       "  'undibitably': 1,\n",
       "  'smaller': 1,\n",
       "  'micro': 1,\n",
       "  'skirt': 1,\n",
       "  'malabar': 1,\n",
       "  'welp': 1,\n",
       "  'unsubscribe': 1,\n",
       "  'hate': 1,\n",
       "  'tittle': 1,\n",
       "  'falto': 1,\n",
       "  'trousers': 1,\n",
       "  'susto': 1,\n",
       "  'hijos': 1,\n",
       "  'peligro': 1,\n",
       "  'llamo': 1,\n",
       "  'sebastián': 1,\n",
       "  'denver': 1,\n",
       "  'tornado': 1,\n",
       "  'tun': 1,\n",
       "  'ávila': 1,\n",
       "  'sexo': 1,\n",
       "  'amateur': 1,\n",
       "  'diego': 1,\n",
       "  'lorenzini': 1,\n",
       "  'castillos': 1,\n",
       "  'aire': 1,\n",
       "  'jazzimodo': 1,\n",
       "  'orgullo': 1,\n",
       "  'moral': 1,\n",
       "  'distraída': 1,\n",
       "  'upvoteo': 1,\n",
       "  'asco': 1,\n",
       "  'metete': 1,\n",
       "  'curso': 1,\n",
       "  'taller': 1,\n",
       "  'llame': 1,\n",
       "  'clases': 1,\n",
       "  'dibujo': 1,\n",
       "  'teoría': 1,\n",
       "  'musical': 1,\n",
       "  'filosofía': 1,\n",
       "  'único': 1,\n",
       "  'dispuesto': 1,\n",
       "  'incómodo': 1,\n",
       "  'pocas': 1,\n",
       "  'relaciones': 1,\n",
       "  'fluyen': 1,\n",
       "  'ves': 1,\n",
       "  'incomodo': 1,\n",
       "  'escapas': 1,\n",
       "  'desconocido': 1,\n",
       "  'amigos': 1,\n",
       "  'carretes': 1,\n",
       "  'algún': 1,\n",
       "  'inesperado': 1,\n",
       "  'iba': 1,\n",
       "  'conocer': 1,\n",
       "  'terminar': 1,\n",
       "  'convertirse': 1,\n",
       "  'usar': 1,\n",
       "  'degrade': 1,\n",
       "  'banderas': 1,\n",
       "  'rechazada': 1,\n",
       "  'profesor': 1,\n",
       "  'universitario': 1,\n",
       "  'prestado': 1,\n",
       "  'magister': 1,\n",
       "  'líneas': 1,\n",
       "  'suficiente': 1,\n",
       "  'establecer': 1,\n",
       "  'tonto': 1,\n",
       "  'aconsejo': 1,\n",
       "  'documento': 1,\n",
       "  'siglo': 1,\n",
       "  'xxi': 1,\n",
       "  'unesco': 1,\n",
       "  'delinia': 1,\n",
       "  'relevancia': 1,\n",
       "  'apertura': 1,\n",
       "  'interdisciplinario': 1,\n",
       "  'exitosos': 1,\n",
       "  'futuro': 1,\n",
       "  'ted': 1,\n",
       "  'schools': 1,\n",
       "  'kill': 1,\n",
       "  'creativity': 1,\n",
       "  'explica': 1,\n",
       "  'refiere': 1,\n",
       "  'escultor': 1,\n",
       "  'cuerpo': 1,\n",
       "  'bailarina': 1,\n",
       "  'nariz': 1,\n",
       "  'chef': 1,\n",
       "  'ayudar': 1,\n",
       "  'productivos': 1,\n",
       "  'aporte': 1,\n",
       "  'ridiculisacion': 1,\n",
       "  'computación': 1,\n",
       "  'telar': 1,\n",
       "  'introducción': 1,\n",
       "  'disfrutan': 1,\n",
       "  'trabajar': 1,\n",
       "  'considero': 1,\n",
       "  'educacional': 1,\n",
       "  'dar': 1,\n",
       "  'espacios': 1,\n",
       "  'desarrollo': 1,\n",
       "  'experimentación': 1,\n",
       "  'afectar': 1,\n",
       "  'convertiríamos': 1,\n",
       "  'exportación': 1,\n",
       "  'fácil': 1,\n",
       "  'mandan': 1,\n",
       "  'lados': 1,\n",
       "  'cagada': 1,\n",
       "  'país': 1,\n",
       "  'excepto': 1,\n",
       "  'subiendo': 1,\n",
       "  'volver': 1,\n",
       "  'europa': 1,\n",
       "  'legalización': 1,\n",
       "  'necesaria': 1,\n",
       "  'pregunta': 1,\n",
       "  'sacamos': 1,\n",
       "  'legaliza': 1,\n",
       "  'resto': 1,\n",
       "  'existiendo': 1,\n",
       "  'aquí': 1,\n",
       "  'legal': 1,\n",
       "  'podremos': 1,\n",
       "  'viajar': 1,\n",
       "  'territorios': 1,\n",
       "  'incluyendo': 1,\n",
       "  'luna': 1,\n",
       "  'claaaro': 1,\n",
       "  'mentality': 1,\n",
       "  'superiority': 1,\n",
       "  'making': 1,\n",
       "  'china': 1,\n",
       "  'fiesta': 1,\n",
       "  'caguines': 1,\n",
       "  'traslucen': 1,\n",
       "  'churrines': 1,\n",
       "  'lee': 1,\n",
       "  'tweet': 1,\n",
       "  'cabro': 1,\n",
       "  'quedó': 1,\n",
       "  'inteligente': 1,\n",
       "  'salar': 1,\n",
       "  'uyuni': 1,\n",
       "  'cojones': 1,\n",
       "  'aceptación': 1,\n",
       "  'entorno': 1,\n",
       "  'permita': 1,\n",
       "  'interesadas': 1,\n",
       "  'diferentes': 1,\n",
       "  'rubro': 1,\n",
       "  'desarrollarse': 1,\n",
       "  'practicar': 1,\n",
       "  'quiere': 1,\n",
       "  'guiones': 1,\n",
       "  'encontrar': 1,\n",
       "  'espacio': 1,\n",
       "  '18': 1,\n",
       "  'literatura': 1,\n",
       "  'clubes': 1,\n",
       "  'colegios': 1,\n",
       "  'pocos': 1,\n",
       "  'lugares': 1,\n",
       "  'haberlo': 1,\n",
       "  'extraprogramaticamente': 1,\n",
       "  'profes': 1,\n",
       "  'tampoco': 1,\n",
       "  'mismo': 1,\n",
       "  'podríamos': 1,\n",
       "  'decir': 1,\n",
       "  'actuación': 1,\n",
       "  'fotografía': 1,\n",
       "  'arte': 1,\n",
       "  'vestuario': 1,\n",
       "  'iluminación': 1,\n",
       "  'necesitan': 1,\n",
       "  'película': 1,\n",
       "  'además': 1,\n",
       "  'cultura': 1,\n",
       "  'amateurismo': 1,\n",
       "  'enseñan': 1,\n",
       "  'probar': 1,\n",
       "  'atreverse': 1,\n",
       "  'personales': 1,\n",
       "  'simplemente': 1,\n",
       "  'importa': 1,\n",
       "  'resultado': 1,\n",
       "  'malo': 1,\n",
       "  'ganar': 1,\n",
       "  'último': 1,\n",
       "  'dejar': 1,\n",
       "  'limitar': 1,\n",
       "  'través': 1,\n",
       "  'hablo': 1,\n",
       "  'niñes': 1,\n",
       "  'jóvenes': 1,\n",
       "  'tranquilidad': 1,\n",
       "  'laboral': 1,\n",
       "  'libertad': 1,\n",
       "  'atrevernos': 1,\n",
       "  'malos': 1,\n",
       "  'paso': 1,\n",
       "  'equipo': 1,\n",
       "  'poca': 1,\n",
       "  'creadores': 1,\n",
       "  'tapar': 1,\n",
       "  'directores': 1,\n",
       "  'izquierda': 1,\n",
       "  'opuesto': 1,\n",
       "  'libertario': 1,\n",
       "  'comentaba': 1,\n",
       "  'daban': 1,\n",
       "  'bonos': 1,\n",
       "  'mentía': 1,\n",
       "  'peruvian': 1,\n",
       "  'restaurant': 1,\n",
       "  'conveyor': 1,\n",
       "  'kramer': 1,\n",
       "  'sichel': 1,\n",
       "  'nuevamente': 1,\n",
       "  'pego': 1,\n",
       "  'palo': 1,\n",
       "  'gato': 1,\n",
       "  'asamblea': 1,\n",
       "  'deliberación': 1,\n",
       "  'pública': 1,\n",
       "  'iniciativa': 1,\n",
       "  'popular': 1,\n",
       "  'constituyente': 1,\n",
       "  'giovanna': 1,\n",
       "  'grandón': 1,\n",
       "  'tía': 1,\n",
       "  'pikachu': 1,\n",
       "  'entrega': 1,\n",
       "  'propuestas': 1,\n",
       "  'convención': 1,\n",
       "  'cristian': 1,\n",
       "  'cuturrufo': 1,\n",
       "  'iconic': 1,\n",
       "  'trumpet': 1,\n",
       "  'player': 1,\n",
       "  'opened': 1,\n",
       "  'mind': 1,\n",
       "  'ears': 1,\n",
       "  'beauty': 1,\n",
       "  'died': 1,\n",
       "  'covid': 1,\n",
       "  'complications': 1,\n",
       "  'age': 1,\n",
       "  '49': 1,\n",
       "  'cristián': 1,\n",
       "  'extrañaremos': 1,\n",
       "  'apesta': 1,\n",
       "  'acuesto': 1,\n",
       "  'haber': 1,\n",
       "  'revisado': 1,\n",
       "  'mañana': 1,\n",
       "  'contenido': 1,\n",
       "  'nuevo': 1,\n",
       "  'weones': 1,\n",
       "  'durmiendo': 1,\n",
       "  'actualizado': 1,\n",
       "  'niuna': 1,\n",
       "  'super': 1,\n",
       "  'revisada': 1,\n",
       "  'matutina': 1,\n",
       "  'trono': 1,\n",
       "  'di': 1,\n",
       "  'saludos': 1,\n",
       "  'chiquilines': 1,\n",
       "  'pregunto': 1,\n",
       "  'directo': 1,\n",
       "  'definen': 1,\n",
       "  ...}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_users_groups_keywords_dict['pdonoso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
