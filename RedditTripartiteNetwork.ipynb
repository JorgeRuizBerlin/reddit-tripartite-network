{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design and construction of tripartite network from Reddit\n",
    "\n",
    "Datasets from multipartite complex networks with 3 or more levels (tripartite, quadripartite, etc.) are very scarce, unlike the case of only 2 levels better known as bipartite graphs, which are quite common.\n",
    "\n",
    "I designed and began to construct a tripartite network for my Ph.D. thesis, using the website [Reddit](https://www.reddit.com). According to their own description, \"*Reddit is a network of communities where people can dive into their interests, hobbies and passions. There's a community for whatever you're interested in on Reddit*\". In this context, I use the term *groups* instead of *communities* for technical reasons and to avoid misunderstandings.\n",
    "\n",
    "The tripartite network I defined is composed of:\n",
    "1. **Users** (usernames)\n",
    "2. **Groups** (subreddits)\n",
    "3. **Keywords** (words)\n",
    "\n",
    "My main interest is the tripartite network analysis in two important topics:\n",
    "* **Link prediction**. This can be used in recommendation systems for example, so we could recommend an user certain groups that might find interesting based on our anaylsis.\n",
    "* **Community detection**. Also called clustering in (sligthly) different contexts, and it can be used to detect clusters of users based on the groups they frecuent and the keyword they use, for instance.\n",
    "\n",
    "I already developed many algorithms to do **link prediction** and **community detection** in multipartite networks, but I was lacking of datasets to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community\n",
    "from pyvis import network as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Reddit API you should have first a Reddit account and\n",
    "# sign up for an OAUTH Client ID in https://www.reddit.com/prefs/apps\n",
    "# and at the page bottom click on: \"are you a developer? create an app...\"\n",
    "# https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\n",
    "\n",
    "my_username = 'tripartitenetwork' #account created only for this purpose\n",
    "my_password = '987654321reddit123456789'\n",
    "\n",
    "personal_use_script = 'jVFLZzCvn9H82rRg_M_O1w'\n",
    "secret = 'djzraeUgBxE5U-BKirzY7OG9RQm7_w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_connection_request():\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(personal_use_script, secret)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': 'password',\n",
    "            'username': my_username,\n",
    "            'password': my_password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the first call to headers_connection_request() doesn't work, we need a while loop\n",
    "def Headers():\n",
    "    my_headers = None\n",
    "    while my_headers is None:\n",
    "        try: # try until connects and therefore initialize the process\n",
    "            my_headers = headers_connection_request()\n",
    "            return my_headers\n",
    "        except:\n",
    "             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyBot/0.0.1',\n",
       " 'Authorization': 'bearer 1206362233968-5smKYQmr7gCB4580Lre3CO-d_8uS7A'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "my_headers = Headers()\n",
    "my_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The starting point is any Reddit username, it's the only input we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'zip759' #'urbannomadberlin' #'GovSchwarzenegger'\n",
    "my_limit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) We start extracting all the words used from our specific user, and simultaneously, the groups where they were posted\n",
    "\n",
    "We describe every text that a certain **user** writes (publicly) as a *post*. Hence, calling the Reddit API we indentify two main types of *posts* and some more subtypes:\n",
    "\n",
    "1. `comment`\n",
    "\n",
    "\n",
    "2. `submitted`\n",
    "\n",
    "    i. `title`\n",
    "    \n",
    "    ii. `selftext` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) We extract the keywords from comments and the subreddits where they were posted.\n",
    "\n",
    "We extract the **keywords** from every `comment` *post*, every `title` of a `submitted` *post*, and optionally from the `selftext` of a `submitted` post, if any. Then we saved all of them in a common string `posts_full_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_full_text = \"\"\n",
    "groups_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/comments\",\n",
    "                                    headers = my_headers,\n",
    "                                    params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_comments.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['body']\n",
    "    groups_list.append(post['data']['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Extracting keywords from submitted title, and from submitted selftext, if any, and the subreddits where they were posted.\n",
    "\n",
    "At the same time, we will append the subreddits, i.e. the **groups** where every *post* belongs, in a list called `groups_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/submitted\",\n",
    "                                     headers = my_headers,\n",
    "                                     params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['title']\n",
    "    groups_list.append(post['data']['subreddit'])\n",
    "    if post['data']['selftext']:\n",
    "        posts_full_text += \" \" + post['data']['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having all the groups where a user posted we make a very simple analysis of them.\n",
    "\n",
    "We count the **groups** repetitions and save them as a Python dictionary `groups_dict`. This will help us later to associate every **group** with its respective **user**, where the associated value will correspond to the link weight of the newly defined bipartite **user-groups** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "#groups_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After retrieving all of the user posts keywords, we start to analyze them using the simplest approach: the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The intention is to improve this analysis later with methods such as n-grams or more sophisticaed ones within the natural language processing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_text = posts_full_text.lower()\n",
    "#corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download if necessary!\n",
    "#nltk.download('punkt') #download if necessary!\n",
    "\n",
    "stopwords_e = nltk.corpus.stopwords.words('english')\n",
    "stopwords_g = nltk.corpus.stopwords.words('german')\n",
    "stopwords_s = nltk.corpus.stopwords.words('spanish') #add languages if needed\n",
    "stopwords = stopwords_e + stopwords_g + stopwords_s\n",
    "\n",
    "mystopwords = [\"a\", \"a's\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \n",
    "                \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \n",
    "                \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \n",
    "                \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \n",
    "                \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \n",
    "                \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \n",
    "                \"aren't\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \n",
    "                \"available\", \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\", \n",
    "                \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \n",
    "                \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \n",
    "                \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \n",
    "                \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \n",
    "                \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \n",
    "                \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \n",
    "                \"contains\", \"corresponding\", \"could\", \"couldn't\", \"course\", \"currently\", \n",
    "                \"d\", \"definitely\", \"described\", \"despite\", \"did\", \"didn't\", \"different\", \n",
    "                \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\", \n",
    "                \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \n",
    "                \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \n",
    "                \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \n",
    "                \"example\", \"except\", \"f\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \n",
    "                \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \n",
    "                \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\", \"getting\", \n",
    "                \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \n",
    "                \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn't\", \n",
    "                \"have\", \"haven't\", \"having\", \"he\", \"he's\", \"hello\", \"help\", \"hence\", \"her\", \n",
    "                \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \n",
    "                \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \n",
    "                \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\", \"if\", \n",
    "                \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \n",
    "                \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \n",
    "                \"is\", \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \n",
    "                \"k\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\", \n",
    "                \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \n",
    "                \"let's\", \"like\", \"liked\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \n",
    "                \"ltd\", \"m\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \n",
    "                \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \n",
    "                \"my\", \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \n",
    "                \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \n",
    "                \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \n",
    "                \"nothing\", \"novel\", \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \n",
    "                \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \n",
    "                \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \n",
    "                \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"p\", \"particular\", \n",
    "                \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \n",
    "                \"presumably\", \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\", \n",
    "                \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \n",
    "                \"regards\", \"relatively\", \"respectively\", \"right\", \"s\", \"said\", \"same\", \n",
    "                \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \n",
    "                \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \n",
    "                \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \n",
    "                \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \n",
    "                \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \n",
    "                \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \n",
    "                \"such\", \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \n",
    "                \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\", \"thats\", \"the\", \n",
    "                \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \n",
    "                \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \n",
    "                \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \n",
    "                \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \n",
    "                \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \n",
    "                \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \n",
    "                \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \n",
    "                \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \n",
    "                \"using\", \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\", \"viz\", \n",
    "                \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\", \n",
    "                \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \n",
    "                \"what's\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \n",
    "                \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \n",
    "                \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \n",
    "                \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \n",
    "                \"without\", \"won't\", \"wonder\", \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \n",
    "                \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \n",
    "                \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"] #complete with words to exclude if necessary\n",
    "\n",
    "stopwords += mystopwords\n",
    "\n",
    "def common_words(text): # isalpha() optional for words made of only letters \n",
    "    return [word for word in TextBlob(text).words if word not in stopwords]# and word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the most common words as a Python dictionary `keywords_dict`, will help us later to associate every **keyword** with its respective **user**, where the associated value will correspond to the link weight of the newly defined **user-keywords** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) We continue extracting, for our specific input user, all the associated users.\n",
    "\n",
    "In principle, this is not really necessary. Since we already have the basic code to extract all the **groups** and **keywords** for any specific **user**, we could do the same procedure for any arbitrary list of Reddit usernames. But it would make absolute sense to search for **users** connected somehow to our input **user**, and we will find them with a similar approach to the previous one, retrieving our input **user** information. Once we obtain all the **users** associated to our input **user**, we applied to them the full procedure describe in **(A)** to obtain their respective **groups** and **keywords**, and having this we'll have all the needed information to construct our tripartite network. Other different Reddit usernames can be also added manually at any point to expand the network even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) For any given input user and from its submitted posts, we extract the users from the direct replies (first children) to any of them.\n",
    "\n",
    "We save all the associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    name = post['data']['name']\n",
    "    while True:\n",
    "        try:\n",
    "            res_name = requests.get(\"https://oauth.reddit.com\" + \"/comments\" + \"/\" + name[3:] + \"/api\"\n",
    "                                    + \"/morechildren\",\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    for comment in res_name.json()[1]['data']['children']:\n",
    "        if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "            associated_users.append(comment['data']['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) For the same input user and from its comments, we extract the users from the previous comment (parent or link author).\n",
    "\n",
    "We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    link = post['data']['link_id']\n",
    "    parent = post['data']['parent_id']\n",
    "    if link != parent: #if parent is not the main post\n",
    "        while True:\n",
    "            try:\n",
    "                res_parent = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'][:-8]\n",
    "                                          + parent[3:],\n",
    "                                          headers = my_headers)\n",
    "                break\n",
    "            except requests.ConnectionError:\n",
    "                print(\"ConnectionError, trying again...\")\n",
    "                my_headers = Headers()#headers_connection_request()\n",
    "        for j, comment in enumerate(res_parent.json()[1]['data']['children']):\n",
    "            if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "                #print(j, comment['data']['author'])\n",
    "                associated_users.append(comment['data']['author'])\n",
    "    else: #parent is the main post\n",
    "        associated_users.append(post['data']['link_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) For the same input user and from its comments, we extract the users from all the following comments (first childrens).\n",
    "\n",
    "This is very tricky to do given the structure of the retrieved information, we need to define a recursive function which acts directly over the adecuate part of the retrieved json and returns a list of **users**. We start doing it only for one comment, then for all of them. We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_in_json(subjson, i=0, depth_limit=1, lst=[]): #depth_limit=1 will show only direct children from a comment\n",
    "    for post in subjson['data']['children']:\n",
    "        if i <= depth_limit:\n",
    "            if 'replies' in post['data']:\n",
    "                lst.append(post['data']['author'])\n",
    "                if post['data']['replies']:\n",
    "                    recursive_in_json(post['data']['replies'], i+1, depth_limit=depth_limit, lst=lst)\n",
    "    return lst[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "userstestlist = []\n",
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    while True:\n",
    "        try:\n",
    "            res_test = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'],\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    utl = recursive_in_json(res_test.json()[1], lst=[])\n",
    "    if utl:\n",
    "        userstestlist.extend(utl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users.extend(userstestlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean this list deleting repeating entries using a Python set, deleting the input **user** and the `'[deleted]'` ones (profiles that doesn't exist anymore), finally creating the list `users_list` to save all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = list(set(associated_users))\n",
    "if username in users_list:\n",
    "    users_list.remove(username)\n",
    "if '[deleted]' in users_list:\n",
    "    users_list.remove('[deleted]')\n",
    "if 'AutoModerator' in users_list:\n",
    "    users_list.remove('AutoModerator')\n",
    "    \n",
    "users_full_list = users_list + [username]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the input user, we found the associated groups, keywords and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_users_groups_keywords_dict = {}\n",
    "final_users_groups_keywords_dict[username] = {}\n",
    "final_users_groups_keywords_dict[username]['groups'] = groups_dict\n",
    "final_users_groups_keywords_dict[username]['keywords'] = keywords_dict\n",
    "final_users_groups_keywords_dict[username]['users'] = users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_full_set = set(groups_dict.keys())\n",
    "keywords_full_set = set(keywords_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all groups and keywords for the associated users\n",
    "\n",
    "We automatize now the previous procedure to obtain **groups** and **keywords** for every **user** in `users_list`, and save them in a Python dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_keywords_dict(user, the_headers):\n",
    "\n",
    "    posts_full_text = \"\"\n",
    "    groups_list = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/comments\",\n",
    "                                        headers = the_headers,\n",
    "                                        params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "    try:\n",
    "        for post in res_comments.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['body']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/submitted\",\n",
    "                                         headers = the_headers,\n",
    "                                         params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "    try:\n",
    "        for post in res_submitted.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['title']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "            if post['data']['selftext']:\n",
    "                posts_full_text += \" \" + post['data']['selftext']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "\n",
    "    corpus_text = posts_full_text.lower()\n",
    "    keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "\n",
    "    return groups_dict, keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 BobVosh\n",
      "1 wsbfan1123\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "2 Jungaktien_Jannik\n",
      "3 itsbotpixel\n",
      "4 thisimpetus\n",
      "5 billionai1\n",
      "6 docsyzygy\n",
      "7 General_Ad4617\n",
      "8 secret759\n",
      "9 Oculosdegrau\n",
      "ConnectionError, trying again...\n",
      "10 qhyirrstynne\n",
      "11 sonia72quebec\n",
      "12 HLef\n",
      "13 Okzuo\n",
      "14 laymanlinguist\n",
      "15 Patty_Henry\n",
      "16 cerebraldormancy\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "17 DemocraticRepublic\n",
      "18 BrewCityChaser\n",
      "19 flatfisher\n",
      "20 ThatPortraitGuy\n",
      "21 biggest_____chungus\n",
      "22 Four4TheRoad\n",
      "23 EpaFdx\n",
      "ConnectionError, trying again...\n",
      "ConnectionError, trying again...\n",
      "24 SilentSamamander\n"
     ]
    }
   ],
   "source": [
    "#my_headers = headers_connection_request() #if needed\n",
    "\n",
    "for i, user in enumerate(users_list):\n",
    "    print(i, user)\n",
    "    gkd = groups_keywords_dict(user, my_headers)\n",
    "    final_users_groups_keywords_dict[user] = {}\n",
    "    final_users_groups_keywords_dict[user]['groups'] = gkd[0]\n",
    "    final_users_groups_keywords_dict[user]['keywords'] = gkd[1]\n",
    "    \n",
    "    groups_full_set = groups_full_set.union(gkd[0].keys())\n",
    "    keywords_full_set = keywords_full_set.union(gkd[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groups': {'u_zip759': 3,\n",
       "  'Pharmadrug': 1,\n",
       "  'france': 1,\n",
       "  'technology': 1,\n",
       "  'aviation': 1,\n",
       "  'AskReddit': 1,\n",
       "  'programming': 1,\n",
       "  'science': 1},\n",
       " 'keywords': {'canon': 2,\n",
       "  'time': 2,\n",
       "  'worry': 1,\n",
       "  'nida-funded': 1,\n",
       "  \"'s\": 1,\n",
       "  'shame': 1,\n",
       "  'uk': 1,\n",
       "  'left': 1,\n",
       "  'eu': 1,\n",
       "  'reason': 1,\n",
       "  'enabled': 1,\n",
       "  '‘': 1,\n",
       "  'auto': 1,\n",
       "  'refill': 1,\n",
       "  '’': 1,\n",
       "  'printer': 1,\n",
       "  'office': 1,\n",
       "  'guy': 1,\n",
       "  'stupid': 1,\n",
       "  'write': 1,\n",
       "  'instant': 1,\n",
       "  'messages': 1,\n",
       "  'bragging': 1,\n",
       "  'misleading': 1,\n",
       "  'faa': 1,\n",
       "  'moving': 1,\n",
       "  'fast': 1,\n",
       "  '1-17': 1,\n",
       "  'forever': 1,\n",
       "  '18-20': 1,\n",
       "  'feels': 1,\n",
       "  'month': 1,\n",
       "  '20-30': 1,\n",
       "  'feel': 1,\n",
       "  'week': 1,\n",
       "  'phrx': 1,\n",
       "  'partner': 1,\n",
       "  'jhu': 1,\n",
       "  'news': 1,\n",
       "  'receiving': 1,\n",
       "  'federal': 1,\n",
       "  'funding': 1,\n",
       "  'psychedelic': 1,\n",
       "  'research': 1,\n",
       "  'sued': 1,\n",
       "  'disabling': 1,\n",
       "  'scanner': 1,\n",
       "  'printers': 1,\n",
       "  'run': 1,\n",
       "  'ink': 1,\n",
       "  'software': 1,\n",
       "  'developers': 1,\n",
       "  'stopped': 1,\n",
       "  'caring': 1,\n",
       "  'reliability': 1,\n",
       "  'covid-19': 1,\n",
       "  'caused': 1,\n",
       "  'extinction': 1,\n",
       "  'influenza': 1,\n",
       "  'lineage': 1,\n",
       "  'b/yamagata': 1,\n",
       "  'april': 1,\n",
       "  '2020': 1,\n",
       "  'august': 1,\n",
       "  '2021': 1,\n",
       "  'dopamine': 1,\n",
       "  'smartphones': 1,\n",
       "  'amp': 1,\n",
       "  'battle': 1},\n",
       " 'users': ['BobVosh',\n",
       "  'wsbfan1123',\n",
       "  'Jungaktien_Jannik',\n",
       "  'itsbotpixel',\n",
       "  'thisimpetus',\n",
       "  'billionai1',\n",
       "  'docsyzygy',\n",
       "  'General_Ad4617',\n",
       "  'secret759',\n",
       "  'Oculosdegrau',\n",
       "  'qhyirrstynne',\n",
       "  'sonia72quebec',\n",
       "  'HLef',\n",
       "  'Okzuo',\n",
       "  'laymanlinguist',\n",
       "  'Patty_Henry',\n",
       "  'cerebraldormancy',\n",
       "  'DemocraticRepublic',\n",
       "  'BrewCityChaser',\n",
       "  'flatfisher',\n",
       "  'ThatPortraitGuy',\n",
       "  'biggest_____chungus',\n",
       "  'Four4TheRoad',\n",
       "  'EpaFdx',\n",
       "  'SilentSamamander']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_users_groups_keywords_dict[username]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping to a json file the raw information of the tripartite network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on he input user, this could create a json file of a couple of MB\n",
    "with open('tripartite_raw.json', 'w') as f:\n",
    "    json.dump(final_users_groups_keywords_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing all elements \n",
    "\n",
    "We need to asign every element (user, group or keyword) a certain index, we choose to do it alphabetically, in order to make the correspondence to Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_users = {us: i for i, us in enumerate(sorted(users_full_list))}\n",
    "full_groups = {gr: i for i, gr in enumerate(sorted(groups_full_set))}\n",
    "full_keywords = {ke: i for i, ke in enumerate(sorted(keywords_full_set))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14892"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'195': 0,\n",
       " '196': 1,\n",
       " '2624': 2,\n",
       " '2healthbars': 3,\n",
       " '2meirl4meirl': 4,\n",
       " '4x4': 5,\n",
       " '52weeksofcooking': 6,\n",
       " 'AMA': 7,\n",
       " 'ANormalDayInRussia': 8,\n",
       " 'AOC': 9,\n",
       " 'AccidentalComedy': 10,\n",
       " 'AcousticOriginals': 11,\n",
       " 'Advice': 12,\n",
       " 'AmITheAngel': 13,\n",
       " 'AmItheAsshole': 14,\n",
       " 'AnimalsBeingDerps': 15,\n",
       " 'AntiJokes': 16,\n",
       " 'ArchitecturalRevival': 17,\n",
       " 'ArenaHS': 18,\n",
       " 'AskALiberal': 19,\n",
       " 'AskAnAmerican': 20,\n",
       " 'AskBiology': 21,\n",
       " 'AskEngineers': 22,\n",
       " 'AskEurope': 23,\n",
       " 'AskGameMasters': 24,\n",
       " 'AskHistorians': 25,\n",
       " 'AskHistory': 26,\n",
       " 'AskMen': 27,\n",
       " 'AskReddit': 28,\n",
       " 'AskScienceFiction': 29,\n",
       " 'Assistance': 30,\n",
       " 'Atlanta': 31,\n",
       " 'AutoChess': 32,\n",
       " 'AutoTuga': 33,\n",
       " 'Awww': 34,\n",
       " 'Baking': 35,\n",
       " 'BakingNoobs': 36,\n",
       " 'Bigpharmagame': 37,\n",
       " 'BirdsArentReal': 38,\n",
       " 'BlackPeopleTwitter': 39,\n",
       " 'BobsTavern': 40,\n",
       " 'Borderlands2': 41,\n",
       " 'Brewers': 42,\n",
       " 'Buttcoin': 43,\n",
       " 'COMPLETEANARCHY': 44,\n",
       " 'Calgary': 45,\n",
       " 'CalgaryClassifieds': 46,\n",
       " 'CanadaPolitics': 47,\n",
       " 'CasualConversation': 48,\n",
       " 'CasualUK': 49,\n",
       " 'CatastrophicFailure': 50,\n",
       " 'Catbun': 51,\n",
       " 'ChineseLanguage': 52,\n",
       " 'ChoosingBeggars': 53,\n",
       " 'CircleofTrust': 54,\n",
       " 'CollectiveGaming': 55,\n",
       " 'CompetitiveHS': 56,\n",
       " 'ContagiousLaughter': 57,\n",
       " 'ConvenientCop': 58,\n",
       " 'Cooking': 59,\n",
       " 'Coronavirus': 60,\n",
       " 'CrappyDesign': 61,\n",
       " 'CrazyIdeas': 62,\n",
       " 'CreateMod': 63,\n",
       " 'Cringetopia': 64,\n",
       " 'CrueltySquad': 65,\n",
       " 'CrusaderKings': 66,\n",
       " 'CryptoCurrency': 67,\n",
       " 'CryptoMarkets': 68,\n",
       " 'CuratedTumblr': 69,\n",
       " 'CurseofStrahd': 70,\n",
       " 'Cynicalbrit': 71,\n",
       " 'DC_Cinematic': 72,\n",
       " 'DIY': 73,\n",
       " 'DMAcademy': 74,\n",
       " 'Damnthatsinteresting': 75,\n",
       " 'DaysGone': 76,\n",
       " 'DeepIntoYouTube': 77,\n",
       " 'Deltarune': 78,\n",
       " 'Design': 79,\n",
       " 'Dimension20': 80,\n",
       " 'DnD': 81,\n",
       " 'DnDHomebrew': 82,\n",
       " 'DragonAgeCoOp': 83,\n",
       " 'Economics': 84,\n",
       " 'EggInc': 85,\n",
       " 'EliteDangerous': 86,\n",
       " 'ElizabethWarren': 87,\n",
       " 'EnterTheGungeon': 88,\n",
       " 'FPGA': 89,\n",
       " 'FanTheories': 90,\n",
       " 'FellowKids': 91,\n",
       " 'Finanzen': 92,\n",
       " 'FindMeADistro': 93,\n",
       " 'Fitness': 94,\n",
       " 'FoundPaper': 95,\n",
       " 'Frisson': 96,\n",
       " 'Frugal': 97,\n",
       " 'FuckApple': 98,\n",
       " 'GSAT': 99,\n",
       " 'Games': 100,\n",
       " 'GamingDetails': 101,\n",
       " 'Gamingcirclejerk': 102,\n",
       " 'Georgia': 103,\n",
       " 'GifRecipes': 104,\n",
       " 'GoCommitDie': 105,\n",
       " 'GoldenAgeMinecraft': 106,\n",
       " 'Goldendoodles': 107,\n",
       " 'GoogleAnalytics': 108,\n",
       " 'GoogleWiFi': 109,\n",
       " 'GrinningGoat': 110,\n",
       " 'GrumpyBabyBirds': 111,\n",
       " 'HPfanfiction': 112,\n",
       " 'Habs': 113,\n",
       " 'HadesTheGame': 114,\n",
       " 'HawaiiVisitors': 115,\n",
       " 'HeadphoneAdvice': 116,\n",
       " 'HeavySeas': 117,\n",
       " 'Helicopters': 118,\n",
       " 'HermitCraft': 119,\n",
       " 'Hiphopcirclejerk': 120,\n",
       " 'HistoryPorn': 121,\n",
       " 'HolUp': 122,\n",
       " 'HongKong': 123,\n",
       " 'HumansBeingBros': 124,\n",
       " 'HydroHomies': 125,\n",
       " 'ImTheMainCharacter': 126,\n",
       " 'Invincible': 127,\n",
       " 'IoniqEV': 128,\n",
       " 'IsItBullshit': 129,\n",
       " 'Jeopardy': 130,\n",
       " 'JimSterling': 131,\n",
       " 'Jokes': 132,\n",
       " 'Kanye': 133,\n",
       " 'KidsAreFuckingStupid': 134,\n",
       " 'KipoAndTheAgeOfWB': 135,\n",
       " 'Kitboga': 136,\n",
       " 'KitchenConfidential': 137,\n",
       " 'LateStageCapitalism': 138,\n",
       " 'LifeProTips': 139,\n",
       " 'LuckBeALandlord': 140,\n",
       " 'MBMBAM': 141,\n",
       " 'MINI': 142,\n",
       " 'MadeMeSmile': 143,\n",
       " 'MadokaMagica': 144,\n",
       " 'MaliciousCompliance': 145,\n",
       " 'MapPorn': 146,\n",
       " 'MasterchefAU': 147,\n",
       " 'MechanicAdvice': 148,\n",
       " 'MemeEconomy': 149,\n",
       " 'Memes_Of_The_Dank': 150,\n",
       " 'MensLib': 151,\n",
       " 'Minecraft': 152,\n",
       " 'Missing411Discussions': 153,\n",
       " 'MissingPersons': 154,\n",
       " 'MkeBucks': 155,\n",
       " 'MonsterStrike': 156,\n",
       " 'MorbidReality': 157,\n",
       " 'MovieDetails': 158,\n",
       " 'MovieSuggestions': 159,\n",
       " 'MurderedByWords': 160,\n",
       " 'Music': 161,\n",
       " 'NASCAR': 162,\n",
       " 'NASCARCollectors': 163,\n",
       " 'NEU': 164,\n",
       " 'NFT': 165,\n",
       " 'NFTCollect': 166,\n",
       " 'NFTExchange': 167,\n",
       " 'NLSSCircleJerk': 168,\n",
       " 'NascarPaintBooth': 169,\n",
       " 'NatureIsFuckingLit': 170,\n",
       " 'NewSkaters': 171,\n",
       " 'NintendoSwitch': 172,\n",
       " 'NintendoSwitchDeals': 173,\n",
       " 'NoStupidQuestions': 174,\n",
       " 'NobodyAsked': 175,\n",
       " 'NotMyJob': 176,\n",
       " 'Nr2003': 177,\n",
       " 'OculusQuest2': 178,\n",
       " 'OfficeChairs': 179,\n",
       " 'OldSchoolCool': 180,\n",
       " 'Oldhouses': 181,\n",
       " 'OnePiece': 182,\n",
       " 'OpenseaMarket': 183,\n",
       " 'OutOfTheLoop': 184,\n",
       " 'PORTUGALCARALHO': 185,\n",
       " 'PS4': 186,\n",
       " 'PS5': 187,\n",
       " 'Parenting': 188,\n",
       " 'PastaPortuguesa': 189,\n",
       " 'PersonalFinanceCanada': 190,\n",
       " 'Peterborough': 191,\n",
       " 'PhantomBorders': 192,\n",
       " 'Pharmadrug': 193,\n",
       " 'PokemonGoCalgary': 194,\n",
       " 'PokemonSwordAndShield': 195,\n",
       " 'PoliticalCompassMemes': 196,\n",
       " 'PoliticalHumor': 197,\n",
       " 'Prague': 198,\n",
       " 'ProgrammerHumor': 199,\n",
       " 'PromoteYourMusic': 200,\n",
       " 'PublicFreakout': 201,\n",
       " 'PuzzleAndDragons': 202,\n",
       " 'RadicalChristianity': 203,\n",
       " 'RelayForReddit': 204,\n",
       " 'RimWorld': 205,\n",
       " 'Roadcam': 206,\n",
       " 'RocketLeague': 207,\n",
       " 'SHIBArmy': 208,\n",
       " 'SapphoAndHerFriend': 209,\n",
       " 'Scotland': 210,\n",
       " 'ScottishPeopleTwitter': 211,\n",
       " 'Sherbrooke': 212,\n",
       " 'ShitAmericansSay': 213,\n",
       " 'ShittyMapPorn': 214,\n",
       " 'Showerthoughts': 215,\n",
       " 'SiliconValleyHBO': 216,\n",
       " 'SimpsonsFaces': 217,\n",
       " 'SpaceXLounge': 218,\n",
       " 'StardewValley': 219,\n",
       " 'Stellaris': 220,\n",
       " 'Superstonk': 221,\n",
       " 'Tarmack': 222,\n",
       " 'Terminator': 223,\n",
       " 'TheExpanse': 224,\n",
       " 'TheWayWeWere': 225,\n",
       " 'TikTokCringe': 226,\n",
       " 'Tinder': 227,\n",
       " 'TooAfraidToAsk': 228,\n",
       " 'Tools': 229,\n",
       " 'TwoSentenceHorror': 230,\n",
       " 'TwoXChromosomes': 231,\n",
       " 'UFOs': 232,\n",
       " 'Unexpected': 233,\n",
       " 'UniversityOfHouston': 234,\n",
       " 'UnresolvedMysteries': 235,\n",
       " 'UnsentLetters': 236,\n",
       " 'UpliftingNews': 237,\n",
       " 'VintageApple': 238,\n",
       " 'VoteBlue': 239,\n",
       " 'WTF': 240,\n",
       " 'Wallstreetbetsnew': 241,\n",
       " 'Warbreaker': 242,\n",
       " 'WatchPeopleDieInside': 243,\n",
       " 'Watches': 244,\n",
       " 'WeAreTheMusicMakers': 245,\n",
       " 'WeatherGifs': 246,\n",
       " 'WeirdWings': 247,\n",
       " 'Wellthatsucks': 248,\n",
       " 'WhatsWrongWithYourDog': 249,\n",
       " 'Whiskyporn': 250,\n",
       " 'WhitePeopleTwitter': 251,\n",
       " 'WinStupidPrizes': 252,\n",
       " 'Wordpress': 253,\n",
       " 'WormMemes': 254,\n",
       " 'WritingPrompts': 255,\n",
       " 'Zoomies': 256,\n",
       " 'aaaaaaacccccccce': 257,\n",
       " 'adhdmeme': 258,\n",
       " 'ageofcivilization': 259,\n",
       " 'angband': 260,\n",
       " 'anime_titties': 261,\n",
       " 'animememes': 262,\n",
       " 'ankylosingspondylitis': 263,\n",
       " 'antergos': 264,\n",
       " 'ape': 265,\n",
       " 'apolloapp': 266,\n",
       " 'apple': 267,\n",
       " 'argentina': 268,\n",
       " 'aromantic': 269,\n",
       " 'asexuality': 270,\n",
       " 'askastronomy': 271,\n",
       " 'askgeology': 272,\n",
       " 'askscience': 273,\n",
       " 'askspain': 274,\n",
       " 'autorepair': 275,\n",
       " 'aviation': 276,\n",
       " 'awfuleverything': 277,\n",
       " 'aws': 278,\n",
       " 'aww': 279,\n",
       " 'badwomensanatomy': 280,\n",
       " 'baseball': 281,\n",
       " 'bearsdoinghumanthings': 282,\n",
       " 'bindingofisaac': 283,\n",
       " 'blackmagicfuckery': 284,\n",
       " 'blacksummer_': 285,\n",
       " 'books': 286,\n",
       " 'booksuggestions': 287,\n",
       " 'borderlands3': 288,\n",
       " 'boston': 289,\n",
       " 'braga': 290,\n",
       " 'brasil': 291,\n",
       " 'bropill': 292,\n",
       " 'bugs': 293,\n",
       " 'buildapc': 294,\n",
       " 'buildapcforme': 295,\n",
       " 'bullcity': 296,\n",
       " 'business': 297,\n",
       " 'callofcthulhu': 298,\n",
       " 'camping': 299,\n",
       " 'canada': 300,\n",
       " 'cardistry': 301,\n",
       " 'carporn': 302,\n",
       " 'casualiama': 303,\n",
       " 'clevercomebacks': 304,\n",
       " 'climate_science': 305,\n",
       " 'confusing_perspective': 306,\n",
       " 'conspiracy': 307,\n",
       " 'consulting': 308,\n",
       " 'crappyoffbrands': 309,\n",
       " 'cryptopt': 310,\n",
       " 'cursedcomments': 311,\n",
       " 'dadjokes': 312,\n",
       " 'dankmemes': 313,\n",
       " 'darkestdungeon': 314,\n",
       " 'dataisbeautiful': 315,\n",
       " 'daughtersofash': 316,\n",
       " 'depression': 317,\n",
       " 'dndmaps': 318,\n",
       " 'dndmemes': 319,\n",
       " 'dogelore': 320,\n",
       " 'dogpictures': 321,\n",
       " 'dogs': 322,\n",
       " 'dune': 323,\n",
       " 'egg_irl': 324,\n",
       " 'electricvehicles': 325,\n",
       " 'elliottsmith': 326,\n",
       " 'entertainment': 327,\n",
       " 'environment': 328,\n",
       " 'ethstaker': 329,\n",
       " 'eu4': 330,\n",
       " 'europe': 331,\n",
       " 'evopsych': 332,\n",
       " 'excel': 333,\n",
       " 'explainlikeimfive': 334,\n",
       " 'facepalm': 335,\n",
       " 'fatlogic': 336,\n",
       " 'fightporn': 337,\n",
       " 'food': 338,\n",
       " 'footballmanagergames': 339,\n",
       " 'france': 340,\n",
       " 'fuckHOA': 341,\n",
       " 'funny': 342,\n",
       " 'funnysigns': 343,\n",
       " 'furry_irl': 344,\n",
       " 'gaming': 345,\n",
       " 'gayspiderbrothel': 346,\n",
       " 'geology': 347,\n",
       " 'geopolitics': 348,\n",
       " 'germany': 349,\n",
       " 'gifs': 350,\n",
       " 'gifsthatkeepongiving': 351,\n",
       " 'giftcardexchange': 352,\n",
       " 'godtiersuperpowers': 353,\n",
       " 'goldenretrievers': 354,\n",
       " 'goodmythicalmorning': 355,\n",
       " 'greebles': 356,\n",
       " 'grubhub': 357,\n",
       " 'grubhubdrivers': 358,\n",
       " 'guitars': 359,\n",
       " 'hackintosh': 360,\n",
       " 'halifax': 361,\n",
       " 'harrypotter': 362,\n",
       " 'hearthstone': 363,\n",
       " 'help': 364,\n",
       " 'heroesofthestorm': 365,\n",
       " 'hiphopheads': 366,\n",
       " 'hockey': 367,\n",
       " 'hockeygoalies': 368,\n",
       " 'hockeymemes': 369,\n",
       " 'hockeyplayers': 370,\n",
       " 'hoi4': 371,\n",
       " 'homestead': 372,\n",
       " 'hometheater': 373,\n",
       " 'horror': 374,\n",
       " 'housepetscomic': 375,\n",
       " 'iRacing': 376,\n",
       " 'iamatotalpieceofshit': 377,\n",
       " 'ifyoulikeblank': 378,\n",
       " 'improv': 379,\n",
       " 'india': 380,\n",
       " 'insanepeoplefacebook': 381,\n",
       " 'insects': 382,\n",
       " 'insideno9': 383,\n",
       " 'interestingasfuck': 384,\n",
       " 'iphone': 385,\n",
       " 'itookapicture': 386,\n",
       " 'jerma985': 387,\n",
       " 'laptops': 388,\n",
       " 'latin': 389,\n",
       " 'legaladvice': 390,\n",
       " 'lgbt': 391,\n",
       " 'lifehacks': 392,\n",
       " 'linux': 393,\n",
       " 'linuxmemes': 394,\n",
       " 'lisboa': 395,\n",
       " 'listentothis': 396,\n",
       " 'literaciafinanceira': 397,\n",
       " 'london': 398,\n",
       " 'longisland': 399,\n",
       " 'loseit': 400,\n",
       " 'lostgeneration': 401,\n",
       " 'lotrmemes': 402,\n",
       " 'malelivingspace': 403,\n",
       " 'marvelstudios': 404,\n",
       " 'mauerstrassenwetten': 405,\n",
       " 'maybemaybemaybe': 406,\n",
       " 'me_irl': 407,\n",
       " 'mead': 408,\n",
       " 'mechanics': 409,\n",
       " 'medical_advice': 410,\n",
       " 'meirl': 411,\n",
       " 'memes': 412,\n",
       " 'midlyintersting': 413,\n",
       " 'mildlyinfuriating': 414,\n",
       " 'mildlyinteresting': 415,\n",
       " 'millionairemakers': 416,\n",
       " 'milwaukee': 417,\n",
       " 'movies': 418,\n",
       " 'natureismetal': 419,\n",
       " 'netflix': 420,\n",
       " 'news': 421,\n",
       " 'nextfuckinglevel': 422,\n",
       " 'nfl': 423,\n",
       " 'noita': 424,\n",
       " 'northernlion': 425,\n",
       " 'nottheonion': 426,\n",
       " 'nutrition': 427,\n",
       " 'nuzlocke': 428,\n",
       " 'oddlysatisfying': 429,\n",
       " 'oddlyterrifying': 430,\n",
       " 'ontario': 431,\n",
       " 'oopsotherhand': 432,\n",
       " 'outriders': 433,\n",
       " 'parrots': 434,\n",
       " 'paslegorafi': 435,\n",
       " 'patientgamers': 436,\n",
       " 'pcgaming': 437,\n",
       " 'pcmasterrace': 438,\n",
       " 'penguins': 439,\n",
       " 'personalfinance': 440,\n",
       " 'photography': 441,\n",
       " 'pics': 442,\n",
       " 'playboicarti': 443,\n",
       " 'playstation': 444,\n",
       " 'pokemonzetaomicron': 445,\n",
       " 'politics': 446,\n",
       " 'popping': 447,\n",
       " 'porto': 448,\n",
       " 'portugal': 449,\n",
       " 'postprocessing': 450,\n",
       " 'powerwashingporn': 451,\n",
       " 'programming': 452,\n",
       " 'projectors': 453,\n",
       " 'raisedbynarcissists': 454,\n",
       " 'rance': 455,\n",
       " 'rape': 456,\n",
       " 'readanotherbook': 457,\n",
       " 'redditgetsdrawn': 458,\n",
       " 'roosterteeth': 459,\n",
       " 'runescape': 460,\n",
       " 'rust': 461,\n",
       " 'sad': 462,\n",
       " 'science': 463,\n",
       " 'shittyaskscience': 464,\n",
       " 'shittyfoodporn': 465,\n",
       " 'short': 466,\n",
       " 'shortscarystories': 467,\n",
       " 'shutupandtakemymoney': 468,\n",
       " 'skateboarding': 469,\n",
       " 'slavelabour': 470,\n",
       " 'slaythespire': 471,\n",
       " 'soccer': 472,\n",
       " 'solarpunk': 473,\n",
       " 'space': 474,\n",
       " 'spelunky': 475,\n",
       " 'squidgame': 476,\n",
       " 'starterpacks': 477,\n",
       " 'startrek': 478,\n",
       " 'stocks': 479,\n",
       " 'stopdrinking': 480,\n",
       " 'suspiciouslyspecific': 481,\n",
       " 'talesfromtechsupport': 482,\n",
       " 'taskmaster': 483,\n",
       " 'tattoos': 484,\n",
       " 'tax': 485,\n",
       " 'technicallythetruth': 486,\n",
       " 'technology': 487,\n",
       " 'techsupport': 488,\n",
       " 'television': 489,\n",
       " 'terriblefacebookmemes': 490,\n",
       " 'teslamotors': 491,\n",
       " 'theocho': 492,\n",
       " 'therewasanattempt': 493,\n",
       " 'theydidthemath': 494,\n",
       " 'thisismylifenow': 495,\n",
       " 'tifu': 496,\n",
       " 'timetravel': 497,\n",
       " 'tipofmytongue': 498,\n",
       " 'todayilearned': 499,\n",
       " 'traderjoes': 500,\n",
       " 'travel': 501,\n",
       " 'trees': 502,\n",
       " 'truegaming': 503,\n",
       " 'tumblr': 504,\n",
       " 'u_sonia72quebec': 505,\n",
       " 'u_zip759': 506,\n",
       " 'ukpolitics': 507,\n",
       " 'underlords': 508,\n",
       " 'unitedkingdom': 509,\n",
       " 'unpopularopinion': 510,\n",
       " 'vexillology': 511,\n",
       " 'vexillologycirclejerk': 512,\n",
       " 'videography': 513,\n",
       " 'videos': 514,\n",
       " 'vim': 515,\n",
       " 'visualization': 516,\n",
       " 'vosfinances': 517,\n",
       " 'wallstreetbets': 518,\n",
       " 'wallstreetbets2': 519,\n",
       " 'weather': 520,\n",
       " 'web_design': 521,\n",
       " 'webhosting': 522,\n",
       " 'whatisthisthing': 523,\n",
       " 'whatsthisbug': 524,\n",
       " 'whatsthisrock': 525,\n",
       " 'windowsphone': 526,\n",
       " 'wisconsin': 527,\n",
       " 'worldbuilding': 528,\n",
       " 'worldnews': 529,\n",
       " 'worldpolitics': 530,\n",
       " 'wow': 531,\n",
       " 'ynab': 532,\n",
       " 'youtubehaiku': 533,\n",
       " 'zelda': 534}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy bipartite arrays\n",
    "biparr_gu = np.zeros((len(full_groups), len(full_users)))\n",
    "biparr_uk = np.zeros((len(full_users), len(full_keywords)))\n",
    "\n",
    "#pandas bipartite dataframes\n",
    "df_gu = pd.DataFrame(columns=('group', 'user', 'repetitions'))\n",
    "#df_uk = pd.DataFrame(columns=('user', 'keywords', 'repetitions'))\n",
    "\n",
    "#biparr_gu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same loop for populating the numpy arrays, we create some pandas dataframes to be called by networkx and immediately used by pyvis to obtain interactive network visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#j = 0\n",
    "for user, values in final_users_groups_keywords_dict.items():\n",
    "    u_idx = full_users[user]\n",
    "    for group, gvalue in values['groups'].items():\n",
    "        g_idx = full_groups[group]\n",
    "        biparr_gu[g_idx][u_idx] = gvalue\n",
    "        df_gu.loc[i] = [group, user, gvalue]\n",
    "        i += 1\n",
    "    for keyword, kvalue in values['keywords'].items():\n",
    "        k_idx = full_keywords[keyword]\n",
    "        biparr_uk[u_idx][k_idx] = kvalue\n",
    "        #df_uk.loc[j] = [user, keyword, kvalue]\n",
    "        #j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"G_uk = nx.from_pandas_edgelist(df_uk, 'user', 'keyword', edge_attr='repetitions')\\npartition_G_uk = community.best_partition(G_uk, weight='repetitions')\\nfor n, p in partition_G_uk.items():\\n    G_uk.nodes[n]['group'] = p\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_gu = nx.from_pandas_edgelist(df_gu, 'group', 'user', edge_attr='repetitions')\n",
    "#partition_G_gu = community.best_partition(G_gu, weight='repetitions')\n",
    "#for n, p in partition_G_gu.items():\n",
    "#    G_gu.nodes[n]['group'] = p\n",
    "    \n",
    "'''G_uk = nx.from_pandas_edgelist(df_uk, 'user', 'keyword', edge_attr='repetitions')\n",
    "partition_G_uk = community.best_partition(G_uk, weight='repetitions')\n",
    "for n, p in partition_G_uk.items():\n",
    "    G_uk.nodes[n]['group'] = p'''\n",
    "\n",
    "#G_gu.edges.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_gu.nodes['u_zip759']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u_zip759': 0,\n",
       " 'zip759': 0,\n",
       " 'Pharmadrug': 1,\n",
       " 'france': 0,\n",
       " 'technology': 2,\n",
       " 'aviation': 3,\n",
       " 'AskReddit': 4,\n",
       " 'programming': 0,\n",
       " 'science': 5,\n",
       " 'BobVosh': 4,\n",
       " 'CrusaderKings': 6,\n",
       " 'GifRecipes': 4,\n",
       " 'TikTokCringe': 4,\n",
       " 'whatsthisbug': 4,\n",
       " 'Showerthoughts': 5,\n",
       " 'HPfanfiction': 4,\n",
       " 'AskHistorians': 7,\n",
       " 'slaythespire': 4,\n",
       " 'KidsAreFuckingStupid': 4,\n",
       " 'nextfuckinglevel': 5,\n",
       " 'WormMemes': 4,\n",
       " 'casualiama': 4,\n",
       " 'truegaming': 4,\n",
       " 'oddlysatisfying': 8,\n",
       " 'HolUp': 9,\n",
       " 'harrypotter': 4,\n",
       " 'UniversityOfHouston': 4,\n",
       " 'BakingNoobs': 4,\n",
       " '52weeksofcooking': 4,\n",
       " 'Baking': 4,\n",
       " 'buildapc': 10,\n",
       " 'redditgetsdrawn': 4,\n",
       " 'AskGameMasters': 4,\n",
       " 'CollectiveGaming': 4,\n",
       " 'JimSterling': 4,\n",
       " 'HeadphoneAdvice': 4,\n",
       " 'AskHistory': 4,\n",
       " 'WritingPrompts': 5,\n",
       " 'Cynicalbrit': 4,\n",
       " 'CircleofTrust': 4,\n",
       " 'MadokaMagica': 4,\n",
       " 'Tarmack': 4,\n",
       " 'Fitness': 4,\n",
       " 'gaming': 11,\n",
       " 'pcgaming': 12,\n",
       " 'patientgamers': 6,\n",
       " 'heroesofthestorm': 4,\n",
       " 'Frisson': 4,\n",
       " 'AskScienceFiction': 4,\n",
       " 'guitars': 4,\n",
       " 'giftcardexchange': 4,\n",
       " 'DnD': 4,\n",
       " 'SHIBArmy': 12,\n",
       " 'wsbfan1123': 12,\n",
       " 'PublicFreakout': 12,\n",
       " 'CryptoCurrency': 12,\n",
       " 'camping': 12,\n",
       " 'horror': 12,\n",
       " 'grubhub': 12,\n",
       " 'MadeMeSmile': 12,\n",
       " 'news': 12,\n",
       " 'movies': 2,\n",
       " 'DaysGone': 12,\n",
       " 'Missing411Discussions': 12,\n",
       " 'EliteDangerous': 12,\n",
       " 'politics': 7,\n",
       " 'squidgame': 12,\n",
       " 'TwoXChromosomes': 13,\n",
       " '4x4': 12,\n",
       " 'malelivingspace': 12,\n",
       " 'ImTheMainCharacter': 12,\n",
       " 'grubhubdrivers': 12,\n",
       " 'blacksummer_': 12,\n",
       " 'fuckHOA': 12,\n",
       " 'help': 12,\n",
       " 'OfficeChairs': 12,\n",
       " 'UnresolvedMysteries': 12,\n",
       " 'MissingPersons': 12,\n",
       " 'stocks': 11,\n",
       " 'Wallstreetbetsnew': 12,\n",
       " 'Jungaktien_Jannik': 4,\n",
       " 'Finanzen': 4,\n",
       " 'mauerstrassenwetten': 4,\n",
       " 'hockey': 14,\n",
       " 'itsbotpixel': 14,\n",
       " 'iRacing': 14,\n",
       " 'NASCAR': 14,\n",
       " 'hockeygoalies': 14,\n",
       " 'elliottsmith': 14,\n",
       " 'GoldenAgeMinecraft': 14,\n",
       " 'hockeyplayers': 14,\n",
       " 'Minecraft': 14,\n",
       " 'penguins': 14,\n",
       " 'roosterteeth': 14,\n",
       " 'NascarPaintBooth': 14,\n",
       " 'loseit': 14,\n",
       " 'Nr2003': 14,\n",
       " '2meirl4meirl': 14,\n",
       " 'depression': 14,\n",
       " 'housepetscomic': 14,\n",
       " 'stopdrinking': 14,\n",
       " 'KipoAndTheAgeOfWB': 14,\n",
       " 'AcousticOriginals': 14,\n",
       " 'goodmythicalmorning': 14,\n",
       " 'furry_irl': 14,\n",
       " 'rape': 14,\n",
       " 'WeAreTheMusicMakers': 14,\n",
       " 'PromoteYourMusic': 14,\n",
       " 'hockeymemes': 14,\n",
       " 'techsupport': 5,\n",
       " 'NASCARCollectors': 14,\n",
       " 'GoCommitDie': 14,\n",
       " 'jerma985': 14,\n",
       " 'Kitboga': 14,\n",
       " 'hearthstone': 5,\n",
       " 'thisimpetus': 5,\n",
       " 'NatureIsFuckingLit': 5,\n",
       " 'AutoChess': 5,\n",
       " 'worldnews': 5,\n",
       " 'MemeEconomy': 5,\n",
       " 'BobsTavern': 5,\n",
       " 'ArenaHS': 5,\n",
       " 'tumblr': 5,\n",
       " 'WhitePeopleTwitter': 9,\n",
       " 'todayilearned': 5,\n",
       " 'LateStageCapitalism': 5,\n",
       " 'TwoSentenceHorror': 5,\n",
       " 'GrinningGoat': 5,\n",
       " 'memes': 15,\n",
       " 'space': 9,\n",
       " 'askscience': 5,\n",
       " 'CuratedTumblr': 16,\n",
       " 'technicallythetruth': 5,\n",
       " 'Superstonk': 5,\n",
       " 'marvelstudios': 5,\n",
       " 'PoliticalCompassMemes': 5,\n",
       " 'apolloapp': 11,\n",
       " 'halifax': 5,\n",
       " 'CompetitiveHS': 5,\n",
       " 'maybemaybemaybe': 5,\n",
       " 'CrappyDesign': 6,\n",
       " 'pics': 13,\n",
       " 'WinStupidPrizes': 17,\n",
       " 'Cringetopia': 5,\n",
       " 'interestingasfuck': 17,\n",
       " 'anime_titties': 5,\n",
       " 'videos': 16,\n",
       " 'fightporn': 8,\n",
       " 'WTF': 11,\n",
       " 'terriblefacebookmemes': 5,\n",
       " 'adhdmeme': 5,\n",
       " 'LifeProTips': 15,\n",
       " 'unpopularopinion': 15,\n",
       " 'blackmagicfuckery': 5,\n",
       " 'meirl': 6,\n",
       " 'animememes': 5,\n",
       " 'me_irl': 5,\n",
       " 'iamatotalpieceofshit': 5,\n",
       " 'Wellthatsucks': 10,\n",
       " 'Damnthatsinteresting': 17,\n",
       " 'AnimalsBeingDerps': 5,\n",
       " 'hackintosh': 5,\n",
       " 'UnsentLetters': 5,\n",
       " 'trees': 5,\n",
       " 'bearsdoinghumanthings': 5,\n",
       " 'powerwashingporn': 5,\n",
       " 'theydidthemath': 5,\n",
       " 'FuckApple': 5,\n",
       " 'MovieSuggestions': 5,\n",
       " 'sad': 5,\n",
       " 'COMPLETEANARCHY': 5,\n",
       " 'Assistance': 5,\n",
       " 'apple': 0,\n",
       " 'booksuggestions': 5,\n",
       " 'thisismylifenow': 5,\n",
       " 'egg_irl': 5,\n",
       " 'TheExpanse': 5,\n",
       " 'iphone': 11,\n",
       " 'godtiersuperpowers': 5,\n",
       " 'underlords': 5,\n",
       " 'lifehacks': 5,\n",
       " 'oopsotherhand': 5,\n",
       " 'OutOfTheLoop': 0,\n",
       " 'RelayForReddit': 5,\n",
       " 'legaladvice': 10,\n",
       " 'netflix': 5,\n",
       " 'askastronomy': 5,\n",
       " 'NoStupidQuestions': 15,\n",
       " 'billionai1': 4,\n",
       " 'linuxmemes': 4,\n",
       " 'callofcthulhu': 4,\n",
       " 'DMAcademy': 4,\n",
       " 'worldbuilding': 4,\n",
       " 'darkestdungeon': 4,\n",
       " 'CurseofStrahd': 4,\n",
       " 'DnDHomebrew': 4,\n",
       " 'MaliciousCompliance': 4,\n",
       " 'ProgrammerHumor': 4,\n",
       " 'CreateMod': 4,\n",
       " 'HermitCraft': 4,\n",
       " 'funnysigns': 4,\n",
       " 'linux': 4,\n",
       " 'cardistry': 4,\n",
       " 'EggInc': 4,\n",
       " 'antergos': 4,\n",
       " 'talesfromtechsupport': 4,\n",
       " 'vim': 4,\n",
       " 'lotrmemes': 4,\n",
       " 'Catbun': 4,\n",
       " 'angband': 4,\n",
       " 'therewasanattempt': 13,\n",
       " 'rust': 4,\n",
       " 'HydroHomies': 4,\n",
       " 'FPGA': 4,\n",
       " 'AskBiology': 4,\n",
       " 'docsyzygy': 4,\n",
       " 'aww': 13,\n",
       " 'bullcity': 4,\n",
       " 'General_Ad4617': 18,\n",
       " 'NFT': 18,\n",
       " 'NFTExchange': 18,\n",
       " 'OpenseaMarket': 18,\n",
       " 'NFTCollect': 18,\n",
       " 'funny': 11,\n",
       " 'secret759': 16,\n",
       " '196': 16,\n",
       " 'youtubehaiku': 16,\n",
       " 'NEU': 16,\n",
       " 'runescape': 16,\n",
       " 'noita': 16,\n",
       " 'boston': 16,\n",
       " 'vexillologycirclejerk': 16,\n",
       " 'Games': 16,\n",
       " 'starterpacks': 16,\n",
       " 'hiphopheads': 16,\n",
       " 'Deltarune': 16,\n",
       " 'playboicarti': 16,\n",
       " 'Kanye': 16,\n",
       " 'OnePiece': 16,\n",
       " 'spelunky': 16,\n",
       " 'SimpsonsFaces': 16,\n",
       " 'Hiphopcirclejerk': 16,\n",
       " '195': 16,\n",
       " 'millionairemakers': 16,\n",
       " 'MensLib': 16,\n",
       " 'FoundPaper': 16,\n",
       " 'Dimension20': 16,\n",
       " 'bropill': 16,\n",
       " 'CrueltySquad': 16,\n",
       " '2624': 16,\n",
       " 'dogelore': 16,\n",
       " 'mead': 16,\n",
       " 'daughtersofash': 16,\n",
       " 'readanotherbook': 16,\n",
       " 'ape': 16,\n",
       " 'HadesTheGame': 16,\n",
       " 'DeepIntoYouTube': 16,\n",
       " 'MBMBAM': 16,\n",
       " 'brasil': 10,\n",
       " 'Oculosdegrau': 10,\n",
       " 'dankmemes': 10,\n",
       " 'AskEngineers': 10,\n",
       " 'germany': 10,\n",
       " 'HumansBeingBros': 10,\n",
       " 'OldSchoolCool': 13,\n",
       " 'HistoryPorn': 10,\n",
       " 'dndmemes': 10,\n",
       " 'DIY': 10,\n",
       " 'WatchPeopleDieInside': 17,\n",
       " 'ChoosingBeggars': 10,\n",
       " 'ANormalDayInRussia': 10,\n",
       " 'BlackPeopleTwitter': 9,\n",
       " 'pcmasterrace': 10,\n",
       " 'UpliftingNews': 10,\n",
       " 'popping': 6,\n",
       " 'Economics': 10,\n",
       " 'FindMeADistro': 10,\n",
       " 'hoi4': 10,\n",
       " 'argentina': 10,\n",
       " 'eu4': 10,\n",
       " 'fatlogic': 15,\n",
       " 'qhyirrstynne': 15,\n",
       " 'parrots': 15,\n",
       " 'asexuality': 15,\n",
       " 'Coronavirus': 15,\n",
       " 'aaaaaaacccccccce': 15,\n",
       " 'cursedcomments': 15,\n",
       " 'aromantic': 15,\n",
       " 'nottheonion': 15,\n",
       " 'CasualConversation': 15,\n",
       " 'medical_advice': 15,\n",
       " 'sonia72quebec': 13,\n",
       " 'personalfinance': 13,\n",
       " 'television': 13,\n",
       " 'Awww': 13,\n",
       " 'tifu': 13,\n",
       " 'Frugal': 13,\n",
       " 'Zoomies': 13,\n",
       " 'mildlyinteresting': 6,\n",
       " 'books': 13,\n",
       " 'PersonalFinanceCanada': 11,\n",
       " 'u_sonia72quebec': 13,\n",
       " 'midlyintersting': 13,\n",
       " 'Habs': 11,\n",
       " 'HLef': 11,\n",
       " 'Calgary': 11,\n",
       " 'ynab': 11,\n",
       " 'playstation': 11,\n",
       " 'IoniqEV': 11,\n",
       " 'PuzzleAndDragons': 11,\n",
       " 'DragonAgeCoOp': 11,\n",
       " 'teslamotors': 11,\n",
       " 'Watches': 11,\n",
       " 'electricvehicles': 11,\n",
       " 'windowsphone': 11,\n",
       " 'PS5': 11,\n",
       " 'borderlands3': 11,\n",
       " 'MINI': 11,\n",
       " 'CanadaPolitics': 11,\n",
       " 'shutupandtakemymoney': 11,\n",
       " 'CrazyIdeas': 11,\n",
       " 'whatisthisthing': 11,\n",
       " 'theocho': 11,\n",
       " 'NintendoSwitchDeals': 11,\n",
       " 'GamingDetails': 11,\n",
       " 'Sherbrooke': 11,\n",
       " 'Borderlands2': 11,\n",
       " 'NintendoSwitch': 11,\n",
       " 'carporn': 11,\n",
       " 'laptops': 11,\n",
       " 'MechanicAdvice': 11,\n",
       " 'insanepeoplefacebook': 6,\n",
       " 'mechanics': 11,\n",
       " 'GoogleAnalytics': 11,\n",
       " 'RocketLeague': 8,\n",
       " 'buildapcforme': 11,\n",
       " 'aws': 11,\n",
       " 'PokemonGoCalgary': 11,\n",
       " 'excel': 11,\n",
       " 'SiliconValleyHBO': 11,\n",
       " 'Bigpharmagame': 11,\n",
       " 'wow': 11,\n",
       " 'MonsterStrike': 11,\n",
       " 'gifs': 6,\n",
       " 'CalgaryClassifieds': 11,\n",
       " 'hometheater': 11,\n",
       " 'projectors': 11,\n",
       " 'tax': 11,\n",
       " 'laymanlinguist': 9,\n",
       " 'NewSkaters': 9,\n",
       " 'askgeology': 9,\n",
       " 'MapPorn': 9,\n",
       " 'climate_science': 9,\n",
       " 'AOC': 9,\n",
       " 'ontario': 9,\n",
       " 'geology': 9,\n",
       " 'whatsthisrock': 9,\n",
       " 'geopolitics': 9,\n",
       " 'AskMen': 9,\n",
       " 'evopsych': 9,\n",
       " 'clevercomebacks': 9,\n",
       " 'MurderedByWords': 9,\n",
       " 'canada': 9,\n",
       " 'suspiciouslyspecific': 9,\n",
       " 'ShittyMapPorn': 9,\n",
       " 'Peterborough': 9,\n",
       " 'skateboarding': 9,\n",
       " 'lgbt': 9,\n",
       " 'gayspiderbrothel': 9,\n",
       " 'GSAT': 1,\n",
       " 'Patty_Henry': 1,\n",
       " 'wallstreetbets': 1,\n",
       " 'autorepair': 1,\n",
       " 'wallstreetbets2': 1,\n",
       " 'cerebraldormancy': 17,\n",
       " 'Unexpected': 17,\n",
       " 'longisland': 17,\n",
       " 'TooAfraidToAsk': 17,\n",
       " 'KitchenConfidential': 17,\n",
       " 'environment': 17,\n",
       " 'homestead': 17,\n",
       " 'traderjoes': 17,\n",
       " 'AMA': 17,\n",
       " 'IsItBullshit': 17,\n",
       " 'UFOs': 17,\n",
       " 'Goldendoodles': 17,\n",
       " 'conspiracy': 17,\n",
       " 'awfuleverything': 17,\n",
       " 'dataisbeautiful': 17,\n",
       " 'nutrition': 17,\n",
       " 'timetravel': 17,\n",
       " 'AmITheAngel': 17,\n",
       " 'dogs': 17,\n",
       " 'HawaiiVisitors': 17,\n",
       " 'gifsthatkeepongiving': 17,\n",
       " 'confusing_perspective': 17,\n",
       " 'AskAnAmerican': 17,\n",
       " 'shittyaskscience': 17,\n",
       " 'TheWayWeWere': 17,\n",
       " 'Oldhouses': 17,\n",
       " 'natureismetal': 17,\n",
       " 'ankylosingspondylitis': 17,\n",
       " 'ConvenientCop': 17,\n",
       " 'insects': 17,\n",
       " 'DemocraticRepublic': 7,\n",
       " 'AskALiberal': 7,\n",
       " 'europe': 7,\n",
       " 'soccer': 7,\n",
       " 'Parenting': 7,\n",
       " 'consulting': 7,\n",
       " 'dune': 7,\n",
       " 'Tinder': 7,\n",
       " 'MovieDetails': 7,\n",
       " 'ContagiousLaughter': 7,\n",
       " 'Stellaris': 7,\n",
       " 'Terminator': 7,\n",
       " 'PoliticalHumor': 7,\n",
       " 'PhantomBorders': 7,\n",
       " 'explainlikeimfive': 7,\n",
       " 'bugs': 7,\n",
       " 'vexillology': 7,\n",
       " 'HongKong': 6,\n",
       " 'latin': 7,\n",
       " 'Scotland': 6,\n",
       " 'VoteBlue': 7,\n",
       " 'GoogleWiFi': 7,\n",
       " 'worldpolitics': 7,\n",
       " 'Georgia': 7,\n",
       " 'Atlanta': 7,\n",
       " 'ElizabethWarren': 7,\n",
       " 'footballmanagergames': 7,\n",
       " 'ukpolitics': 7,\n",
       " 'ageofcivilization': 7,\n",
       " 'unitedkingdom': 7,\n",
       " 'baseball': 3,\n",
       " 'BrewCityChaser': 3,\n",
       " 'Brewers': 3,\n",
       " 'MkeBucks': 3,\n",
       " 'SpaceXLounge': 3,\n",
       " 'CatastrophicFailure': 3,\n",
       " 'weather': 3,\n",
       " 'WeirdWings': 3,\n",
       " 'WeatherGifs': 3,\n",
       " 'milwaukee': 3,\n",
       " 'Roadcam': 3,\n",
       " 'nfl': 3,\n",
       " 'HeavySeas': 3,\n",
       " 'wisconsin': 3,\n",
       " 'greebles': 3,\n",
       " 'Helicopters': 3,\n",
       " 'flatfisher': 0,\n",
       " 'vosfinances': 0,\n",
       " 'Buttcoin': 0,\n",
       " 'rance': 0,\n",
       " 'VintageApple': 0,\n",
       " 'ethstaker': 0,\n",
       " 'Memes_Of_The_Dank': 0,\n",
       " 'lostgeneration': 0,\n",
       " 'visualization': 0,\n",
       " 'AskEurope': 2,\n",
       " 'ArchitecturalRevival': 0,\n",
       " 'paslegorafi': 0,\n",
       " 'BirdsArentReal': 0,\n",
       " 'RadicalChristianity': 0,\n",
       " 'GrumpyBabyBirds': 0,\n",
       " 'solarpunk': 0,\n",
       " 'WhatsWrongWithYourDog': 0,\n",
       " 'dogpictures': 0,\n",
       " 'india': 2,\n",
       " 'ThatPortraitGuy': 2,\n",
       " 'slavelabour': 2,\n",
       " 'MasterchefAU': 2,\n",
       " 'photography': 2,\n",
       " 'outriders': 2,\n",
       " 'videography': 2,\n",
       " 'Jeopardy': 2,\n",
       " 'business': 2,\n",
       " 'Prague': 2,\n",
       " 'MorbidReality': 2,\n",
       " 'Design': 2,\n",
       " 'web_design': 2,\n",
       " 'food': 2,\n",
       " 'itookapicture': 2,\n",
       " 'goldenretrievers': 2,\n",
       " 'entertainment': 2,\n",
       " 'tattoos': 2,\n",
       " 'Music': 2,\n",
       " 'travel': 6,\n",
       " 'postprocessing': 2,\n",
       " 'DC_Cinematic': 2,\n",
       " 'webhosting': 2,\n",
       " 'startrek': 2,\n",
       " 'askspain': 2,\n",
       " 'ShitAmericansSay': 2,\n",
       " 'Wordpress': 2,\n",
       " 'biggest_____chungus': 4,\n",
       " 'raisedbynarcissists': 4,\n",
       " 'Advice': 4,\n",
       " 'OculusQuest2': 4,\n",
       " 'Four4TheRoad': 4,\n",
       " 'RimWorld': 4,\n",
       " 'AmItheAsshole': 4,\n",
       " 'portugal': 8,\n",
       " 'EpaFdx': 8,\n",
       " 'literaciafinanceira': 8,\n",
       " 'PORTUGALCARALHO': 8,\n",
       " 'oddlyterrifying': 8,\n",
       " 'AutoTuga': 8,\n",
       " 'braga': 8,\n",
       " 'lisboa': 8,\n",
       " 'porto': 8,\n",
       " 'PS4': 8,\n",
       " 'PastaPortuguesa': 8,\n",
       " 'cryptopt': 8,\n",
       " 'FellowKids': 8,\n",
       " 'CryptoMarkets': 8,\n",
       " 'Tools': 8,\n",
       " 'facepalm': 8,\n",
       " 'SilentSamamander': 6,\n",
       " 'ScottishPeopleTwitter': 6,\n",
       " 'northernlion': 6,\n",
       " 'CasualUK': 6,\n",
       " 'taskmaster': 6,\n",
       " 'EnterTheGungeon': 6,\n",
       " 'shittyfoodporn': 6,\n",
       " 'Cooking': 6,\n",
       " 'Warbreaker': 6,\n",
       " 'StardewValley': 6,\n",
       " 'ChineseLanguage': 6,\n",
       " 'listentothis': 6,\n",
       " 'bindingofisaac': 6,\n",
       " 'mildlyinfuriating': 6,\n",
       " 'nuzlocke': 6,\n",
       " 'short': 6,\n",
       " 'dadjokes': 6,\n",
       " 'london': 6,\n",
       " 'Invincible': 6,\n",
       " 'NobodyAsked': 6,\n",
       " 'PokemonSwordAndShield': 6,\n",
       " 'insideno9': 6,\n",
       " 'improv': 6,\n",
       " 'zelda': 6,\n",
       " 'dndmaps': 6,\n",
       " 'crappyoffbrands': 6,\n",
       " 'shortscarystories': 6,\n",
       " '2healthbars': 6,\n",
       " 'LuckBeALandlord': 6,\n",
       " 'SapphoAndHerFriend': 6,\n",
       " 'AccidentalComedy': 6,\n",
       " 'NotMyJob': 6,\n",
       " 'NLSSCircleJerk': 6,\n",
       " 'Gamingcirclejerk': 6,\n",
       " 'badwomensanatomy': 6,\n",
       " 'Whiskyporn': 6,\n",
       " 'FanTheories': 6,\n",
       " 'tipofmytongue': 6,\n",
       " 'AntiJokes': 6,\n",
       " 'pokemonzetaomicron': 6,\n",
       " 'ifyoulikeblank': 6,\n",
       " 'Jokes': 6}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_G_gu = community.best_partition(G_gu, weight='repetitions')\n",
    "for n, p in partition_G_gu.items():\n",
    "    G_gu.nodes[n]['group'] = p\n",
    "\n",
    "partition_G_gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group': 0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_gu.nodes['u_zip759']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"bipartite_ggu.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb24234c198>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggu = net.Network(width=1000, height=1000, notebook=True, heading='Bipartite network of groups-users simple plot (unipartite Louvain communities)')\n",
    "ggu.toggle_physics(True)#False)\n",
    "ggu.from_nx(G_gu)\n",
    "ggu.show(\"bipartite_ggu.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guk = net.Network(width=1000, height=1000, notebook=True, heading=\\'Bipartite users-keywords\\')\\nguk.toggle_physics(False)\\nguk.from_nx(G_uk)\\nguk.show(\"test_guk.html\")'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''guk = net.Network(width=1000, height=1000, notebook=True, heading='Bipartite users-keywords')\n",
    "guk.toggle_physics(False)\n",
    "guk.from_nx(G_uk)\n",
    "guk.show(\"test_guk.html\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
