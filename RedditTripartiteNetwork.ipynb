{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design and construction of tripartite network from Reddit\n",
    "\n",
    "Datasets from multipartite complex networks with 3 or more levels (tripartite, quadripartite, etc.) are very scarce, unlike the case of only 2 levels better known as bipartite graphs, which are quite common.\n",
    "\n",
    "I designed and began to construct a tripartite network for my Ph.D. thesis, using the website [Reddit](https://www.reddit.com). According to their own description, \"*Reddit is a network of communities where people can dive into their interests, hobbies and passions. There's a community for whatever you're interested in on Reddit*\". In this context, I use the term *groups* instead of *communities* for technical reasons and to avoid misunderstandings.\n",
    "\n",
    "The tripartite network I defined is composed of:\n",
    "1. **Users** (usernames)\n",
    "2. **Groups** (subreddits)\n",
    "3. **Keywords** (words)\n",
    "\n",
    "My main interest is the tripartite network analysis in two important topics:\n",
    "* **Link prediction**. This can be used in recommendation systems for example, so we could recommend an user certain groups that might find interesting based on our anaylsis.\n",
    "* **Community detection**. Also called clustering in (sligthly) different contexts, and it can be used to detect clusters of users based on the groups they frecuent and the keyword they use, for instance.\n",
    "\n",
    "I already developed many algorithms to do **link prediction** and **community detection** in multipartite networks, but I was lacking of datasets to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the Reddit API you should have first a Reddit account and\n",
    "# sign up for an OAUTH Client ID in https://www.reddit.com/prefs/apps\n",
    "# and at the page bottom click on: \"are you a developer? create an app...\"\n",
    "# https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\n",
    "\n",
    "my_username = 'tripartitenetwork' #account created only for this purpose\n",
    "my_password = '987654321reddit123456789'\n",
    "\n",
    "personal_use_script = 'jVFLZzCvn9H82rRg_M_O1w'\n",
    "secret = 'djzraeUgBxE5U-BKirzY7OG9RQm7_w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_connection_request():\n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(personal_use_script, secret)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': 'password',\n",
    "            'username': my_username,\n",
    "            'password': my_password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the first call to headers_connection_request() doesn't work, we need a while loop\n",
    "def Headers():\n",
    "    my_headers = None\n",
    "    while my_headers is None:\n",
    "        #print(\"test\")\n",
    "        try:\n",
    "            # try until connects and therefore initialize the process\n",
    "            my_headers = headers_connection_request()\n",
    "            return my_headers\n",
    "        except:\n",
    "             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyBot/0.0.1',\n",
       " 'Authorization': 'bearer 1206362233968-BOksz2mPLhzTQCwXXzwQUsguLlp8qQ'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "my_headers = Headers()\n",
    "my_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The starting point is any Reddit username, it's the only input we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'urbannomadberlin'#'GovSchwarzenegger'\n",
    "my_limit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) We start extracting all the words used from our specific user, and simultaneously, the groups where they were posted\n",
    "\n",
    "We describe every text that a certain **user** writes (publicly) as a *post*. Hence, calling the Reddit API we indentify two main types of *posts* and some more subtypes:\n",
    "\n",
    "1. `comment`\n",
    "\n",
    "\n",
    "2. `submitted`\n",
    "\n",
    "    i. `title`\n",
    "    \n",
    "    ii. `selftext` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) We extract the keywords from comments and the subreddits where they were posted.\n",
    "\n",
    "We extract the **keywords** from every `comment` *post*, every `title` of a `submitted` *post*, and optionally from the `selftext` of a `submitted` post, if any. Then we saved all of them in a common string `posts_full_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_full_text = \"\"\n",
    "groups_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/comments\",\n",
    "                                    headers = my_headers,\n",
    "                                    params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_comments.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['body']\n",
    "    groups_list.append(post['data']['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Extracting keywords from submitted title, and from submitted selftext, if any, and the subreddits where they were posted.\n",
    "\n",
    "At the same time, we will append the subreddits, i.e. the **groups** where every *post* belongs, in a list called `groups_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + username + \"/submitted\",\n",
    "                                     headers = my_headers,\n",
    "                                     params = {'limit': my_limit})\n",
    "        break\n",
    "    except requests.ConnectionError:\n",
    "        print(\"ConnectionError, trying again...\")\n",
    "        my_headers = Headers()#headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    posts_full_text += \" \" + post['data']['title']\n",
    "    groups_list.append(post['data']['subreddit'])\n",
    "    if post['data']['selftext']:\n",
    "        posts_full_text += \" \" + post['data']['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having all the groups where a user posted we make a very simple analysis of them.\n",
    "\n",
    "We count the **groups** repetitions and save them as a Python dictionary `groups_dict`. This will help us later to associate every **group** with its respective **user**, where the associated value will correspond to the link weight of the newly defined bipartite **user-groups** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "#groups_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After retrieving all of the user posts keywords, we start to analyze them using the simplest approach: the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The intention is to improve this analysis later with methods such as n-grams or more sophisticaed ones within the natural language processing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_text = posts_full_text.lower()\n",
    "#corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download if necessary!\n",
    "\n",
    "stopwords_e = nltk.corpus.stopwords.words('english')\n",
    "stopwords_g = nltk.corpus.stopwords.words('german')\n",
    "stopwords_s = nltk.corpus.stopwords.words('spanish') #add languages if needed\n",
    "stopwords = stopwords_e + stopwords_g + stopwords_s\n",
    "\n",
    "mystopwords = [\"also\", \"b\", \"best\", \"cannot\", \"can't\", \"cant\"] #complete with words to exclude if necessary\n",
    "\n",
    "stopwords += mystopwords\n",
    "\n",
    "def common_words(text):\n",
    "    # isalpha() method optional for words made of only letters \n",
    "    return [word for word in TextBlob(text).words if word not in stopwords]# and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_words(corpus_text)\n",
    "#set(common_words(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the most common words as a Python dictionary `keywords_dict`, will help us later to associate every **keyword** with its respective **user**, where the associated value will correspond to the link weight of the newly defined **user-keywords** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "#keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) We continue extracting, for our specific input user, all the associated users.\n",
    "\n",
    "In principle, this is not really necessary. Since we already have the basic code to extract all the **groups** and **keywords** for any specific **user**, we could do the same procedure for any arbitrary list of Reddit usernames. But it would make absolute sense to search for **users** connected somehow to our input **user**, and we will find them with a similar approach to the previous one, retrieving our input **user** information. Once we obtain all the **users** associated to our input **user**, we applied to them the full procedure describe in **(A)** to obtain their respective **groups** and **keywords**, and having this we'll have all the needed information to construct our tripartite network. Other different Reddit usernames can be also added manually at any point to expand the network even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) For any given input user and from its submitted posts, we extract the users from the direct replies (first children) to any of them.\n",
    "\n",
    "We save all the associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for post in res_submitted.json()['data']['children']:\n",
    "    name = post['data']['name']\n",
    "    while True:\n",
    "        try:\n",
    "            res_name = requests.get(\"https://oauth.reddit.com\" + \"/comments\" + \"/\" + name[3:] + \"/api\"\n",
    "                                    + \"/morechildren\",\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    for comment in res_name.json()[1]['data']['children']:\n",
    "        if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "            associated_users.append(comment['data']['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#associated_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) For the same input user and from its comments, we extract the users from the previous comment (parent or link author).\n",
    "\n",
    "We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    #print(i, \"https://www.reddit.com\" + post['data']['permalink'])#, post['data']['body'][:50])\n",
    "    link = post['data']['link_id']\n",
    "    parent = post['data']['parent_id']\n",
    "    if link != parent: #if parent is not the main post\n",
    "        while True:\n",
    "            try:\n",
    "                res_parent = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'][:-8]\n",
    "                                          + parent[3:],\n",
    "                                          headers = my_headers)\n",
    "                break\n",
    "            except requests.ConnectionError:\n",
    "                print(\"ConnectionError, trying again...\")\n",
    "                my_headers = Headers()#headers_connection_request()\n",
    "        for j, comment in enumerate(res_parent.json()[1]['data']['children']):\n",
    "            if 'author' in comment['data']: #there's a weird behaviour of Reddit API when retreiving long posts!\n",
    "                #print(j, comment['data']['author'])\n",
    "                associated_users.append(comment['data']['author'])\n",
    "    else: #parent is the main post\n",
    "        #print(post['data']['link_author'])\n",
    "        associated_users.append(post['data']['link_author'])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#associated_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) For the same input user and from its comments, we extract the users from all the following comments (first childrens).\n",
    "\n",
    "This is very tricky to do given the structure of the retrieved information, we need to define a recursive function which acts directly over the adecuate part of the retrieved json and returns a list of **users**. We start doing it only for one comment, then for all of them. We append all these new associated **users** in the `associated_users` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_in_json(subjson, i=0, depth_limit=1, lst=[]): #depth_limit=1 will show only direct children from a comment\n",
    "    for post in subjson['data']['children']:\n",
    "        if i <= depth_limit:\n",
    "            if 'replies' in post['data']:\n",
    "                #print(post['data']['author'])\n",
    "                lst.append(post['data']['author'])\n",
    "                #print(i, \"Name:\", post['data']['name'], \"Depth:\", post['data']['depth'], \"Body:\", post['data']['body'][:50])\n",
    "                #print(\"********************\")\n",
    "                if post['data']['replies']:\n",
    "                    recursive_in_json(post['data']['replies'], i+1, depth_limit=depth_limit, lst=lst)\n",
    "                #if 'parent_id' in post['data']:\n",
    "                #    print('Parent_id:', post['data']['parent_id'])\n",
    "            #print(\"________________________________________________________________________________________________\")\n",
    "            #print()\n",
    "    return lst[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS EXAMPLE IS MEANT TO SHOW THE PARENTS OF A COMMENT\n",
    "# IT CAN USE \"comment\" INSTEAD OF POST NAME AT THE 2ND TO LAST PLACE\n",
    "\n",
    "##res_test = requests.get(\"https://oauth.reddit.com\"\n",
    "                        #+ \"/r/counting/comments/plti3p/4482k_counting_thread/hcdbx5n\"\n",
    "                        #+ \"/r/books/comments/q1sq8m/comment/hfh0glo/?utm_source=share&utm_medium=web2x&context=3\"\n",
    "##                        + \"/r/berlin/comments/pzeryw/why_is_getting_an_anmeldung_so_hard/hf3285d/\",\n",
    "                        #+ \"/?context=8\",\n",
    "##                        headers = my_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#userstest = recursive_in_json(res_test.json()[1], lst=[]) #lst=[] is needed to call the function correctly\n",
    "#userstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "userstestlist = []\n",
    "for i, post in enumerate(res_comments.json()['data']['children']): #up to 100 comments\n",
    "    #print(i, \"https://www.reddit.com\" + post['data']['permalink'])\n",
    "    while True:\n",
    "        try:\n",
    "            res_test = requests.get(\"https://oauth.reddit.com\" + post['data']['permalink'],\n",
    "                                    headers = my_headers)\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()#headers_connection_request()\n",
    "    utl = recursive_in_json(res_test.json()[1], lst=[])\n",
    "    #print(utl)\n",
    "    #print()\n",
    "    if utl:\n",
    "        userstestlist.extend(utl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userstestlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_users.extend(userstestlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean this list deleting repeating entries using a Python set, deleting the input **user** and the `'[deleted]'` ones (profiles that doesn't exist anymore), finally creating the list `users_list` to save all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = list(set(associated_users))\n",
    "users_list.remove(username)\n",
    "users_list.remove('[deleted]')\n",
    "#users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final step for the input user is to find the associated groups, keywords and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urbannomadberlin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chile': 32,\n",
       " 'berlin': 22,\n",
       " 'ifyoulikeblank': 21,\n",
       " 'berlinsocialclub': 16,\n",
       " 'musicsuggestions': 10,\n",
       " 'InternetIsBeautiful': 2,\n",
       " 'worldnews': 2,\n",
       " 'LesPaul': 2,\n",
       " 'MusicCritique': 2,\n",
       " 'mildlyinteresting': 1,\n",
       " 'europe': 1,\n",
       " 'Music': 1,\n",
       " 'dataisbeautiful': 1,\n",
       " 'movies': 1,\n",
       " 'funny': 1,\n",
       " 'AskReddit': 1,\n",
       " 'HeadphoneAdvice': 1,\n",
       " 'CryptoCurrency': 1,\n",
       " 'KrakenSupport': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https': 31,\n",
       " 'amp': 18,\n",
       " \"n't\": 16,\n",
       " 'music': 16,\n",
       " 'www.youtube.com/watch': 14,\n",
       " \"'s\": 13,\n",
       " 'jajaja': 12,\n",
       " 'one': 12,\n",
       " 'rock': 12,\n",
       " 'please': 11,\n",
       " 'thanks': 11,\n",
       " 'like': 11,\n",
       " 'x200b': 11,\n",
       " 'hi': 9,\n",
       " \"'m\": 9,\n",
       " 'chile': 8,\n",
       " 'si': 8,\n",
       " 'looking': 8,\n",
       " '3': 7,\n",
       " 'well': 7,\n",
       " 'know': 7,\n",
       " 'want': 7,\n",
       " 'make': 7,\n",
       " 'way': 7,\n",
       " 'songs': 7,\n",
       " 'could': 6,\n",
       " 'pa': 6,\n",
       " 'listen': 6,\n",
       " 'bass': 6,\n",
       " 'course': 6,\n",
       " 'really': 6,\n",
       " 'think': 6,\n",
       " 'comida': 5,\n",
       " 'wrote': 5,\n",
       " 'love': 5,\n",
       " 'berlin': 5,\n",
       " 'soon': 5,\n",
       " 'bands': 5,\n",
       " 'days': 5,\n",
       " 'jazz': 5,\n",
       " 'album': 4,\n",
       " 'going': 4,\n",
       " 'possible': 4,\n",
       " 'find': 4,\n",
       " 'say': 4,\n",
       " '2': 4,\n",
       " 'would': 4,\n",
       " 'let': 4,\n",
       " 'even': 4,\n",
       " 'hahaha': 4,\n",
       " 'something': 4,\n",
       " 'first': 4,\n",
       " '1m5': 4,\n",
       " '1m1': 4,\n",
       " '2m2': 4,\n",
       " 'different': 4,\n",
       " 'sense': 4,\n",
       " 'live': 4,\n",
       " 'good': 4,\n",
       " 'new': 4,\n",
       " 'look': 4,\n",
       " 'canciones': 4,\n",
       " 'metal': 4,\n",
       " 'unfortunately': 4,\n",
       " 'bring': 4,\n",
       " 'march': 4,\n",
       " 'contract': 4,\n",
       " 'money': 4,\n",
       " 'chat': 3,\n",
       " '100': 3,\n",
       " 'sólo': 3,\n",
       " 'procesada': 3,\n",
       " 'hace': 3,\n",
       " 'alguna': 3,\n",
       " 'c3': 3,\n",
       " 'chucha': 3,\n",
       " 'aún': 3,\n",
       " 'años': 3,\n",
       " 'ahora': 3,\n",
       " 'today': 3,\n",
       " 'lot': 3,\n",
       " 'hola': 3,\n",
       " 'lt': 3,\n",
       " 'prost': 3,\n",
       " 'last': 3,\n",
       " 'working': 3,\n",
       " 'park': 3,\n",
       " 'appreciated': 3,\n",
       " '5': 3,\n",
       " 'casi': 3,\n",
       " 'everyone': 3,\n",
       " 'ezra': 3,\n",
       " 'collective': 3,\n",
       " 'bank': 3,\n",
       " 'listening': 3,\n",
       " 'message': 3,\n",
       " 'público': 3,\n",
       " 'available': 3,\n",
       " 'enjoy': 3,\n",
       " 'clearly': 3,\n",
       " 'santana': 3,\n",
       " 'apruebo': 3,\n",
       " '10': 3,\n",
       " '4': 3,\n",
       " 'similar': 3,\n",
       " 'tool': 3,\n",
       " 'previous': 3,\n",
       " 'floydian': 3,\n",
       " 'pt': 3,\n",
       " 'mastodon': 3,\n",
       " 'albums': 3,\n",
       " 'totally': 3,\n",
       " 'sound': 3,\n",
       " 'check': 3,\n",
       " 'home': 3,\n",
       " 'marino': 3,\n",
       " 'nubiyan': 3,\n",
       " 'twist': 3,\n",
       " 'musical': 3,\n",
       " 'and/or': 3,\n",
       " 'playing': 3,\n",
       " 'like/know': 3,\n",
       " 'feel': 3,\n",
       " 'fusion': 3,\n",
       " 'people': 3,\n",
       " 'amazing': 3,\n",
       " 'reddit': 3,\n",
       " 'bluetooth': 3,\n",
       " 'use': 3,\n",
       " 'receive': 3,\n",
       " 'experience': 3,\n",
       " '2017': 3,\n",
       " 'dominance': 3,\n",
       " 'price': 3,\n",
       " 'another': 3,\n",
       " 'deposit': 3,\n",
       " 'german': 3,\n",
       " 'gt': 2,\n",
       " 'sent': 2,\n",
       " 'seguro': 2,\n",
       " 'haber': 2,\n",
       " 'mala': 2,\n",
       " 'cara': 2,\n",
       " 'hecho': 2,\n",
       " 'wea': 2,\n",
       " 'q': 2,\n",
       " 'parece': 2,\n",
       " 'total': 2,\n",
       " 'tan': 2,\n",
       " 'ojalá': 2,\n",
       " 'causa': 2,\n",
       " 'calidad': 2,\n",
       " 'vez': 2,\n",
       " 'interested': 2,\n",
       " 'went': 2,\n",
       " 'daughter': 2,\n",
       " 'funny': 2,\n",
       " 'correct': 2,\n",
       " 'vulfpeck': 2,\n",
       " 'things': 2,\n",
       " 'juntamos': 2,\n",
       " 'parque': 2,\n",
       " 'still': 2,\n",
       " 'info': 2,\n",
       " 'information': 2,\n",
       " 'car': 2,\n",
       " 'basically': 2,\n",
       " 'wonder': 2,\n",
       " 'czech': 2,\n",
       " 'side': 2,\n",
       " 'advance': 2,\n",
       " 'mine': 2,\n",
       " 'typically': 2,\n",
       " 'beers': 2,\n",
       " '7': 2,\n",
       " 'vivo': 2,\n",
       " 'días': 2,\n",
       " 'band': 2,\n",
       " 'play': 2,\n",
       " 'around': 2,\n",
       " 'guess': 2,\n",
       " 'thread': 2,\n",
       " 'yes': 2,\n",
       " 'telegram': 2,\n",
       " 'afrobeat': 2,\n",
       " 'tell': 2,\n",
       " 'exact': 2,\n",
       " 'video': 2,\n",
       " 'chilean': 2,\n",
       " 'president': 2,\n",
       " 'since': 2,\n",
       " 'actual': 2,\n",
       " 'many': 2,\n",
       " 'despicable': 2,\n",
       " 'always': 2,\n",
       " 'weed': 2,\n",
       " 'send': 2,\n",
       " 'adrian': 2,\n",
       " 'belew': 2,\n",
       " 'trio': 2,\n",
       " 'x': 2,\n",
       " 'king': 2,\n",
       " 'us': 2,\n",
       " 'help': 2,\n",
       " 'read': 2,\n",
       " \"'re\": 2,\n",
       " 'happens': 2,\n",
       " 'flies': 2,\n",
       " 'witches': 2,\n",
       " 'www.google.com/maps/dir/beverly+hills': 2,\n",
       " 'california': 2,\n",
       " 'usa/the+bronx': 2,\n",
       " 'new+york': 2,\n",
       " 'usa': 2,\n",
       " '35.9865814': 2,\n",
       " '114.2529463,4z/data': 2,\n",
       " '3m1': 2,\n",
       " '4b1': 2,\n",
       " '4m14': 2,\n",
       " '4m13': 2,\n",
       " '1s0x80c2bc04d6d147ab:0xd6c7c379fd081ed1': 2,\n",
       " '1d-118.4003563': 2,\n",
       " '2d34.0736204': 2,\n",
       " '1s0x89c28b553a697cb1:0x556e43a78ff15c77': 2,\n",
       " '1d-73.8648268': 2,\n",
       " '2d40.8447819': 2,\n",
       " '3e1': 2,\n",
       " 'acá': 2,\n",
       " 'tipo': 2,\n",
       " 'así': 2,\n",
       " 'fila': 2,\n",
       " 'demoré': 2,\n",
       " 'minutos': 2,\n",
       " 'enorme': 2,\n",
       " 'onda': 2,\n",
       " 'balance': 2,\n",
       " 'opinion': 2,\n",
       " 'knower': 2,\n",
       " 'v=zqlsmix03ng': 2,\n",
       " 'ok': 2,\n",
       " 'time': 2,\n",
       " 'pink': 2,\n",
       " 'floyd': 2,\n",
       " 'frame': 2,\n",
       " 'case': 2,\n",
       " 'talking': 2,\n",
       " 'vibe': 2,\n",
       " 'maybe': 2,\n",
       " 'ears': 2,\n",
       " 'wings': 2,\n",
       " 'place': 2,\n",
       " 'set': 2,\n",
       " 'kind': 2,\n",
       " 'order': 2,\n",
       " 'influences': 2,\n",
       " 'none': 2,\n",
       " 'much': 2,\n",
       " 'show': 2,\n",
       " 'youtube': 2,\n",
       " 'later': 2,\n",
       " 'gente': 2,\n",
       " 'twitter.com/ibranaber/status/1314441249089355776': 2,\n",
       " 'puerto': 2,\n",
       " 'elefante': 2,\n",
       " 'general': 2,\n",
       " 'fela': 2,\n",
       " 'kuti': 2,\n",
       " 'contemporary': 2,\n",
       " 'jungle': 2,\n",
       " 'run': 2,\n",
       " 'beer': 2,\n",
       " 'ustedes': 2,\n",
       " 'siempre': 2,\n",
       " 'advice': 2,\n",
       " 'actually': 2,\n",
       " 'post': 2,\n",
       " 'reply': 2,\n",
       " 'especially': 2,\n",
       " 'ana': 2,\n",
       " 'though': 2,\n",
       " 'original': 2,\n",
       " 'guitar': 2,\n",
       " 'month': 2,\n",
       " 'soul': 2,\n",
       " 'tocar': 2,\n",
       " 'elegir': 2,\n",
       " 'feedback': 2,\n",
       " 'jamallama': 2,\n",
       " 'jeo': 2,\n",
       " 'na': 2,\n",
       " 'connections': 2,\n",
       " 'made': 2,\n",
       " 'styles': 2,\n",
       " 'give': 2,\n",
       " 'main': 2,\n",
       " 'starting': 2,\n",
       " 'ending': 2,\n",
       " 'latin': 2,\n",
       " 'folk': 2,\n",
       " 'electronic': 2,\n",
       " 'hop': 2,\n",
       " 'free': 2,\n",
       " '€': 2,\n",
       " 'headphone': 2,\n",
       " 'works': 2,\n",
       " 'wired': 2,\n",
       " 'neutral': 2,\n",
       " 'budget': 2,\n",
       " 'gear': 2,\n",
       " 'mode': 2,\n",
       " 'preferred': 2,\n",
       " 'everything': 2,\n",
       " 'highly': 2,\n",
       " 'festival': 2,\n",
       " 'evening': 2,\n",
       " 'problem': 2,\n",
       " 'speakers': 2,\n",
       " 'every': 2,\n",
       " 'happened': 2,\n",
       " '11': 2,\n",
       " 'appreciate': 2,\n",
       " 'btc': 2,\n",
       " 'cryptomarket': 2,\n",
       " 'increase': 2,\n",
       " 'rent': 2,\n",
       " 'year': 2,\n",
       " 'indefinite': 2,\n",
       " 'user': 2,\n",
       " 'two': 2,\n",
       " 'teacher': 2,\n",
       " 'spanish': 2,\n",
       " 'suggestions': 2,\n",
       " 'recycling': 2,\n",
       " 'bottles': 2,\n",
       " 'supermarkets': 2,\n",
       " 'landlord': 1,\n",
       " 'rarely': 1,\n",
       " 'refuse': 1,\n",
       " 'elaborate': 1,\n",
       " 'duele': 1,\n",
       " 'loc': 1,\n",
       " 'relax': 1,\n",
       " 'quiero': 1,\n",
       " 'hacerte': 1,\n",
       " 'entender': 1,\n",
       " 'salido': 1,\n",
       " 'podría': 1,\n",
       " 'tener': 1,\n",
       " 'opinión': 1,\n",
       " 'aunque': 1,\n",
       " 'intuición': 1,\n",
       " 'allá': 1,\n",
       " 'año': 1,\n",
       " 'quejado': 1,\n",
       " 'relacionada': 1,\n",
       " 'www.reddit.com/r/chile/comments/inrd1j/met': 1,\n",
       " 'ad\\\\_panes\\\\_al\\\\_azar\\\\_bien\\\\_piola\\\\_en\\\\_la\\\\_bolsa\\\\_y\\\\_dio/g49fuvc': 1,\n",
       " 'voy': 1,\n",
       " 'comparar': 1,\n",
       " 'algún': 1,\n",
       " 'país': 1,\n",
       " 'salir': 1,\n",
       " 'vale': 1,\n",
       " 'hectáreas': 1,\n",
       " 'callampa': 1,\n",
       " 'viajado': 1,\n",
       " 'vivir': 1,\n",
       " 'afuera': 1,\n",
       " 'alumbrarse': 1,\n",
       " 'único': 1,\n",
       " 'importante': 1,\n",
       " 'cierta': 1,\n",
       " 'experiencia': 1,\n",
       " 'ayuda': 1,\n",
       " 'afirmar': 1,\n",
       " 'aseveración': 1,\n",
       " 'sellos': 1,\n",
       " 'azúcar': 1,\n",
       " 'buen': 1,\n",
       " 'primer': 1,\n",
       " 'paso': 1,\n",
       " 'falta': 1,\n",
       " 'viviendo': 1,\n",
       " '8': 1,\n",
       " 'puta': 1,\n",
       " 'producen': 1,\n",
       " 'wns': 1,\n",
       " 'hundan': 1,\n",
       " 'desaparezcan': 1,\n",
       " 'pronto': 1,\n",
       " 'nuevo': 1,\n",
       " 'cuestionamiento': 1,\n",
       " 'cómo': 1,\n",
       " 'mejorar': 1,\n",
       " 'disminuir': 1,\n",
       " 'precios': 1,\n",
       " 'aberrantes': 1,\n",
       " 'subsidios': 1,\n",
       " 'estatales': 1,\n",
       " 'buena': 1,\n",
       " 'tal': 1,\n",
       " 'dura': 1,\n",
       " 'lol': 1,\n",
       " 'rave.dj/dlcbmlg9umukga': 1,\n",
       " 'probably': 1,\n",
       " 'chili': 1,\n",
       " 'carne': 1,\n",
       " 'briefly': 1,\n",
       " 'teepeland': 1,\n",
       " 'haha': 1,\n",
       " 'coincidence': 1,\n",
       " 'answer': 1,\n",
       " 'dean': 1,\n",
       " 'town': 1,\n",
       " 'noticed': 1,\n",
       " 'full': 1,\n",
       " 'debut': 1,\n",
       " 'escribes': 1,\n",
       " 'coordinamos': 1,\n",
       " 'jammear': 1,\n",
       " 'además': 1,\n",
       " 'buscando': 1,\n",
       " 'vocalista': 1,\n",
       " 'bandas': 1,\n",
       " 'fair': 1,\n",
       " 'twice': 1,\n",
       " 'months': 1,\n",
       " 'quite': 1,\n",
       " 'functionally': 1,\n",
       " 'justmusic': 1,\n",
       " 'reopens': 1,\n",
       " 'tomorrow': 1,\n",
       " 'relevant': 1,\n",
       " 'camping': 1,\n",
       " 'camp': 1,\n",
       " 'scenario': 1,\n",
       " 'official': 1,\n",
       " 'prices': 1,\n",
       " 'including': 1,\n",
       " '1312': 1,\n",
       " 'squats': 1,\n",
       " 'left': 1,\n",
       " 'wing': 1,\n",
       " 'bars': 1,\n",
       " 'favorite': 1,\n",
       " 'viví': 1,\n",
       " 'santiago': 1,\n",
       " '6': 1,\n",
       " 'quieres': 1,\n",
       " 'charlar': 1,\n",
       " 'chelear': 1,\n",
       " 'puedo': 1,\n",
       " 'complementar': 1,\n",
       " 'información': 1,\n",
       " 'redditores': 1,\n",
       " 'locales': 1,\n",
       " 'dado': 1,\n",
       " 'puedes': 1,\n",
       " 'mandar': 1,\n",
       " 'mensaje': 1,\n",
       " \"'ll\": 1,\n",
       " 'jamming': 1,\n",
       " 'boxi': 1,\n",
       " 'come': 1,\n",
       " '19:00': 1,\n",
       " 'yeah': 1,\n",
       " '90': 1,\n",
       " 'minutes': 1,\n",
       " 'ca': 1,\n",
       " 'joke': 1,\n",
       " 'yet': 1,\n",
       " 'igual': 1,\n",
       " 'humor': 1,\n",
       " 'r/whoosh': 1,\n",
       " 'en-tien-do': 1,\n",
       " 'entendemos': 1,\n",
       " 'jaja': 1,\n",
       " 'desmerecer': 1,\n",
       " 'entretenido': 1,\n",
       " 'interesante': 1,\n",
       " 'historia': 1,\n",
       " 'entiendes': 1,\n",
       " 'nft': 1,\n",
       " 'es.wikipedia.org/wiki/token\\\\_no\\\\_fungible': 1,\n",
       " 'es.wikipedia.org/wiki/token_no_fungible': 1,\n",
       " 'störtebeker': 1,\n",
       " 'atlantik': 1,\n",
       " 'ale': 1,\n",
       " 'thinking': 1,\n",
       " 'making': 1,\n",
       " 'discord': 1,\n",
       " 'replied': 1,\n",
       " 'aka': 1,\n",
       " 'stoner/music-lovers': 1,\n",
       " 'group': 1,\n",
       " 'newen': 1,\n",
       " 'funnier': 1,\n",
       " 'moment': 1,\n",
       " 'television': 1,\n",
       " 'cut': 1,\n",
       " 'says': 1,\n",
       " 'compartimos': 1,\n",
       " 'translates': 1,\n",
       " 'share': 1,\n",
       " 'hilarious': 1,\n",
       " 'known': 1,\n",
       " 'disgusting': 1,\n",
       " 'thief': 1,\n",
       " 'stole': 1,\n",
       " 'become': 1,\n",
       " 'millionaire': 1,\n",
       " 'en.wikipedia.org/wiki/sebasti': 1,\n",
       " 'a1n_pi': 1,\n",
       " 'b1era': 1,\n",
       " 'businesses': 1,\n",
       " 'life': 1,\n",
       " 'fucking': 1,\n",
       " 'burocracy': 1,\n",
       " 'planly': 1,\n",
       " 'stupid': 1,\n",
       " 'cool': 1,\n",
       " 'addict': 1,\n",
       " 'connosieur': 1,\n",
       " 'session': 1,\n",
       " \"'d\": 1,\n",
       " 'learn': 1,\n",
       " 'vegan': 1,\n",
       " 'recipes': 1,\n",
       " 'ad': 1,\n",
       " 'v=v4ww3uryrlg': 1,\n",
       " 'v=ylqeq4jqluk': 1,\n",
       " 'gathering': 1,\n",
       " 'contexto': 1,\n",
       " 'www.reddit.com/r/whitepeopletwitter/comments/989xz2/yeah\\\\_fuck\\\\_off\\\\_rebecca': 1,\n",
       " 'www.reddit.com/r/whitepeopletwitter/comments/989xz2/yeah_fuck_off_rebecca': 1,\n",
       " 'xq': 1,\n",
       " 'censuran': 1,\n",
       " 'nombre': 1,\n",
       " 'rebecca': 1,\n",
       " 'qlia': 1,\n",
       " 'wn': 1,\n",
       " 'ooooooooo': 1,\n",
       " 'twitter': 1,\n",
       " 'reqliao': 1,\n",
       " 'publico': 1,\n",
       " 'ctm': 1,\n",
       " 'hackeado': 1,\n",
       " 'mail': 1,\n",
       " 'ilegalmente': 1,\n",
       " 'publicaron': 1,\n",
       " 'weaita': 1,\n",
       " 'repite': 1,\n",
       " 'conmigo': 1,\n",
       " 'twiter': 1,\n",
       " 'rant': 1,\n",
       " 'día': 1,\n",
       " 'gracias': 1,\n",
       " 'atención': 1,\n",
       " 'gizzard': 1,\n",
       " 'lizard': 1,\n",
       " 'wizard': 1,\n",
       " 'www.setlist.fm/setlists/king-gizzard-and-the-lizard-wizard-23de1823.html': 1,\n",
       " 'columbiahalle': 1,\n",
       " 'join': 1,\n",
       " 'mieterverein': 1,\n",
       " 'talk': 1,\n",
       " 'lawyer': 1,\n",
       " 'paul': 1,\n",
       " 'newest': 1,\n",
       " 'artists': 1,\n",
       " 'inspiring': 1,\n",
       " 'millions': 1,\n",
       " 'forgot': 1,\n",
       " 'comment': 1,\n",
       " 'straightforward': 1,\n",
       " 'deal': 1,\n",
       " 'sarcasm': 1,\n",
       " 'seems': 1,\n",
       " 'trying': 1,\n",
       " 'lines': 1,\n",
       " 'simply': 1,\n",
       " 'projecting': 1,\n",
       " 'issues': 1,\n",
       " 'trolling': 1,\n",
       " 'edgy': 1,\n",
       " 'ignore': 1,\n",
       " 'gut': 1,\n",
       " 'overthink': 1,\n",
       " 'pfaueninsel': 1,\n",
       " 'jar': 1,\n",
       " 'alice': 1,\n",
       " 'chains': 1,\n",
       " 'cnn': 1,\n",
       " 'times': 1,\n",
       " 'bad': 1,\n",
       " 'website': 1,\n",
       " 'manso': 1,\n",
       " 'pique': 1,\n",
       " '5000': 1,\n",
       " 'km': 1,\n",
       " 'súper': 1,\n",
       " 'pendiente': 1,\n",
       " 'resultados': 1,\n",
       " 'online': 1,\n",
       " 'informarlos': 1,\n",
       " 'radio': 1,\n",
       " 'pueblo': 1,\n",
       " 'vives': 1,\n",
       " 'celebración': 1,\n",
       " 'bueno': 1,\n",
       " 'votaciones': 1,\n",
       " 'importantes': 1,\n",
       " 'siguientes': 1,\n",
       " 'ganó': 1,\n",
       " '95': 1,\n",
       " 'voté': 1,\n",
       " 'alemania': 1,\n",
       " 'llegué': 1,\n",
       " '11:00': 1,\n",
       " 'mediodía': 1,\n",
       " '50': 1,\n",
       " 'metros': 1,\n",
       " '20': 1,\n",
       " 'entrar': 1,\n",
       " 'consulado': 1,\n",
       " 'votar': 1,\n",
       " 'salí': 1,\n",
       " 'cuadras': 1,\n",
       " 'rajé': 1,\n",
       " 'esperar': 1,\n",
       " 'horas': 1,\n",
       " 'conteo': 1,\n",
       " 'votos': 1,\n",
       " 'mismo': 1,\n",
       " 'arrasa': 1,\n",
       " '61.69': 1,\n",
       " 'rechazo': 1,\n",
       " '38.31': 1,\n",
       " 'cmc': 1,\n",
       " '42.01': 1,\n",
       " 'cc': 1,\n",
       " '57.99': 1,\n",
       " 'floripondio': 1,\n",
       " 'virtually': 1,\n",
       " 'unknown': 1,\n",
       " 'godzillionarie': 1,\n",
       " 'negative': 1,\n",
       " 'v=s5bv5ppegl0': 1,\n",
       " 'list=pl-aje_d7tppw4uiz6d-batdlloiybgfll': 1,\n",
       " 'eeeeh': 1,\n",
       " 'nope': 1,\n",
       " 'facebook': 1,\n",
       " 'banned': 1,\n",
       " 'china': 1,\n",
       " 'chichichilelele': 1,\n",
       " 'ultra': 1,\n",
       " 'cringe': 1,\n",
       " 'grito': 1,\n",
       " 'decadas': 1,\n",
       " 'popular': 1,\n",
       " 'dias': 1,\n",
       " 'unpopular': 1,\n",
       " 'overtime': 1,\n",
       " 'v=gnemd17kyse': 1,\n",
       " 'mention': 1,\n",
       " 'fault': 1,\n",
       " 'extremely': 1,\n",
       " 'porcupine': 1,\n",
       " 'tree': 1,\n",
       " 'v=qecg-yl4ky8': 1,\n",
       " 'dogs': 1,\n",
       " 'v=4qa30qkryy8': 1,\n",
       " 'related': 1,\n",
       " 'mentioned': 1,\n",
       " '7empest': 1,\n",
       " 'v=9d2r69gvyz0': 1,\n",
       " 'crimson': 1,\n",
       " 'youtu.be/oxfsc9gybpa': 1,\n",
       " 't=67': 1,\n",
       " 'riffs': 1,\n",
       " 'sooooo': 1,\n",
       " 'tribute': 1,\n",
       " 'distinguish': 1,\n",
       " 'influence': 1,\n",
       " 'obviously': 1,\n",
       " 'examples': 1,\n",
       " 'end': 1,\n",
       " 'stuff': 1,\n",
       " 'third': 1,\n",
       " 'eye': 1,\n",
       " 'reflection': 1,\n",
       " 'marie': 1,\n",
       " '1': 1,\n",
       " '10,000': 1,\n",
       " 'czar': 1,\n",
       " 'cold': 1,\n",
       " 'dark': 1,\n",
       " 'north': 1,\n",
       " 'star': 1,\n",
       " 'remember': 1,\n",
       " 'pf': 1,\n",
       " 'controls': 1,\n",
       " 'heart': 1,\n",
       " 'sun': 1,\n",
       " 'careful': 1,\n",
       " 'axe': 1,\n",
       " 'eugene': 1,\n",
       " 'echoes': 1,\n",
       " 'welcome': 1,\n",
       " 'machine': 1,\n",
       " 'hey': 1,\n",
       " 'chronological': 1,\n",
       " 'anyway': 1,\n",
       " 'difficult': 1,\n",
       " 'compare': 1,\n",
       " 'piper': 1,\n",
       " 'gates': 1,\n",
       " 'dawn': 1,\n",
       " 'meddle': 1,\n",
       " 'years': 1,\n",
       " 'someone': 1,\n",
       " 'musicians': 1,\n",
       " 'logic': 1,\n",
       " 'applies': 1,\n",
       " 'shows': 1,\n",
       " 'agree': 1,\n",
       " 'less': 1,\n",
       " 'remotely': 1,\n",
       " 'avishai': 1,\n",
       " 'cohen': 1,\n",
       " 'never': 1,\n",
       " 'listened': 1,\n",
       " 'eh': 1,\n",
       " 'spectrum': 1,\n",
       " 'billy': 1,\n",
       " 'cobham': 1,\n",
       " 'louis': 1,\n",
       " 'cole': 1,\n",
       " 'project': 1,\n",
       " 'sesh': 1,\n",
       " 'sessions': 1,\n",
       " 'thank': 1,\n",
       " 'crack': 1,\n",
       " 'skye': 1,\n",
       " 'bien': 1,\n",
       " 'frialdad': 1,\n",
       " 'conociste': 1,\n",
       " 'debes': 1,\n",
       " 'sumar': 1,\n",
       " 'ciudad': 1,\n",
       " 'caótica': 1,\n",
       " 'temer': 1,\n",
       " 'dedíquense': 1,\n",
       " 'simplente': 1,\n",
       " 'disfrutar': 1,\n",
       " 'claro': 1,\n",
       " 'esperes': 1,\n",
       " 'tampoco': 1,\n",
       " 'vaya': 1,\n",
       " 'actitud': 1,\n",
       " 'pura': 1,\n",
       " 'vida': 1,\n",
       " 'calle': 1,\n",
       " 'vengan': 1,\n",
       " 'contactan': 1,\n",
       " 'tiempo': 1,\n",
       " 'costa': 1,\n",
       " 'rica': 1,\n",
       " 'lindas': 1,\n",
       " 'memorias': 1,\n",
       " 'context': 1,\n",
       " 'kurzgesagt': 1,\n",
       " 'forward': 1,\n",
       " 'ro': 1,\n",
       " 'watch': 1,\n",
       " 'compared': 1,\n",
       " 'documentary': 1,\n",
       " 'conocí': 1,\n",
       " 'descendiente': 1,\n",
       " 'kawésqar': 1,\n",
       " 'creo': 1,\n",
       " 'guardaparques': 1,\n",
       " 'nacional': 1,\n",
       " 'bernardo': 1,\n",
       " \"o'higgins\": 1,\n",
       " 'edén': 1,\n",
       " 'contaba': 1,\n",
       " 'tomaban': 1,\n",
       " 'cucharada': 1,\n",
       " 'aceite': 1,\n",
       " 'puro': 1,\n",
       " 'lobo': 1,\n",
       " 'después': 1,\n",
       " 'sumergían': 1,\n",
       " 'bucear': 1,\n",
       " 'desnudos': 1,\n",
       " 'aguas': 1,\n",
       " 'gélidas': 1,\n",
       " 'fuente': 1,\n",
       " 'www.facebook.com/radioautenticafmdepuertocisnes/videos/1253583628346466': 1,\n",
       " 'aparentemente': 1,\n",
       " 'maisha': 1,\n",
       " 'gogo': 1,\n",
       " 'penguin': 1,\n",
       " 'jacob': 1,\n",
       " 'collier': 1,\n",
       " 'mainly': 1,\n",
       " 'classics': 1,\n",
       " 'gentleman': 1,\n",
       " 'v=0tbwndhdowu': 1,\n",
       " 'v=jmtccus8l9o': 1,\n",
       " 'sounds': 1,\n",
       " 'v=ejftdshk-f0': 1,\n",
       " 'question': 1,\n",
       " 'right': 1,\n",
       " 'makes': 1,\n",
       " 'craiglist': 1,\n",
       " 'told': 1,\n",
       " 'thing': 1,\n",
       " 'pasa': 1,\n",
       " 'leen': 1,\n",
       " 'noticia': 1,\n",
       " 'portales': 1,\n",
       " 'extranjeros': 1,\n",
       " 'pasado': 1,\n",
       " 'noticias': 1,\n",
       " 'tono': 1,\n",
       " 'energy': 1,\n",
       " 'technology': 1,\n",
       " 'great': 1,\n",
       " 'suspicacias': 1,\n",
       " 'razón': 1,\n",
       " 'logro': 1,\n",
       " 'comprender': 1,\n",
       " 'alguien': 1,\n",
       " 'luces': 1,\n",
       " 'asunto': 1,\n",
       " 'agradezco': 1,\n",
       " 'short': 1,\n",
       " 'go': 1,\n",
       " 'schöneberg': 1,\n",
       " 'source': 1,\n",
       " 'bayerisches': 1,\n",
       " 'viertel': 1,\n",
       " 'drink': 1,\n",
       " 'move': 1,\n",
       " 'looks': 1,\n",
       " 'primus': 1,\n",
       " 'members': 1,\n",
       " 'v=aydfwujzyqg': 1,\n",
       " 'www.reddit.com/r/ifyoulikeblank/comments/iqh7oy/iil_larger_than_life_music_such_as_echoes_by_pink/g4t6g89': 1,\n",
       " 'utm_source=share': 1,\n",
       " 'utm_medium=web2x': 1,\n",
       " 'context=3': 1,\n",
       " 'schwarzes': 1,\n",
       " 'cafe': 1,\n",
       " 'simple': 1,\n",
       " 'hervirlos': 1,\n",
       " 'rato': 1,\n",
       " 'olla': 1,\n",
       " 'presión': 1,\n",
       " 'mejor': 1,\n",
       " 'piñones': 1,\n",
       " 'subterranean': 1,\n",
       " 'homesick': 1,\n",
       " 'alien': 1,\n",
       " 'radiohead': 1,\n",
       " \"c'mmon\": 1,\n",
       " 'hollywood': 1,\n",
       " 'movie': 1,\n",
       " 'center': 1,\n",
       " 'planet': 1,\n",
       " 'tetas': 1,\n",
       " 'mercedes': 1,\n",
       " 'call': 1,\n",
       " 'exactly': 1,\n",
       " 'protest': 1,\n",
       " 'singer': 1,\n",
       " 'vengo': 1,\n",
       " '2014': 1,\n",
       " 'v=s-zgljquqwm': 1,\n",
       " 'tijoux': 1,\n",
       " 'friend': 1,\n",
       " 'old': 1,\n",
       " 'laptop': 1,\n",
       " 'ignorance': 1,\n",
       " 'www.ebay-kleinanzeigen.de/s-anzeige/e-gitarre-les-paul/1814238758-74-3481': 1,\n",
       " 'madame': 1,\n",
       " 'tussauds': 1,\n",
       " 'dumps': 1,\n",
       " 'trump': 1,\n",
       " 'u.s': 1,\n",
       " 'election': 1,\n",
       " 'cisnes': 1,\n",
       " 'nearly': 1,\n",
       " 'beirut': 1,\n",
       " 'explosion': 1,\n",
       " 'rescue': 1,\n",
       " 'dog': 1,\n",
       " 'able': 1,\n",
       " 'presence': 1,\n",
       " 'heartbeat': 1,\n",
       " 'rubble': 1,\n",
       " 'miraculously': 1,\n",
       " 'save': 1,\n",
       " 'wounded': 1,\n",
       " 'trapped': 1,\n",
       " 'chilenas': 1,\n",
       " 'latinoamericanas': 1,\n",
       " 'extranjero': 1,\n",
       " 'tod': 1,\n",
       " 'invitaron': 1,\n",
       " 'cantar': 1,\n",
       " 'par': 1,\n",
       " 'pequeño': 1,\n",
       " 'concierto': 1,\n",
       " 'castellano': 1,\n",
       " 'supuesto': 1,\n",
       " 'formal': 1,\n",
       " 'guitarra': 1,\n",
       " 'europa': 1,\n",
       " 'hispanohablante': 1,\n",
       " 'cuesta': 1,\n",
       " 'imaginar': 1,\n",
       " 'chileno': 1,\n",
       " 'español': 1,\n",
       " 'latinoamericano': 1,\n",
       " 'fácil': 1,\n",
       " 'conoce': 1,\n",
       " 'letra': 1,\n",
       " 'populares': 1,\n",
       " 'letras': 1,\n",
       " 'relevantes': 1,\n",
       " 'ritmo': 1,\n",
       " 'melodía': 1,\n",
       " 'pasan': 1,\n",
       " 'ser': 1,\n",
       " 'primordial': 1,\n",
       " '¿qué': 1,\n",
       " 'canción': 1,\n",
       " 'elegirían': 1,\n",
       " 'vacilonas': 1,\n",
       " 'lentas': 1,\n",
       " 'representen': 1,\n",
       " 'forma': 1,\n",
       " 'cultura': 1,\n",
       " '¡gracias': 1,\n",
       " 'consejos': 1,\n",
       " 'real': 1,\n",
       " 'virtual': 1,\n",
       " 'hungry': 1,\n",
       " 'quick': 1,\n",
       " 'list': 1,\n",
       " 'comfortable': 1,\n",
       " 'categorizing': 1,\n",
       " 'brief': 1,\n",
       " 'idea': 1,\n",
       " 'combination': 1,\n",
       " 'try': 1,\n",
       " 'put': 1,\n",
       " 'genres': 1,\n",
       " 'ones': 1,\n",
       " 'progressive': 1,\n",
       " 'american': 1,\n",
       " 'alternative': 1,\n",
       " 'world': 1,\n",
       " 'grunge': 1,\n",
       " 'psychedelic': 1,\n",
       " 'nu': 1,\n",
       " 'tropicalia': 1,\n",
       " 'funk': 1,\n",
       " 'cumbia': 1,\n",
       " 'trip': 1,\n",
       " 'trash': 1,\n",
       " 'bossa': 1,\n",
       " 'nova': 1,\n",
       " 'avant-garde': 1,\n",
       " 'classic': 1,\n",
       " 'flamenco': 1,\n",
       " 'krautrock': 1,\n",
       " 'dub': 1,\n",
       " 'space': 1,\n",
       " 'nueva': 1,\n",
       " 'cancion': 1,\n",
       " 'african': 1,\n",
       " 'bebop': 1,\n",
       " 'bolero': 1,\n",
       " 'manouche': 1,\n",
       " 'reggae': 1,\n",
       " 'acid': 1,\n",
       " 'experimental': 1,\n",
       " 'punk': 1,\n",
       " 'electroacoustic': 1,\n",
       " 'classical': 1,\n",
       " 'idm': 1,\n",
       " 'groove': 1,\n",
       " 'andean': 1,\n",
       " 'art': 1,\n",
       " 'trance': 1,\n",
       " 'balkan': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dummdumm92',\n",
       " 'pdonoso',\n",
       " 'Javiercdx',\n",
       " 'satalana',\n",
       " 'FookenZyzzBrah',\n",
       " 'musicatito',\n",
       " 'PM-me-ur-kittenz',\n",
       " 'Iyon_Tichy',\n",
       " 'Manomana-cl',\n",
       " 'ancdefghi',\n",
       " 'alexb599',\n",
       " 'Rodrigoarc',\n",
       " 'butchYbutch_',\n",
       " 'maialen09',\n",
       " 'Melonemelo123',\n",
       " 'h-u-g-o',\n",
       " 'GreenSpectr3',\n",
       " 'idontbrowseaww',\n",
       " 'warmans',\n",
       " 'SkyNet03',\n",
       " 'TheRealWeedAtman',\n",
       " 'its_mango_time',\n",
       " 'saproxilico',\n",
       " 'flrianjst',\n",
       " 'Tiger_Mann',\n",
       " 'DirtyProtest',\n",
       " 'CVirus',\n",
       " 'Belodri',\n",
       " 'lilo910',\n",
       " 'Relayedroid',\n",
       " 'og_han',\n",
       " 'badwives',\n",
       " 'jabonkagigi',\n",
       " 'ivan_xd',\n",
       " 'pweepweemuggins',\n",
       " 'Mugen_1212',\n",
       " 'Schtiglitz',\n",
       " 'FunLovinMonotreme',\n",
       " 'ElFlaco2',\n",
       " 'paulmccartney',\n",
       " 'Naldrek',\n",
       " 'EB3031',\n",
       " 'chicosapo',\n",
       " 'rainman_104',\n",
       " 'NNorAl',\n",
       " 'Milo0192',\n",
       " 'Keanu_Norris',\n",
       " 'Peyotine',\n",
       " 'jsnaomi6',\n",
       " 'Afterlifehappydeath',\n",
       " 'patiperro_v3',\n",
       " 'tresclow',\n",
       " 'underground_Luau',\n",
       " 'Aquarium-Luxor',\n",
       " 'Tierrrez',\n",
       " 'danggupta90',\n",
       " 'gerrypoliteandcunty',\n",
       " 'acid8k',\n",
       " 'KDamage',\n",
       " 'alimuzammil1998',\n",
       " 'KennyTurbo',\n",
       " 'JohnAvi',\n",
       " 'Irresponsible_Tune',\n",
       " 'upka',\n",
       " 'AntonioZamorano58',\n",
       " 'covidparis',\n",
       " 'restoreprivacydotcom',\n",
       " 'Cuntable',\n",
       " 'Ok-Aerie-6822',\n",
       " 'bold_and_brash_trash',\n",
       " 'flashcatcher',\n",
       " 'xzinik',\n",
       " 'BlackViperMWG',\n",
       " 'JMG_99',\n",
       " 'Lo_Innombrable',\n",
       " 'TitoDantito',\n",
       " 's_berliner',\n",
       " 'gazaehl',\n",
       " 'Kawtcho',\n",
       " 'OsoGuti',\n",
       " 'lulaloops',\n",
       " 'SevenOfSpadess',\n",
       " 'godless-life',\n",
       " 'ghsgjgfngngf',\n",
       " '93WhiteStrat',\n",
       " 'kwanbix',\n",
       " 'raverbashing',\n",
       " 'Deneb0la',\n",
       " 'AstaS-',\n",
       " 'FortePianoForte',\n",
       " 'kachol',\n",
       " 'meep_42',\n",
       " 'Just-me-fmCR',\n",
       " 'weaweonaaweonao',\n",
       " 'uffjedn',\n",
       " 'Tukurito',\n",
       " 'Mayhzon',\n",
       " 'HombreContrafactual',\n",
       " 'ky2k',\n",
       " 'Skrondo87',\n",
       " 'Dynaparte',\n",
       " 'will1707',\n",
       " 'salomown',\n",
       " 'saoirsecaoilfhoinn',\n",
       " 'JaLogoJa',\n",
       " 'ComfortablyFloyd',\n",
       " 'CashmereShiv',\n",
       " 'BunyipPouch',\n",
       " 'gramoun-kal',\n",
       " 'sandiaazucar',\n",
       " 'Yuseiger',\n",
       " 'Lopr1621',\n",
       " 'possi1',\n",
       " 'NuQ',\n",
       " 'dxtp',\n",
       " 'verycrafty',\n",
       " 'iushiush',\n",
       " 'ohravenyouneverlearn',\n",
       " 'breathepls',\n",
       " 'Phrodo_00',\n",
       " 'magezt',\n",
       " 'kinte',\n",
       " 'Hard_Rain_Falling',\n",
       " 'IronStrange7',\n",
       " 'Kid_In_Sandbox',\n",
       " 'puntastic_name',\n",
       " 'vectorpropio',\n",
       " 'Maur0',\n",
       " 'EuSouAssimReddit',\n",
       " 'cimocw',\n",
       " 'eeeeonflux',\n",
       " 'LJMcMillan',\n",
       " 'jotanova',\n",
       " 'n1c0_ds',\n",
       " 'AutoModerator',\n",
       " 'frenchliner',\n",
       " 'markzlz',\n",
       " 'PhilipJay99',\n",
       " 'Iatrogenia',\n",
       " 'burupie',\n",
       " 'longanizas',\n",
       " 'rakeguitar',\n",
       " 'cYzzie',\n",
       " 'navtaq',\n",
       " 'brent_mused',\n",
       " 'ruincreep',\n",
       " 'IamYodaBot',\n",
       " 'mambita',\n",
       " 'Trubinio',\n",
       " 'kingdraven',\n",
       " 'magallanes2010',\n",
       " 'vu67',\n",
       " 'rebelrebel2013']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_users_groups_keywords_dict = {}\n",
    "final_users_groups_keywords_dict[username] = {}\n",
    "final_users_groups_keywords_dict[username]['groups'] = groups_dict\n",
    "final_users_groups_keywords_dict[username]['keywords'] = keywords_dict\n",
    "final_users_groups_keywords_dict[username]['users'] = users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all groups and keywords for the associated users\n",
    "\n",
    "We automatize now the previous procedure to obtain **groups** and **keywords** for every **user** in `users_list`, and save them in a Python dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_keywords_dict(user, the_headers):\n",
    "\n",
    "    posts_full_text = \"\"\n",
    "    groups_list = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_comments = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/comments\",\n",
    "                                        headers = the_headers,\n",
    "                                        params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "            #my_headers = None\n",
    "            #while my_headers is None:\n",
    "            #    try:\n",
    "            #        # connect\n",
    "            #        my_headers = headers_connection_request()\n",
    "            #    except:\n",
    "            #         pass\n",
    "    try:\n",
    "        for post in res_comments.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['body']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            res_submitted = requests.get(\"https://oauth.reddit.com\" + \"/user\" + \"/\" + user + \"/submitted\",\n",
    "                                         headers = the_headers,\n",
    "                                         params = {'limit': my_limit})\n",
    "            break\n",
    "        except requests.ConnectionError:\n",
    "            print(\"ConnectionError, trying again...\")\n",
    "            my_headers = Headers()\n",
    "            #my_headers = None\n",
    "            #while my_headers is None:\n",
    "            #    try:\n",
    "            #        # connect\n",
    "            #        my_headers = headers_connection_request()\n",
    "            #    except:\n",
    "            #         pass\n",
    "    try:\n",
    "        for post in res_submitted.json()['data']['children']:\n",
    "            posts_full_text += \" \" + post['data']['title']\n",
    "            groups_list.append(post['data']['subreddit'])\n",
    "            if post['data']['selftext']:\n",
    "                posts_full_text += \" \" + post['data']['selftext']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    groups_dict = {group: count for group, count in Counter(groups_list).most_common()}\n",
    "\n",
    "    corpus_text = posts_full_text.lower()\n",
    "    keywords_dict = {word: count for word, count in Counter(common_words(corpus_text)).most_common()}\n",
    "\n",
    "    return groups_dict, keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_headers = headers_connection_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dummdumm92\n",
      "1 pdonoso\n",
      "2 Javiercdx\n",
      "3 satalana\n",
      "4 FookenZyzzBrah\n",
      "5 musicatito\n",
      "6 PM-me-ur-kittenz\n",
      "7 Iyon_Tichy\n",
      "8 Manomana-cl\n",
      "9 ancdefghi\n",
      "10 alexb599\n",
      "11 Rodrigoarc\n",
      "12 butchYbutch_\n",
      "13 maialen09\n",
      "14 Melonemelo123\n",
      "15 h-u-g-o\n",
      "16 GreenSpectr3\n",
      "17 idontbrowseaww\n",
      "18 warmans\n",
      "19 SkyNet03\n",
      "20 TheRealWeedAtman\n",
      "21 its_mango_time\n",
      "22 saproxilico\n",
      "23 flrianjst\n",
      "24 Tiger_Mann\n",
      "25 DirtyProtest\n",
      "26 CVirus\n",
      "27 Belodri\n",
      "28 lilo910\n",
      "29 Relayedroid\n",
      "30 og_han\n",
      "31 badwives\n",
      "32 jabonkagigi\n",
      "33 ivan_xd\n",
      "34 pweepweemuggins\n",
      "35 Mugen_1212\n",
      "36 Schtiglitz\n",
      "37 FunLovinMonotreme\n",
      "38 ElFlaco2\n",
      "39 paulmccartney\n",
      "40 Naldrek\n",
      "41 EB3031\n",
      "42 chicosapo\n",
      "43 rainman_104\n",
      "44 NNorAl\n",
      "45 Milo0192\n",
      "46 Keanu_Norris\n",
      "47 Peyotine\n",
      "48 jsnaomi6\n",
      "49 Afterlifehappydeath\n",
      "50 patiperro_v3\n",
      "51 tresclow\n",
      "52 underground_Luau\n",
      "53 Aquarium-Luxor\n",
      "54 Tierrrez\n",
      "55 danggupta90\n",
      "56 gerrypoliteandcunty\n",
      "57 acid8k\n",
      "58 KDamage\n",
      "59 alimuzammil1998\n",
      "60 KennyTurbo\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "61 JohnAvi\n",
      "62 Irresponsible_Tune\n",
      "63 upka\n",
      "64 AntonioZamorano58\n",
      "65 covidparis\n",
      "66 restoreprivacydotcom\n",
      "67 Cuntable\n",
      "68 Ok-Aerie-6822\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "69 bold_and_brash_trash\n",
      "70 flashcatcher\n",
      "71 xzinik\n",
      "72 BlackViperMWG\n",
      "73 JMG_99\n",
      "74 Lo_Innombrable\n",
      "75 TitoDantito\n",
      "76 s_berliner\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "77 gazaehl\n",
      "78 Kawtcho\n",
      "79 OsoGuti\n",
      "80 lulaloops\n",
      "81 SevenOfSpadess\n",
      "82 godless-life\n",
      "83 ghsgjgfngngf\n",
      "84 93WhiteStrat\n",
      "85 kwanbix\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "86 raverbashing\n",
      "87 Deneb0la\n",
      "88 AstaS-\n",
      "89 FortePianoForte\n",
      "90 kachol\n",
      "91 meep_42\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "92 Just-me-fmCR\n",
      "93 weaweonaaweonao\n",
      "94 uffjedn\n",
      "95 Tukurito\n",
      "96 Mayhzon\n",
      "97 HombreContrafactual\n",
      "98 ky2k\n",
      "99 Skrondo87\n",
      "100 Dynaparte\n",
      "101 will1707\n",
      "102 salomown\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "103 saoirsecaoilfhoinn\n",
      "104 JaLogoJa\n",
      "105 ComfortablyFloyd\n",
      "106 CashmereShiv\n",
      "107 BunyipPouch\n",
      "108 gramoun-kal\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "109 sandiaazucar\n",
      "110 Yuseiger\n",
      "111 Lopr1621\n",
      "112 possi1\n",
      "113 NuQ\n",
      "114 dxtp\n",
      "115 verycrafty\n",
      "116 iushiush\n",
      "117 ohravenyouneverlearn\n",
      "118 breathepls\n",
      "119 Phrodo_00\n",
      "120 magezt\n",
      "121 kinte\n",
      "122 Hard_Rain_Falling\n",
      "123 IronStrange7\n",
      "124 Kid_In_Sandbox\n",
      "125 puntastic_name\n",
      "126 vectorpropio\n",
      "127 Maur0\n",
      "128 EuSouAssimReddit\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "129 cimocw\n",
      "130 eeeeonflux\n",
      "131 LJMcMillan\n",
      "132 jotanova\n",
      "133 n1c0_ds\n",
      "134 AutoModerator\n",
      "135 frenchliner\n",
      "136 markzlz\n",
      "137 PhilipJay99\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "138 Iatrogenia\n",
      "139 burupie\n",
      "140 longanizas\n",
      "141 rakeguitar\n",
      "142 cYzzie\n",
      "143 navtaq\n",
      "144 brent_mused\n",
      "145 ruincreep\n",
      "146 IamYodaBot\n",
      "147 mambita\n",
      "148 Trubinio\n",
      "149 kingdraven\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "ConnectionError, trying again...\n",
      "test\n",
      "150 magallanes2010\n",
      "151 vu67\n",
      "152 rebelrebel2013\n"
     ]
    }
   ],
   "source": [
    "for i, user in enumerate(users_list):\n",
    "    print(i, user)\n",
    "    gkd = groups_keywords_dict(user, my_headers)\n",
    "    final_users_groups_keywords_dict[user] = {}\n",
    "    final_users_groups_keywords_dict[user]['groups'] = gkd[0]\n",
    "    final_users_groups_keywords_dict[user]['keywords'] = gkd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_users_groups_keywords_dict['rebelrebel2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping to a json file the raw information of the tripartite network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would likely create a json file of a couple of MB\n",
    "with open('tripartite_raw.json', 'w') as f:\n",
    "    json.dump(final_users_groups_keywords_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
